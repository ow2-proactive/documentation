:docinfo:
= Machine Learning Open Studio
include::../common-settings.adoc[]
:toc-title: ML OS User Guide

== Overview

include::references/Overview.adoc[]

=== Glossary

include::../Glossary.adoc[]

== Get started

To submit your first Machine Learning workflow to *ProActive Scheduler*, link:../admin/ProActiveAdminGuide.html#_getting_started[install] it in
your environment (default credentials: admin/admin) or just use our demo platform http://try.activeeon.com[try.activeeon.com^].

*ProActive Scheduler* provides comprehensive interfaces that allow to:

- http://try.activeeon.com/studio[Create workflows^]
- http://try.activeeon.com/scheduler[Submit workflows, monitor their execution and retrieve the tasks results^]
- http://try.activeeon.com/rm[Add resources and monitor them^]
- http://try.activeeon.com//cloud-automation[Add services^]

We also provide REST and command line interfaces for advanced users.

To add the *Machine Learning Bucket* containing the generic ML tasks, you need to click on `Catalog` menu then `Set Bucket as Main Catalog Menu` and select `machine-learning` bucket.

This can also be achieved by adding `/templates/machine-learning` at the end of the URL of the proactive workflow studio.

NOTE: Set Bucket as Main Catalog Menu allows the user to change the bucket used to get workflows from the Catalog in the studio. By selecting a bucket, the user can change the content of the main Catalog menu (named as the current bucket) to get workflows from another bucket as templates.

== Machine Learning Bucket

=== Public Datasets

==== Load_Boston_Dataset

*Task Overview:*  Load and return the Boston House-Prices dataset.

.Boston Dataset Description
[cols="17%,17%,20%,20%", options="header"]
|===
| Features | Targets | Dimensionality | Samples Total
| Real, positive | Real 5. -5O | 13 | 506
|===

*How to use this task:*

- The Boston House-Prices is a dataset regression, you can only use it with a regression algorithm, such as Linear Regression and Support Vector Regression.

- After this task, you can use the *Split_Data* task to divide the dataset into training and testing sets.

NOTE: More information about this dataset can be found http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html[here^].

==== Load_Iris_Dataset

*Task Overview:*  Load and return the iris dataset.

.Iris Dataset Description
[cols="17%,17%,20%,20%,20%", options="header"]
|===
| Features | Classes | Dimensionality | Samples per class | Samples total
| Real, positive | 3 | 4 | 50 | 150
|===

*How to use this task:*

- The Iris is a dataset for classification, you can only use it with a classification algorithm, such as Support Vector Machines and Logistic Regression.

- After this task, you can use the *Split_Data* task to divide the dataset into training and testing sets.

NOTE: More information about this dataset can be found http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris[here^].

=== Input and Output Data

==== Download_Model

*Task Overview:* Download a trained model on your computer device.

*How to use this task:* Should be used after the *Train_Model* or *Train_Clustering_Model* tasks.

==== Export_Results

*Task Overview:* Export the results of the predictions generated by a classification, clustering or regression algorithm.

*Task Variables:*

.Export_Results_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `OUTPUT_FILE`
| Converts the prediction results to HTML or CSV file.
| String [HTML or CSV]
|===

*How to use this task:* Should be used after *Predict_Model* or *Predict_Clustering_Model* tasks.

==== Import_Data

*Task Overview:* Load data from external sources.

*Task Variables:*

.Import_Data_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `FILE_URL`
| Enter you URL of the CSV file.
| String
| `FILE_DELIMITER`
| Delimiter to use.
| String
| `IS_LABELED_DATA`
| True if your data have label data.
| Boolean [True or False]
|===

WARNING: Your CSV file should be in a table format. See the example below.

image::csv_file_organisation.png[align="center"]

==== Load_Model

*Task Overview:* Load a trained model, and use it to make predictions for new coming data.

*Task Variables:*

.Load_Model_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `MODEL_URL`
| Type the URL to load your trained model.
default:  https://s3.eu-west-2.amazonaws.com/activeeon-public/models/pima-indians-diabetes.model
| String
|===

*How to use this task:* Should be used before *Predict_Model* or *Predict_Clustering_Model* to make predictions.

==== Log_Parser

*Task Overview:* Convert an *unstructured* raw log file into a structured one by matching a group of event patterns.

*Task Variables:*

.Log_Parser_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `LOG_FILE`
| Put the URL of the raw log file that you need to parse.
| String
| `PATTERNS_FILE`
| Put the URL of the CSV file that contains the different REGEX expressions of each possible pattern
  and their corresponding variables.
  The csv file must contain three columns (See the example below):

    A. *id_pattern*: Integer
        Specify the column containing the identifier of each pattern

    B. *Pattern*: RegEx expression
        Define the regex expression of each pattern

    C. *Variables*: String
        Specify the name of each variable included in the pattern.
        N.B: Use the symbol ‘*’ for variables that you need to neglect. (e.g. in the example below the 5th
        variable is neglected)
        N.B: All variables specified in each Regex expressions have to be mentioned in the column
        « Variables » in the right order (use ',' to separate the variable names).

| String
| `STRUCTURED_LOG`
| Indicate the extension of the file where you will save the resulted structured logs.
| String [CSV or HTML]
|===

image::pattern_file.png[align="center"]

*How to use this task:* Could be connected with *Filter_Data* task and feature extraction's tasks.

=== Data Preprocessing

==== Add_Data

*Task Overview:* Concatenate the new added data to the original input data.

*Task Variables:*

.Add_Data_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `FILE_URL`
| Put the URL of the data that you need to add.
| String
| `FILE_DELIMITER`
| Delimiter to use.
| String
| `IS_LABELED_DATA`
| TRUE if the data is labeled.
| Boolean [True or False]
| `IGNORE_INDEX`
| False if you need to ignore the index of the input data.
| Boolean [True or False]
|===

*How to use this task:* Should be used after *Import_Data* task.

NOTE: More details about the source code of this task can be found https://pandas.pydata.org/pandas-docs/stable/merging.html[here^].

==== Add_Label

*Task Overview:* Add a new label column to the original input data.

*Task Variables:*

.Add_Label_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `FILE_URL`
| The URL of the file containing the labels that you need to concatenate.
| String
| `FILE_DELIMITER`
| Delimiter to use.
|===

*How to use this task:* Could be used after *Feature_Extraction* and *Import_Data* tasks.

NOTE: More details about the source code of this task can be found https://pandas.pydata.org/pandas-docs/stable/merging.html[here^].

==== Filter_Data

*Task Overview:* Query the columns of your data with a boolean expression.

*Task Variables:*

.Filter_Data_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `Query`
| The query string to evaluate.
| String
| `FILTERED_FILE_OUPUT`
| Refers to the extension of the file where the resulted filtered data will be saved.
| String [CSV or HTML]
|===

NOTE: More details about the source code of this task can be found https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query[here^].

==== Split_Data

*Task Overview:* Separate data into train and test subsets.

*Task Variables:*

.Split_Data_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `TRAIN_SIZE`
| This parameter must be float within the range (0, 100), not including the values 0 and 100. *default* = 0.7
| Float
|===

*How to use this task:* Should be used after Train and Predict tasks.

NOTE: More details about the source code of this task can be found http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html[here^].

=== Feature Extraction

==== Feature_Vector_Extractor

*Task Overview:* Encode *structured* data into numerical feature vectors whereby machine learning models can be applied.

*Task Variables:*

.Feature_Vector_Extractor_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `SESSION_COLUMN`
| the ID of the entity that you need to represent (to group by).
| String
| `FILE_OUT_FEATURES`
| The extension of the file where the resulted features will be saved.
| String [CSV or HTML]
| `PATTERN_COLUMN`
| The index of column containing the log patterns.[specific to features extraction from logs]
| String
| `PATTERNS_COUNT_FEATURES`
| True if you need to extract count the number of occurrence of each pattern per session.
| Boolean [True or False]
| `STATE_VARIABLES`
| The different variables that need to be considered to extract features according to their content.

N.B: separate the different variables with a comma ','
| String
| `COUNT_VARIABLES`
| Refers to the different variables that need to be considered to count their distinct content.
| String

N.B: separate the different variables with a comma ','
| `STATE_COUNT_FEATURES_VARIABLES`
| True if you nedd to extract state and count features per session.
| Boolean [True or False]
|===

*How to use this task:* Could be connected with *Add_Label* if you need to train a model using supervised learning algorithms or with *Train_Clustering_Model* if you need to train a model using unsupervised machine learning techniques.

==== Time_Series_Feature_Extractor

*Task Overview:* Extract features by considering the temporal distribution of data.

*Task Variables:*

.Time_Series_Feature_Extractor_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `SESSION_COLUMN`
| The column name of the entity that you need to represent.
| String
| `SLIDING_STEP`
| The number of minutes to jump while navigating from a window to another one.
| Integer (in Minutes)
| `WINDOW_SIZE`
| The window size in Minutes.
| Integer (in Minutes)
| `START_DATE_TIME`
| The date starting from it the time series vectors representing each entity [Session] will be
  extracted.
| date
| `PATTERN_COLUMN`
| The column name containing the log patterns.[specific to features extraction from logs]
| String
| `PATTERNS_COUNT_FEATURES`
| True if you need to extract count the number of occurrence of each pattern per session and window.
| Boolean [True or False]
| `STATE_VARIABLES`
| The name of columns to use as features by counting the number of occurrence of their distinct
  content per session and window.

  N.B: separate the different variables with a comma ','
| String
| `STATE_COUNT_FEATURES_VARIABLES`
| True if you need to extract state and count features that you already specified in *COUNT_VARIABLES*
  and *STATE_VARIABLES*.

 N.B: separate the different variables with a comma ','
| String [True of False]
|===

*How to use this task:* Should be connected with specific machine learning algorithms suitable to this representation.

=== ML Classification

==== Gaussian_Naive_Bayes
*Task Overview:* Naive Bayes classifier is a family of simple probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features.

*Task Variables:*

.Gaussian_Naive_Bayes_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `PRIORS`
| Prior probabilities of the classes. If specified the priors are not adjusted according to the data.
| array-like, shape (n_classes)
|===

*How to use this task:* Should be connected with *Train_Model* / *Train_Clustering_Model* or *Predict_Model* / *Predict_Clustering_Model*.

NOTE: More information about this task can be found http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html[here^].

==== Logistic_Regression
*Task Overview:* Logistic Regression is a regression model where the Dependent Variable (DV) is categorical.

*Task Variables:*

.Logistic_Regression_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `PENALTY`
| Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. *default*=‘l2’
| String
| `SOLDER`
| Algorithm to use in the optimization problem.
| ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’, default: ‘liblinear’
| `MAX_ITERATIONS`
| Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken for the solvers to converge.
| Integer (default=100)
| `N_JOBS`
| Number of CPU cores used. If given a value of -1, all cores are used.
| Integer (default=100)
|===

*How to use this task:* Should be connected with *Train_Model* / *Train_Clustering_Model* or *Predict_Model* / *Predict_Clustering_Model*.

NOTE: More information about the source code of this task can be found http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html[here^].

==== Support_Vector_Machines

*Task Overview:* Support vector machines are supervised learning models with associated learning algorithms that analyze data used for classification.

*Task Variables:*

.Support_Vector_Machines_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `C`
| Penalty parameter C of the error term.
| Float, optional (default=1.0)
| `KERNEL`
| Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’,‘precomputed’ or a callable.
| string, optional (default=’rbf’)
|===

*How to use this task:* Should be connected with *Train_Model* / *Train_Clustering_Model* or *Predict_Model* / *Predict_Clustering_Model*.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html[here^].

=== ML Regression

==== Bayesian_Ridge_Regression

*Task Overview:* Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference.

*Task Variables:*

.Bayesian_Ridge_Regression_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `N_ITERATIONS`
| Penalty parameter C of the error term.
| Integer, optional (default=300)
| `ALPHA_1`
| Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter.
| Float (default=1.e-6)
| `ALPHA_2`
| Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter.
| Float (default=1.e-6)
| `LAMBDA_1`
| Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter.
| Float (default=1.e-6)
| `LAMBDA_2`
| Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter.
| Float (default=1.e-6)
|===

*How to use this task:* Should be connected with *Train_Model* / *Train_Clustering_Model* or *Predict_Model* / *Predict_Clustering_Model*.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html[here^].

==== Linear_Regression

*Task Overview:* Linear regression is a linear approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X.

*Task Variables:*

.Support_Vector_Machines_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `N_JOBS`
| Penalty parameter C of the error term.
| Integer (default=1)
|===

*How to use this task:* Should be connected with *Train_Model* / *Train_Clustering_Model* or *Predict_Model* / *Predict_Clustering_Model*.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html[here^].

==== Support_Vector_Regression

*Task Overview:* Support vector regression are supervised learning models with associated learning algorithms that analyze data used for regression.

*Task Variables:*

.Support_Vector_Regression_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `C`
| Penalty parameter C of the error term.
| Float, optional (default=1.0)
| `KERNEL`
| Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable.
| String, optional (default=’rbf’)
| `EPSILON`
| It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.
| Float, optional (default=1.0)
|===

*How to use this task:*  Should be connected with *Train_Model* / *Train_Clustering_Model* or *Predict_Model* / *Predict_Clustering_Model*.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html[here^].

=== ML Clustering

==== Kmeans

*Task Overview:* K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster

*Task Variables:*

.Support_Vector_Regression_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `N_CLUSTERS`
| The number of clusters to form as well as the number of centroids to generate
| Integer, optional (default=8)
| `MAX_ITERATIONS`
| Maximum number of iterations of the k-means algorithm for a single run.
| Integer, optional (default=300)
| `N_JOBS`
| The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all
| Integer, optional (default=1)
|===

*How to use this task:*  Should be connected with *Train_Model* / *Train_Clustering_Mode*l or *Predict_Model* / *Predict_Clustering_Model*.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html[here^].

==== Mean_Shift

*Task Overview:* Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function.

*Task Variables:*

.Mean_Shift_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `CLUSTER_ALL`
| The number of clusters to form as well as the number of centroids to generate.
| Boolean [True or False]
| `N_JOBS`
| If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1.
| Integer (default=1)
|===

*How to use this task:* Should be connected with *Train_Model* / *Train_Clustering_Model* or *Predict_Model* / *Predict_Clustering_Model*.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html[here^].

=== Train

==== Train_Clustering_Model

*Task Overview:* Train a clustering model.

*How to use this task:* Should be used after a clustering algorithm such as *Bayesian_Ridge_Regression*, *Linear_Regression* or *Support_Vector_Regression*.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/modules/clustering.html[here^].

==== Train_Model

*Task Overview:* Train a model using a classification or regression algorithm..

*How to use this task:* Should be used after a classification or regression algorithm, such as Support_Vector_Machines, Gaussian_Naive_Bayes and Linear_Regression.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/supervised_learning.html#supervised-learning[here^].

=== Predict

==== Predict_Clustering_Model

*Task Overview:* Generate predictions using a trained model.

*How to use this task:* Should be used after the *Train_Clustering_Model* task.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/supervised_learning.html#supervised-learning[here^].

==== Predict_Model

*Task Overview:* Generate predictions using a trained model.

*How to use this task:* Should be used after the *Train_Model task*.

NOTE: More information about the source of this task can be found http://scikit-learn.org/stable/supervised_learning.html#supervised-learning[here^].

== Data Visualization Bucket
*Visdom Bucket* integrates generic tasks that can be easily used to broadcast visualizations of the analytic results provided by ML tasks. These visualization are created, organized, and shared using https://github.com/facebookresearch/visdom[Visdom^], a flexible tool proposed by Facebook Research.

It provides a large set of plots that can be organized programmatically or through the UI. These plots can be used to create dashboards for both live and real-time data, inspect results of experiments, or debug experimental code.

Visdom bucket provides a fast, easy and practice way to execute different workflows generating these diverse visualizations that are automatically cached by the Visdom Server.

=== Bind_or_Start_Visdom_Service

*Task Overview:* Bind or/and Start Visdom server.

*Task Variables:*

.Bind_or_Start_Visdom_Service_Task variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `service_model`
| The Visdom service to start. The Visdom Service available in http://try.activeeon.com//cloud-automation[Proactive Cloud Automation^] is used by default. (default="http://models.activeeon.com/pca/visdom")
| String
| `instance_name`
| The instance name of the server to use to broadcast visualization
| String (default="visdom-server-1")
|===

WARNING: If two workflows use the same service instance names, then, their generated plots will be created on the same service instance.

=== Visdom_Client_Example

*Task Overview:* Connect and plot a text into the Visdom Server.

*Task Variables:*

To adapt according to the plots that you need to visualize

*How to use this task:*  Customize this task by putting the source code generating your code in the *Task Implementation* section

WARNING: This task has to be connected to the Bind_or_Start_Visdom_Service_Task. The Visdom server should be up in order to be able to broadcast visualizations.

=== Terminate_Visdom_Service

*Task Overview:*  Stop and remove the Visdom Server.

*Task Variables:*

.Terminate_Visdom_Service variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `service_model`
| The Visdom service to start. The Visdom Service available in http://try.activeeon.com//cloud-automation[Proactive Cloud Automation^] is used by default. (default="http://models.activeeon.com/pca/visdom")
| String
| `instance_name`
| The instance name of the server to use to broadcast visualization
| String (default="visdom-server-1")
|===

*How to use this task:* This task can be connected to Visdom_Client_Example. Visualize, then, stop the Visdom Service.

NOTE: This task will immediately stop the service.

=== Visdom_Terminate_Service_Until_Validation

*Task Overview:* wait for the validation of the job by the user to stop and remove the Visdom Server.

*Task Variables:*

.Visdom_Terminate_Service_Until_Validation variables
[cols="2,5,2"]
|===
| *Variable name* | *Description* | *Type*
| `service_model`
| The Visdom service to start. The Visdom Service available in http://try.activeeon.com//cloud-automation[Proactive Cloud Automation^] is used by default. (default="http://models.activeeon.com/pca/visdom")
| String
| `instance_name`
| The instance name of the server to use to broadcast visualization
| String (default="visdom-server-1")
|===

*How to use this task:* This task can be connected to Visdom_Client_Example. Visualize, then, stop the Visdom Service.

NOTE: This task will stop the service once the user has validated this requirement using http://try.activeeon.com//cloud-automation[Proactive Cloud Automation^] portal.

== Create and run your ML workflow

=== Create or Update a ML task

*Machine Learning Bucket* contains various open source tasks that can be easily used by a simple drag and drop.

It is possible to enrich the Machine Learning Bucket by adding your own tasks. (see section 4.5)

It is also possible to customize the code of the generic Machine Learning tasks. In this case, you need to drag and drop the targeted task to modify its code in the *Task Implementation* section.

NOTE: It is also possible to add or/and delete variables of each task, set your own fork environments etc. More details available on https://www.activeeon.com/public_content/documentation/latest/user/ProActiveUserGuide.html#_studio_tasks[Proactive User Guide^]

=== Set the Fork Environment

A fork execution environment is a new Java Virtual Machine (JVM) which is started exclusively to execute a task. Starting a new JVM means that the task inside it will run in a new environment. This environment can be set up by the creator of the task. A new JVMs is set up with a new classpath, new system properties and more customization.

We used a `Docker fork environment` for all the Machine Learning tasks. *activeeon/dlm3* was used as a docker container for all tasks. If your task needs to install new Machine Learning libraries which are not available in this container, then, use your own docker container or an appropriate environment with the needed libraries.

NOTE: The use of docker containers is recommended as that way other tasks will not be affected by change. Docker containers provide isolation so that the host machine’s software stays the same. More details available on https://www.activeeon.com/public_content/documentation/latest/user/ProActiveUserGuide.html#_fork_environment[Proactive User Guide^]

=== Publish a ML task

The `Catalog` menu provides the possibility for a user to publish new created or/and update tasks inside *Machine Learning Bucket*, you need just to click on `Catalog` Menu then `Publish current Workflow to the Catalog`.
Choose `machine-leaning` Bucket to store your new added workflow on it.
If the Task with the same name already exists in the 'machine-leaning' bucket, then, it will be updated.
We recommend to submit Tasks with a commit message for easier differentiation between the different submitted versions.

NOTE: More details available on https://www.activeeon.com/public_content/documentation/latest/user/ProActiveUserGuide.html#_publish_current_workflow_to_the_catalog[Proactive User Guide^]

=== Create a ML Workflow

The link:http://try.activeeon.com/tutorials/quickstart/quickstart.html[quickstart tutorial, window="_blank"] on link:http://try.activeeon.com[try.activeeon.com, window="_blank"]
shows you how to build a simple Machine learning workflow using Machine Learning Open Studio.

We show below an example of a workflow created with the Studio:

image::ML_Workflow_Example.png[align="center"]

At the left part, are illustrated the **General Parameters** of the workflow with the following information:

- `Name`: the name of the workflow.
- `Project`: the project name to which belongs the workflow.
- `Description`: the textual description of the workflow.
- `Documentation`: if the workflow has a Generic Information named "Documentation", then its URL value is displayed as a link.
- `Job Priority`: the priority assigned to the workflow. It is by default set to `NORMAL`, but can be increased or decreased once the job is submitted.

NOTE: The workflow represented in the above is available on the 'machine-learning-workflows' bucket.

== Machine Learning Workflows Examples

=== Basic Machine Learning

*simple_clustering:* trains and tests a clustering model using Mean_shift algorithm.

*simple_collaborative_filtering:* create a movie recommender engine using collaborative filtering algorithm

*simple_linear_regression:*  trains and tests a clustering model using Mean_shift algorithm.

*simple_logistic_regression:*  trains and tests a predictive model using logistic_regressive algorithm.

=== Image Analysis

video::XEMm47GwCn8[youtube, width=700, height=400 start=0, position=center]

*Anomaly_Detection_Demo:* checks if exists an anomaly in a certain scene using the YOLO library. However, we show a scene where only people are allowed on a pedestrian street. Anything detected other than people is considered as anomaly.

*Pytorch_Demo_Obj_Seg_Prediction:* returns the horse segments of a given input image.

*Pytorch_Demo_Obj_Seg_Training:* trains an image segmentation algorithm using PyTorch* to segment horses. It segments horses in the input image using a deep neural network (DNN).

*Tensorflow_Demo_Image_Classification:* classifies a input image using deep ConvNets (specifically, VGG16)* pre-trained on the ImageNet dataset.

*Tensorflow_Demo_Parallel_Prediction:* predicts three different images of flower species in parallel.

*Tensorflow_Demo_Prediction:* returns the flower specie of a given input image.

*Tensorflow_Demo_Training:* trains a deep ConvNets (specifically, Inception)* to recognize flower species.

*YOLO_Demo_Object_Detection:* detects real-world objects in a image with the YOLO* library using a pre-trained.

=== Log Analysis

*real_time_apache_intrusion_detection:* detects intrusions in apache logs.

*supervised_classification:* detects anomalies in HDFS logs based on a supervised algorithm.

*supervised_learning:* trains an anomaly detection model for logs using various supervised machine learning algorithms.

*unsupervised_classification:* detects anomalies in HDFS logs based on an unsupervised algorithm.

*unsupervised_learning:* trains an anomaly detection model for logs using various unsupervised machine learning algorithms.

=== Text Analysis

*unsupervised_learning*

=== Visdom Workflows

video::bQpwS2nMAZY[youtube, width=700, height=400 start=0, position=center]

*Visdom_Plot_Example:* returns numerous examples of plots covered by Visdom.

*Visdom_Realtime_Apache_Intrusion_Detection:*  broadcasts in real time some visualizations of the analytic results provided by the Apache log intrusion detector.

*Visdom_Realtime_Digit_Classification:* evaluates in real time the performance of a simple Convolutional Neural Network (CNN) for MNIST* handwritten digit classification during the training and test processes.

*Visdom_Realtime_HDFS_Anomaly_Detection:* broadcasts in real time some visualizations of the analytic results provided by the supervised anomaly detector for HDFS logs developed by ActiveEon.

*Visdom_Service_Example:* evaluates in real time the performance of a simple Convolutional Neural Network (CNN) for MNIST* handwritten digit classification during the training and test processes.



