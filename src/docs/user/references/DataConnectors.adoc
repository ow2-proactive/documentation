
The `data-connectors` bucket contains diverse generic data connectors for the most frequently used data storage systems (File, SQL, NoSQL, Cloud, ERP). The aim of a data connector is to facilitate and simplify importing data from external sources into your workflows.

=== File
The File connectors allow to import and export data from HTTP, FTP and SFTP servers.
We begin by presenting FTP and SFTP connectors then, URL connector.

==== FTP and SFTP connectors

*Variables:*
The FTP and SFTP connectors share the same list of variables. Consequently, we describe them in the following table using a unique notation.
`<PROTOCOL>` can take one of the following values: {`FTP`, `SFTP`}

.FTP/SFTP Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?*  | *Type*  | *Default/Examples*
| `<PROTOCOL>_HOSTNAME`
| IP address of the server host.
| Workflow, Task
| Yes
| String
| `localhost`
| `<PROTOCOL>_PORT`
| Listening port.
| Workflow, Task
| No
| Integer
| e.g. 21, 22
| `<PROTOCOL>_LOCAL_RELATIVE_PATH`
| Local `relative` path from which we upload (or to which we download) file(s).
It can contain either a path to a file, a directory terminated by `/` or an empty value for the root.
| Workflow, Task
| No
| String
| e.g. localDirectory/, example.zip
| `<PROTOCOL>_REMOTE_RELATIVE_PATH`
| Remote `relative` path to which we upload (or from which we download) file(s).
| Workflow, Task
| Yes
| String
| e.g. remoteDirectory/, test.txt
| `<PROTOCOL>_MODE`
| Transfer mode.
| Workflow, Task
| Yes
| PA:LIST(GET, PUT)
| GET
| `<PROTOCOL>_EXTRACT_ARCHIVE`
| Used only when `<PROTOCOL>_MODE=GET`. If set to `true`, the imported file will be extracted if it is an archive.
| Workflow, Task
| Yes
|  Boolean [true or false]
| false
| `<PROTOCOL>_USERNAME`
| Username to use for the authentication.
| Workflow, Task
| Yes
|  String
| e.g. \ftp://someuser@example.com
|===

*How to use this task:* The task requires the following third-party credential: `{key: <PROTOCOL>://<username>@<hostname>, value: <PROTOCOL>_PASSWORD}`. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].

==== URL connector

The URL connector allows, in addition, to import data using HTTP and HTTPS protocols.

*Variables:*

.URL Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?*  | *Type*  | *Default/Examples*
| `FILE_URL`
| Link to a file accessible using HTTP, HTTPS, SFTP or FTP protocols.

  FTP and SFTP urls must have the following patterns:

- `\ftp://<username>[:<password>]@<hostname>[:<port>]/<relativePath>`

- `sftp://<username>[:<password>]@<hostname>[:<port>]/<relativePath>`

| Task
| Yes
| String
| e.g. sftp://user:pass@example.com/test.txt
| `FTP_LOCAL_RELATIVE_PATH`
|  Local relative path from which we upload (or to which we download) file(s).
  LOCAL_RELATIVE_PATH can contain either a path to a file, a directory terminated by `/` or an empty value for the root.
| Task
| No
| String
| e.g. localDirectory/, example.zip
| `EXTRACT_ARCHIVE`
| If set to `true`, the imported file will be extracted if it is an archive.
| Task
| Yes
| Boolean [true or false]
| false
|===

*How to use this task:* We highly recommend the user to not provide his password within the URL and to instead use the third-party credential mechanism as follows: `{key: <PROTOCOL>://<username>@<hostname>, value: <PROTOCOL>_PASSWORD}`. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].

=== SQL
The SQL connectors allow to import data from Relational DataBase Management Systems (RDBMS).
Currently, we have connectors for Mysql, Oracle, Postgres, Greenplum and Sql server.

*Variables:*
All SQL connectors share the same list of variables. Consequently, we describe them in the following table using a unique notation.
`<RDBMS_NAME>` can take one of the following values: {`POSTGRES`, `GPDB`, `ORACLE`, `SQL_SERVER`, `MYSQL`}

.SQL Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?*  | *Type*  | *Default/Examples*
| `<RDBMS_NAME>_HOSTNAME`
| Label or the IP address of the server hosting the database.
| Workflow
| Yes
| String
| `localhost`
| `<RDBMS_NAME>_PORT`
| Listening port.
| Workflow
| No
| Integer
| e.g. 5432, 1521
| `<RDBMS_NAME>_DATABASE`
| Database name.
| Workflow
| Yes
| String
| e.g. activeeon
| `<RDBMS_NAME>_QUERY`
| Requires an SQL query or a table name to fetch data from.
| Workflow, Task
| Yes
| String
| e.g. select * from users
| `LABEL`
| Used when the imported data is labeled. Then, the user can specify the label column name.
| Task
| No
| String
| e.g. class
| `<RDBMS_NAME>_RMDB_DRIVER`
| The driver to connect to the database.
| Task
| Yes
| String
| e.g. cx_oracle, psycopg2
|===

*How to use this task:* This task uses the driver given in `RMDB_DRIVER` to connect to the database. To use another driver, make sure you have it properly installed before (e.g. using `pip install <RMDBS_DRIVER>`).
The task requires the following third-party credentials: `<RDBMS_NAME>_USERNAME` and `<RDBMS_NAME>_PASSWORD`. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].
The imported data is exported in a JSON format using the variable `DATAFRAME_JSON`.

=== NoSQL
The NoSQL connectors allow to import data from NoSQL Databases.
Currently, we have connectors for MongoDB and Cassandra.

*Variables:*

`<NoSQL_NAME>` can take one of the following values: {`CASSANDRA`, `MONGODB`}

.NoSQL Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?*  | *Type*  | *Default/Examples*
| `<NoSQL_NAME>_HOSTNAME`
| Label or the IP address of the server hosting the database.
| Workflow
| Yes
| String
| `localhost`
| `<NoSQL_NAME>_PORT`
| Listening port.
| Workflow
| No
| Integer
| e.g. 27018, 9042
| `MONGODB_DATABASE` or `CASSANDRA_KEYSPACE`
| Equivalent to the database name.
| Workflow
| Yes
| String
| e.g. activeeon
| `<NoSQL_NAME>_QUERY`
| Requires a NoSQL query to fetch data.
| Workflow, Task
| No (depends on the NoSQL database)
| String
| e.g. {"class":2}
| `MONGODB_COLLECTION`
| Equivalent to a `table` in a RDBMS.
| Workflow, Task
| Yes
| String
| e.g. users
| `LABEL`
| Used when the imported data is labeled. Then, the user can specify the label column name.
| Task
| No
| String
| e.g. class
| `NOSQL_DRIVER`
| The driver to connect to MongoDB.
| Task
| Yes
| String
| e.g. pymongo
|===

*How to use this task:* This task uses the driver given in `NOSQL_DRIVER` to connect to MongoDB. To use another driver, make sure you have it properly installed before.  (e.g. using `pip install <NOSQL_DRIVER>`).
The task requires the following third-party credentials: `<NoSQL_NAME>_USERNAME` and `<NoSQL_NAME>_PASSWORD`. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].
The imported data is exported in a JSON format using the variable `DATAFRAME_JSON`.

=== Cloud

Cloud data connectors allow to interact with cloud storage services. Currently we provide support for Amazon S3, Azure Storage and Azure Data Lake.

==== Azure Data Lake

The Azure Data Lake connector allows to upload U-SQL scripts and then execute them as Data Lake Analytics (DLA) jobs. It requires an existing Data Lake Analytics account and its corresponding Data Lake Store account. The connector workflow consists of three tasks:

* _Submit_job_: Connects to Azure Data Lake and submits the provided script.
* _Wait_for_job_: Periodically monitors the DLA job status until its finalization.
* _Display_result_: Downloads the result file and displays it.

*Variables:*

.Azure Data Lake Connector variables
[cols="2,5,2,2,2,2"]
|===
| *Variable name* | *Description* | *Level* | *Required?* | *Type*  | *Default/Examples*
| `AZ_DLA_ACCOUNT`
| Data Lake *Analytics* account to be used. It should already exist.
| Workflow
| Yes
| String
| e.g. my_dla_account
| `AZ_DLS_ACCOUNT`
| Data Lake *Store* account to be used.  It should already exist.
| Workflow
| Yes
| String
| e.g. my_dls_account
| `AZ_DLA_JOB`
| Name to identify the job to be submitted.
| Workflow
| Yes
| String
| e.g. my_dla_job
| `AZ_DLA_SCRIPT`
| File name of the U-SQL script to submit. The file must be located in the *Global Space* directory. An example file `script.usql` is provided.
| Workflow
| Yes
| String
v|Sample file: script.usql
e.g. my_usql_script.usql
| `AZ_DLA_OUTPUT`
| Name of the output file to store the result of the script.
| Workflow
| Yes
| String
| e.g. my_output_file.csv
| `AZ_CRON_MONITOR`
| Cron expression to determine how frequently to monitor the completion of the job.
| Workflow
| Yes
| String
v|Default: "* * * * \*"
(every minute)
e.g. "*/2 * * * *"
(every 2 minutes)
|===

*How to use these tasks:*

Azure Data Lake tasks require your Azure login credentials to be set as third-party credentials (`key:value` pairs); this is a one-time action and will ensure that your credentials are securely encrypted. Please refer to the User documentation to learn how to add link:../user/ProActiveUserGuide.html#_third_party_credentials[third-party credentials].

You have two options for providing your login credentials:

* Standard Azure login: `AZ_U:your_user` (usually an email). `AZ_P:your_password`.
* Using an link:https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli?toc=%2Fazure%2Fazure-resource-manager%2Ftoc.json&view=azure-cli-latest[Azure service principal]: `AZ_U:appId`. `AZ_P:password`. `AZ_T:tenant`. By default, if `AZ_T` is set, the tasks will attempt to connect through a service principal.

[NOTE]
====
.The Output File
* Instead of hardcoding the name of your output file in your U-SQL script, you can use the placeholder `OUTPUT_FILE`, which is automatically replaced by the value of `AZ_DLA_OUTPUT`.
* Once downloaded, the output file will be stored in your *User Space* (and _not_ in the Global Space).
====
