*ProActive AI Orchestration (PAIO)* is a complete DSML platform (Data Science and Machine Learning) including a ML Studio, AutoML, Data Science Orchestration and MLOps for the deployment,
training, execution and scalability of artificial intelligence and machine learning models on
any type of infrastructure. Created for data scientists and ML engineers, the solution is simple to use and accelerate the development and deployment of machine learning models.


image::PAIO_overview.PNG[align=center]

ProActive AI Orchestration platform provides a rich catalog of generic machine learning tasks that can be connected together to build either basic or advanced machine learning workflows for various use cases such as: fraud detection, text analysis, online offer recommendations, prediction of equipment failures, facial expression analysis, etc.
PAIO workflows enable users to manage machine learning pipelines through the different phases of the development lifecycle and allow them to better control tasks parallelization, by running the tasks on resources matching constraints (Multi-CPU, GPU, FPGA, data locality, libraries, etc).

image::PAIO-Open-Studio-ActiveEon.PNG[align=center]

The ProActive AI Orchestration platform is an open source solution, and it can be tested online without installation on our try platforms https://try.activeeon.com/studio/#presets/1[here^].

PAIO also encompasses a range of interfaces that serve as centralized hubs, offering real-time monitoring, collaboration, and decision-making capabilities across various stages of the machine learning operational pipeline. By providing insights into model performance, infrastructure health, and deployment status, these interfaces empower teams to optimize and enhance the reliability of their machine learning systems. 

image::MLOps_dashboard_model_servers.png[align=center]

Within the MLOps dashboard interface shown in the image above, a set of widgets and tables converge to offer valuable insights into the overall performance and usage of the serving infrastructurea and a comprehensive view of model servers and their deployments.

image::mlops-we-jobs.png[align=center]
image::mlops-we-services.png[align=center]

Within the ProActive platform, an integral facet is the Workflow Execution portal shown in the above images, that includes an ai-mlops-dashboard bucket. This bucket houses a collection of MLOps-oriented workflows designed to orchestrate and streamline various aspects of the model server lifecycle. These workflows encompass tasks spanning from the initialization and scaling of model servers to an inference workflow that recurrently generates predictions consuming deployed models and a performance analyzer workflow allowing thorough assessment and optimization of model performance, ensuring the seamless functioning of the entire machine learning operational pipeline.

The Workflow Execution portal also offers a seamless avenue to launch the MLOps dashboard and to initiate model servers as services. This integration streamlines the operational process by providing a unified interface for managing both the visualization and the execution aspects. Users can effortlessly trigger the deployment of model servers while simultaneously accessing the MLOps dashboard, fostering a cohesive and efficient management experience for the entire machine learning lifecycle.
