A *Node Source Infrastructure*, also called *Infrastructure Manager*, is responsible for deploying ProActive Nodes inside a defined <<_deploy_nodes_from_proactive_rm,*Node Source*>>. In most of the cases, it knows how to launch a set of ProActive Nodes
on a remote machine. For instance the SSH infrastructure manager connects via SSH to a remote machine and launches
nodes.

==== Default Infrastructure

*Default Infrastructure* is designed to be used with ProActive
agent. It cannot perform an automatic deployment but any users
(including an agent) can add already existing nodes into it. In order to
create a node source with this infrastructure, run the following
command:

    $ PROACTIVE_HOME/bin/proactive-client --createns defaultns -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.DefaultInfrastructureManager rmURL

The only parameter to provide is the following one:

-   **rmURL** - the URL of the Resource Manager.

==== Local Infrastructure

*Local Infrastructure* can be used to start nodes locally, i.e,
on the host running the Resource Manager. In order to create a node
source with this infrastructure, run the following command:

    $ PROACTIVE_HOME/bin/proactive-client --createns localns -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.LocalInfrastructure rmURL credentialsPath numberOfNodes timeout javaProperties

-   *rmURL* - URL of the Resource Manager server (can leave it empty to
    use default value, i.e, the URL of the Resource Manager you connect
    to)

-   *credentialsPath* - The absolute path of the credentials file used
    to set the provider of the nodes.

-   *numberOfNodes* - The number of nodes to deploy.

-   *timeout* - The length in ms after which one a node is not expected
    anymore.

-   *javaProperties* - The java properties to setup the ProActive
    environment for the nodes

==== SSH Infrastructure

This infrastructure allows to deploy nodes over SSH.
This infrastructure needs 10 arguments, described hereafter:

-   **RM URL** - The Resource Manager's URL that will be used by the
    deployed nodes to register by themselves.

-   **SSH Options** - Options you can pass to the SSHClient executable (
    -l inria to specify the user for instance )

-   **Java Path** - Path to the java executable on the remote hosts.

-   **Scheduling Path** - Path to the Scheduler installation
    directory on the remote hosts.

-   **Node Time Out** - A duration after which one the remote nodes are
    considered to be lost.

-   **Attempt** - The number of time the Resource Manager tries to
    acquire a node for which one the deployment fails before discarding
    it forever.
    
    **Wait time between failed attempts** - The time in milliseconds that the Resource Manager wait before trying to
    acquire a node for which one the deployment fails before.

-   **Target OS** - One of 'LINUX', 'CYGWIN' or 'WINDOWS' depending on
    the machines' ( in Hosts List file ) operating system.

-   **Java Options** - Java options appended to the command used to
    start the node on the remote host.

-   **RM Credentials Path** - The absolute path of the 'rm.cred' file to
    make the deployed nodes able to register to the Resource Manager (
    config/authentication/rm.cred ).

-   **Hosts List** - Path to a file containing the hosts on which
    resources should be acquired. This file should contain one host per
    line, described as a host name or a public IP address, optionally
    followed by a positive integer describing the number of runtimes to
    start on the related host (default to 1 if not specified). Example:

        rm.example.com
        test.example.net 5
        192.168.9.10 2

==== CLI Infrastructure

This generic infrastructure allows to deploy nodes using deployment
script written in arbitrary language. The infrastructure just launches
this script and waits until the ProActive node is registered in the
Resource Manager. Command line infrastructure could be used when you
prefer to describe the deployment process using shell scripts instead of
Java. Script examples can be found in
+PROACTIVE_HOME/samples/scripts/deployment+. The deployment script has 4
parameters: +HOST_NAME+, +NODE_NAME+, +NODE_SOURCE_NAME+, +RM_URL+. The
removal script has 2 parameters: +HOST_NAME+ and +NODE_NAME+.

This infrastructure needs 7 arguments, described hereafter:

-   **RM URL** - The Resource Manager's URL that will be used by the
    deployed nodes to register by themselves.

-   **Interpreter** - Path to the script interpreter (bash by default).

-   **Deployment Script** - A script that launches a ProActive node and
    register it to the RM.

-   **Removal Script** - A script that removes the node from the
    Resource Manager.

-   **Hosts List** - Path to a file containing the hosts on which
    resources should be acquired. This file should contain one host per
    line, described as a host name or a public IP address, optionally
    followed by a positive integer describing the number of runtimes to
    start on the related host (default to 1 if not specified). Example:

        rm.example.com
        test.example.net 5
        192.168.9.10 2

-   **Node Time Out** - The length in ms after which one a node is not
    expected anymore.

-   **Max Deployment Failure** - the number of times the resource
    manager tries to relaunch the deployment script in case of failure.

==== EC2 Infrastructure

The Elastic Compute Cloud, aka *EC2*, is an Amazon Web Service, that
allows its users to use machines (instances) on demand on the cloud. An
EC2 instance is a Xen virtual machine, running on different kinds of
hardware, at different prices, but always paid by the hour, allowing
lots of flexibility. Being virtual machines, instances can be launched
using custom operating system images, called *AMI* (Amazon Machine
Image). For the Resource Manager to use EC2 instances as computing
nodes, a specific EC2 Infrastructure as well as AMI creation utilities
are provided.

To use the EC2 Infrastructure in the Resource Manager, proper Amazon
credentials are needed. This section describes briefly how to obtain
them, and how to use them to configure your environment.

1.  First, you need to create an AWS account at
    <https://aws.amazon.com/>.

2.  With your new AWS account, sign up for EC2 at
    <https://aws.amazon.com/ec2/>.

3.  Now, you need to obtain the credentials. https://aws.amazon.com[On the AWS website],
    point your browser to the _Your Web Services Account_ button, a drop
    down list displays. Click _View Access Key Identifiers_.

4.  Use this information to fill in the properties *aws_key* (Access
    Key), *aws_secret_key* (Secret Key) in the *Create Node Source* panel located in the Resource Manager.
    Those two parameters should never change, except if you need for
    some reason to handle multiple EC2 accounts. Other properties in the
    *Create Node Source* are:
    
    -   *rmHostname:* The hostname or the public IP address of the Resource Manager. This address needs to be accessible from the AWS cloud.
    
    -   *connectorIaasURL:* Connector-iaas is a service embedded in the Scheduler used to interact with IaaS like AWS. By default it runs on the following URL *rmHostname/connector-iaas*.

    -   *image:* Defines which AMI will be used to create an AWS instances. The
        value to provide is the AWS region together with the unique AMI Id, for example: `eu-west-1/ami-bff32ccc`.

    -   *vmUsername:* Defines the user name that is used to connect to AWS instances.
        If not provided, then AWS instances will be accessed as admin user.

    -   *vmKeyPairName:* Defines the name of the AWS key pair to use.
        If specified, the key pair must exits in AWS in the region of deployment, and the `vmPrivateKey` must be specified as well.
        If not specified, then a default key pair will be created or reused in the given region of the deployment.

    -   *vmPrivateKey:* Defines the .pem file that will be used to connect to AWS instances.
        If specified, the name of the key pair (`vmKeyPairName`) to which this private key belongs to must be specified as well.
        
    -   *numberOfInstances:* Total number of AWS instances to create for this infrastructure.
    
    -   *numberOfNodesPerInstance:* Total number of Proactive Nodes to deploy in each created AWS instance.
    
+
TIP: If all the nodes of an AWS instance are removed, the instance will be terminated. For more information on the terminated state in AWS please see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html[AWS Terminating Instances].
+       
    -   *downloadCommand:* The command to download the Proactive *node.jar*. This command is executed in all the newly created AWS instances.
    	The full URL path of the *node.jar* to download, needs to be accessible from the AWS cloud.
        Example based on AWS image with windows operating system:
        
        	powershell -command "& { (New-Object Net.WebClient).DownloadFile('try.activeeon.com/rest/node.jar', 'node.jar') }"
    	    	
    -   *additionalProperties:* Additional Java command properties to be added when starting each ProActive node JVM in AWS instances (e.g. \"-Dpropertyname=propertyvalue\").
     
    -   *minRam:* The minimum required amount of RAM expressed in Mega Bytes for each AWS instance that needs to be created.
    
    -   *minCores:* The minimum required amount of virtual cores for each AWS instance that needs to be created.

+
WARNING: If the combination between RAM and CORES does not match any existing AWS instance type, then the closest to the specified parameters will be selected.

    -   *spotPrice:* The maximum price that you are willing to pay per hour per instance (your bid price). Amazon EC2 Spot instances allow you to bid on spare Amazon EC2 computing capacity. Since Spot instances are often available at a discount compared to On-Demand pricing. If your bid price is greater than the current Spot price for the specified instance, and the specified instance is available, your request is fulfilled immediately. Otherwise, the request is fulfilled whenever the Spot price falls below your bid price or the specified instance becomes available. Spot instances run until you terminate them or until Amazon EC2 must terminate them (also known as a Spot instance interruption). More information available on https://aws.amazon.com/ec2/spot/[AWS EC2 Spot]

    -   *securityGroupNames:* The securityGroupNames option allows you to specify the name(s) of the Security group(s) configured as a virtual firewall(s) to control inbound and outbound traffic for the EC2 instances hosting the proactive nodes. More information regarding Amazon EC2 Security Group available on https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html[AWS EC2 Security Groups]

    -   *subnetId:* The subnetId option allows you to launch the proactive nodes on EC2 instances, which will run into an existing subnet added to a specific Virtual Private Cloud. More information regarding Amazon EC2 Virtual Private Cloud (Amazon VPC) available on https://aws.amazon.com/vpc/[AWS EC2 Virtual Private Cloud] and Amazon EC2 Virtual Private Cloud and Subnet available on https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html[AWS EC2 Virtual Private Cloud and Subnet]

+
Using this configuration, you can start a Resource Manager and a
Scheduler using the */bin/proactive-server* script. An EC2 NodeSource can
now be added using the *Create Node Source* panel in the Resource Manager or the command line interface:

    $ PROACTIVE_HOME/bin/proactive-client --createns ec2 -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.AWSEC2Infrastructure aws_key aws_secret_key rmDomain connectorIaasURL image numberOfInstances numberOfNodesPerInstance downloadCommand additionalProperties minRam minCores

As AWS is a paying service, when the Scheduler is stopped normally (without removing the created infrastructure), all the created AWS instances will be terminated. And when the Scheduler is restarted, these instances will be re-configured as per the previous settings.

WARNING: If ProActive server is forcibly killed, the created AWS instances will not be terminated. And, when ProActive server is restarted, the infrastructure will be re-configured as per the previous settings. If the instances were deleted at the AWS side, they will be re-created and re-configured.



==== OpenStack Infrastructure

To use OpenStack instances as computing nodes, a specific OpenStack Infrastructure
can be created using the Resource Manager. This section describes briefly how to make it.

1.  First, you need to have an admin account on your OpenStack server. For more information see
https://docs.openstack.org/icehouse/install-guide/install/yum/content/keystone-users.html[OpenStack users and tenants].

2. The creation of OpenStack Infrastructure asks for an authentification to the OpenStack server and a deployment of instances that will host proactive nodes.

Use the proper admin username and password to fill in the properties *username* and *password* and perform the basic OpenStack authentification. Those two parameters should never change, except if you need for some reason to handle multiple OpenStack accounts.

For more information regarding OpenStack authentication mode see https://docs.openstack.org/security-guide/identity/authentication.html[OpenStack authentification mode]. 
    

Other properties needed for the authentification are :

   -   *domain:* The name of the domain to use that refers to the collection of projects and users defining administrative boundaries for managing Identity entities. For more information see https://docs.openstack.org/security-guide/identity/domains.html[OpenStack domain].


  -   *endpoint:* The hostname or the IP address of the OpenStack server. This address needs to be accessible from the Resource Manager.
    

   -   *scopePrefix:* The scope prefix to use. It can be project, projectId, domain or domainId.


   -   *scopeValue:* The value of the scope prefix. 


   -   *region:* The Region for networks and compute resources to use.


   -   *identityVersion:* The REST API version of OpenStack installation. For more information see https://docs.openstack.org/keystone/pike/contributor/http-api.html[OpenStack API Version]. 

Properties needed for the deployment of instance are :  

   -   *image:* Defines which image will be used to create the OpenStack instance. The value to provide is the unique image Id.
   
   -   *flavor:* Defines the size of the instance. The value to provide is the flavor name or the flavor Id. For more information see https://docs.openstack.org/openstack-ops/content/flavors.html[OpenStack flavors].
    
    -   *publicKeyName:* Defines the name of the public key to use for a remote connection when the instance is created.
    
+
WARNING: In order to use publicKeyName, the key pair needs to be created and imported first on the OpenStack server. For more information see https://docs.openstack.org/python-openstackclient/pike/cli/command-objects/keypair.html[OpenStack key pair management].
+
 
        
    -   *numberOfInstances:* Total number of OpenStack instances to create for this infrastructure.
    
    -   *numberOfNodesPerInstance:* Total number of Proactive Nodes to deploy in each OpenStack created instance.
    
+
TIP: If all the nodes of an OpenStack instance are removed, the instance will be terminated.
+


Other properties for the node deployment in the *Create OpenStack Node Source* are:

    -   *connectorIaasURL:* Connector-iaas is a ProActive service used to interact with IaaS like OpenStack. By default it runs on the following URL *rmHostname/connector-iaas*.

    -   *rmHostname:* The hostname or the public IP address of the Resource Manager. This address needs to be accessible from the OpenStack server.

    -   *downloadCommand:* The command to download the Proactive *node.jar*. This command is executed in all the newly created OpenStack instances.
    	The full URL path of the *node.jar* to download needs to be accessible from the OpenStack cloud.

    	    	
    -   *additionalProperties:* Additional Java command properties to be added when starting each ProActive node JVM in OpenStack instances (e.g. \"-Dpropertyname=propertyvalue\").
    

Using this configuration, you can start a Resource Manager and a
Scheduler using the */bin/proactive-server* script. An OpenStack NodeSource can
now be added using the *Create Node Source* panel in the Resource Manager or the command line interface:

    $ PROACTIVE_HOME/bin/proactive-client --createns openstack -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.OpenStackInfrastructure username password endpoint rmHostname connectorIaasURL image flavor publicKeyName numberOfInstances numberOfNodesPerInstance downloadCommand additionalProperties


WARNING: When ProActive server is stopped (without removing the created infrastructure), OpenStack instances will not be terminated. And when ProActive server is restarted, the infrastrucutre will be re-configured as per the previous settings. If the instances were deleted at the OpenStack Cloud side, they will be re-created and re-configured.



==== VMware Infrastructure

To use VMware instances as computing nodes, a specific VMware Infrastructure
can be created using the Resource Manager. This section describes briefly how to make it.

1.  First, you need to have an admin account on your VMware server.For more information see
https://pubs.vmware.com/vsphere-51/topic/com.vmware.vsphere.security.doc/GUID-670B9B8C-3810-4790-AC83-57142A9FE16F.html[VMware users].

2.  Use the login and password information to fill in the properties *vmware_username*,
    *vmware_password* in the *Create Node Source* panel located in the Resource Manager.
    Those two parameters should never change, except if you need for
    some reason to handle multiple VMware accounts. Other properties in the
    *Create Node Source* are:
    
    -   *endpoint:* The hostname or the IP address of the VMware server. This address needs to be accessible from the Resource Manager.
    
    -   *rmHostname:* The hostname or the public IP address of the Resource Manager. This address needs to be accessible from the VMware server.
    
    -   *connectorIaasURL:* Connector-iaas is a service embedded in the Scheduler used to interact with IaaS like VMware. By default it runs on the following URL *rmHostname/connector-iaas*.

    -   *image:* Defines which image will be used to create the VMware instance. The
        value to provide is the VMware folder together with the unique image Id, for example: `ActiveEon/ubuntu`.
        
    -   *minRam:* The minimum required amount of RAM expressed in Mega Bytes for each VMware instance that needs to be created.
    
    -   *minCores:* The minimum required amount of virtual cores for each VMware instance that needs to be created.
    
+
WARNING: If the combination between RAM and CORES does not match any existing VMware instance type, then the closest to the specified parameters will be selected.
+

    -   *vmUsername:* Defines the username to log in the instance when it is created.
    
    -   *vmPassword:* Defines the password to log in the instance when it is created.
    
+
WARNING: The username and password are related to the image.
+
        
    -   *numberOfInstances:* Total number of VMware instances to create for this infrastructure.
    
    -   *numberOfNodesPerInstance:* Total number of Proactive Nodes to deploy in each VMware created instance.
    
+
TIP: If all the nodes of an VMware instance are removed, the instance will be terminated.
+

       
    -   *downloadCommand:* The command to download the Proactive *node.jar*. This command is executed in all the newly created VMware instances.
    	The full URL path of the *node.jar* to download, needs to be accessible from the VMware cloud.
    	    	
    -   *additionalProperties:* Additional Java command properties to be added when starting each ProActive node JVM in VMware instances (e.g. \"-Dpropertyname=propertyvalue\").
    

Using this configuration, you can start a Resource Manager and a
Scheduler using the */bin/proactive-server* script. An VMware NodeSource can
now be added using the *Create Node Source* panel in the Resource Manager or the command line interface:

    $ PROACTIVE_HOME/bin/proactive-client --createns vmware -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.VmwareInfrastructure username password endpoint rmHostname connectorIaasURL image ram cores vmusername vmpassword numberOfInstances numberOfNodesPerInstance downloadCommand additionalProperties


WARNING: When ProActive server is stopped (without removing the created infrastructure), VMware instances will not be terminated. And when ProActive server is restarted, the infrastrucutre will be re-configured as per the previous settings. If the instances were deleted at the VMware server side, they will be re-created and re-configured.

==== GCE Infrastructure

Google Compute Engine, aka GCE, delivers virtual machines running on Google's infrastructure.
To use GCE virtual machines as computing nodes, a specific GCE Infrastructure needs to be created using the Resource Manager.
This section describes briefly how to make it.

===== Pre-Requisites

First, to use the GCE Infrastructure in the Resource Manager, proper Google Cloud credentials are needed for an authentication to the Google Cloud Platform APIs.
To obtain them, you can take the following steps:

1. Go to the https://console.developers.google.com[Developer Console].
2. Log in with an account which has the permissions to create service accounts and service account keys (i.e., granted the _Service Account Admin_ and _Service Account Key Admin_ role).
3. Following the document https://cloud.google.com/iam/docs/creating-managing-service-accounts#creating_a_service_account[Creating a service account] to create a service account *granted the _Compute Admin_ role*
4. Following the document https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys[Creating service account keys] to create a service key of the type *_JSON_*, a JSON file for the created service account key should be downloaded to your machine.

For more information regarding Google Cloud service accounts see
https://cloud.google.com/compute/docs/access/service-accounts[Google Cloud Service Accounts].

===== Infrastructure Configuration

Now, you are ready to create a new node source of the type _GCE Infrastructure_.
The properties needed for the node deployment in the _Create GCE Node Source_ are :

-   *gceCredential:* The credentials to perform the basic Google Cloud Platform authentication.
        Upload the JSON file of a Google Cloud service account key (downloaded in the section Pre-Requisites) to fill in this property.
-   *totalNumberOfInstances:* Total number of GoogleComputeEngine instances to create for this infrastructure.
-   *numberOfNodesPerInstance:* Total number of Proactive Nodes to deploy in each created GoogleComputeEngine instance.
+
TIP: If all the nodes of an GoogleComputeEngine instance are removed, the instance will be terminated.
+
-   *vmUsername*: Defines the user name that will be used to connect to GoogleComputeEngine instances.
        If not provided, then GoogleComputeEngine instances will be accessed as the default user.
        If specified, the corresponding `vmPublicKey` and `vmPrivateKey` must be specified as well.
-   *vmPublicKey*: Defines the public key to grant a user specified access for the created GoogleComputeEngine instances.
        If specified, the corresponding `vmUsername` and `vmPrivateKey` must be specified as well.
-   *vmPrivateKey*: Defines the private key that will be used to connect to GoogleComputeEngine instances.
        If specified, the corresponding `vmUsername` and `vmPublicKey` must be specified as well.
-   *image*: Defines which image will be used to create the GoogleComputeEngine instance.
        The value to provide is the unique name of the image.
        If not provided, the default value "debian-9-stretch-v20190326" will be used.
        For more information see https://cloud.google.com/compute/docs/images/[Google Compute Engine Images List].
-   *region*: The geographic zone for Google Cloud Platform resources to use.
        If not provided, the default value "us-central1-a" will be used.
        For more information see https://cloud.google.com/compute/docs/regions-zones/[Google Compute Engine Regions and Zones].
-   *ram*: The minimum required amount of RAM (expressed in Mega Bytes) for each GoogleComputeEngine instance to be created.
        If not provided, the default value 1740 will be used.
-   *cores*: The minimum required amount of virtual cores for each GoogleComputeEngine instance to be created.
        If not provided, the default value 1 will be used.
-   *rmHostname*: The hostname or the public IP address of the Resource Manager. This address needs to be accessible from the GoogleComputeEngine server.
-   *connectorIaasURL*: Connector-iaas is a ProActive service used to interact with IaaS like GoogleComputeEngine.
        By default it runs on the following URL _rmHostname/connector-iaas_.
-   *nodeJarURL*: The full URL path of the _node.jar_ to download the Proactive node.jar on each new created GoogleComputeEngine instance.
        The URL needs to be accessible from the GoogleComputeEngine server.
-   *additionalProperties*: Additional Java command properties to be added when starting each ProActive node JVM in GoogleComputeEngine instances (e.g. "-Dpropertyname=propertyvalue").
-   *nodeTimeout*: The estimated startup time of the nodes (expressed in millisecond). After this timeout expired, the node is considered as lost.

Using this configuration, you can start a Resource Manager and a Scheduler using the */bin/proactive-server* script.
An GoogleComputeEngine NodeSource can now be added using the *Create Node Source* panel in the Resource Manager or the command line interface:

    $ PROACTIVE_HOME/bin/proactive-client --createns googlecomputeengine -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.GCEInfrastructure gceCredential totalNumberOfInstances numberOfNodesPerInstance vmUsername vmPublicKey vmPrivateKey image region ram cores rmHostname connectorIaasURL nodeJarURL additionalProperties nodeTimeout

WARNING: When ProActive server is stopped, GoogleComputeEngine instances will be automatically deleted. And when ProActive server is restarted, the infrastructure will be recovered as per the previous settings. The required GoogleComputeEngine instances will be re-created and re-configured.


==== Load Sharing Facility (LSF) Infrastructure

This infrastructure knows how to acquire nodes from LSF by submitting a
corresponding job. It will be submitted through SSH from the RM to the
LSF server. This is the static version of the LSF infrastructure, for a more dynamic mechanism, as described in <<_deploy_via_other_schedulers>>, use the <<_native_scheduler_infrastructure>> instead.

    $ PROACTIVE_HOME/bin/proactive-client --createns lsf -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.LSFInfrastructure rmURL javaPath SSHOptions schedulingPath javaOptions maxNodes nodeTimeout LSFServer RMCredentialsPath bsubOptions

where:

-   **RMURL** - URL of the Resource Manager from the LSF nodes point of
  view - this is the URL the nodes will try to lookup when attempting
  to register to the RM after their creation.

-   **javaPath** - path to the java executable on the remote hosts (ie
  the LSF slaves).

-   **SSH Options** - Options you can pass to the SSHClient executable (
  -l inria to specify the user for instance )

-   **schedulingPath** - path to the Scheduling/RM installation
  directory on the remote hosts.

-   **javaOptions** - Java options appended to the command used to start
  the node on the remote host.

-   **maxNodes** - maximum number of nodes this infrastructure can
  simultaneously hold from the LSF server. That is useful considering
  that LSF does not provide a mechanism to evaluate the number of
  currently available or idle cores on the cluster. This can result to
  asking more resources than physically available, and waiting for the
  resources to come up for a very long time as the request would be
  queued until satisfiable.

-   **Node Time Out** - The length in ms after which one a node is not
  expected anymore.

-   **Server Name** - URL of the LSF server, which is responsible for
  acquiring LSF nodes. This server will be contacted by the Resource
  Manager through an SSH connection.

-   **RM Credentials Path** - Encrypted credentials file, as created by
  the create-cred[.bat] utility. These credentials will be used by the
  nodes to authenticate on the Resource Manager.

-   **Submit Job Opt** - Options for the bsub command client when
  acquiring nodes on the LSF master. Default value should be enough in
  most cases, if not, refer to the documentation of the LSF cluster.

==== Portable Batch System (PBS) Infrastructure

This infrastructure knows how to acquire nodes from PBS (i.e. Torque) by
submitting a corresponding job. It will be submitted through SSH from
the RM to the PBS server. This is the static version of the PBS infrastructure, for a more dynamic mechanism, as described in <<_deploy_via_other_schedulers>>, use the <<_native_scheduler_infrastructure>> instead.

    $ PROACTIVE_HOME/bin/proactive-client --createns pbs -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.PBSInfrastructure rmURL javaPath SSHOptions schedulingPath javaOptions maxNodes nodeTimeout PBSServer RMCredentialsPath qsubOptions

where:

-   **RMURL** - URL of the Resource Manager from the PBS nodes point of
    view - this is the URL the nodes will try to lookup when attempting
    to register to the RM after their creation.

-   **javaPath** - path to the java executable on the remote hosts (ie
    the PBS slaves).

-   **SSH Options** - Options you can pass to the SSHClient executable (
    -l inria to specify the user for instance )

-   **schedulingPath** - path to the Scheduling/RM installation
    directory on the remote hosts.

-   **javaOptions** - Java options appended to the command used to start
    the node on the remote host.

-   **maxNodes** - maximum number of nodes this infrastructure can
    simultaneously hold from the PBS server. That is useful considering
    that PBS does not provide a mechanism to evaluate the number of
    currently available or idle cores on the cluster. This can result to
    asking more resources than physically available, and waiting for the
    resources to come up for a very long time as the request would be
    queued until satisfiable.

-   **Node Time Out** - The length in ms after which one a node is not
    expected anymore.

-   **Server Name** - URL of the PBS server, which is responsible for
    acquiring PBS nodes. This server will be contacted by the Resource
    Manager through an SSH connection.

-   **RM Credentials Path** - Encrypted credentials file, as created by
    the create-cred[.bat] utility. These credentials will be used by the
    nodes to authenticate on the Resource Manager.

-   **Submit Job Opt** - Options for the qsub command client when
    acquiring nodes on the PBS master. Default value should be enough in
    most cases, if not, refer to the documentation of the PBS cluster.


==== Generic Batch Job Infrastructure

*Generic Batch Job infrastructure* provides users with the capability to
add the support of new batch job scheduler by providing a class
extending
org.ow2.proactive.resourcemanager.nodesource.infrastructure.BatchJobInfrastructure.
Once you have written that implementation, you can create a node source
which makes usage of this infrastructure by running the following
command:

    $ PROACTIVE_HOME/bin/proactive-client --createns pbs -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.GenericBatchJobInfrastructure rmURL javaPath SSHOptions schedulingPath javaOptions maxNodes nodeTimeout BatchJobServer RMCredentialsPath subOptions implementationName implementationPath

where:

-   **RMURL** - URL of the Resource Manager from the batch job scheduler
    nodes point of view - this is the URL the nodes will try to lookup
    when attempting to register to the RM after their creation.

-   **javaPath** - path to the java executable on the remote hosts (ie
    the slaves of the batch job scheduler).

-   **SSH Options** - Options you can pass to the SSHClient executable (
    -l inria to specify the user for instance )

-   **schedulingPath** - path to the Scheduling/RM installation
    directory on the remote hosts.

-   **javaOptions** - Java options appended to the command used to start
    the node on the remote host.

-   **maxNodes** - maximum number of nodes this infrastructure can
    simultaneously hold from the batch job scheduler server.

-   **Node Time Out** - The length in ms after which one a node is not
    expected anymore.

-   **Server Name** - URL of the batch job scheduler server, which is
    responsible for acquiring nodes. This server will be contacted by
    the Resource Manager through an SSH connection.

-   **RM Credentials Path** - Encrypted credentials file, as created by
    the create-cred[.bat] utility. These credentials will be used by the
    nodes to authenticate on the Resource Manager.

-   **Submit Job Opt** - Options for the submit command client when
    acquiring nodes on the batch job scheduler master.

-   **implementationName** - Fully qualified name of the implementation
    of
    org.ow2.proactive.resourcemanager.nodesource.infrastructure.BatchJobInfrastructure
    provided by the end user.

-   **implementationPath** - The absolute path of the implementation of
    org.ow2.proactive.resourcemanager.nodesource.infrastructure.BatchJobInfrastructure.

==== Native Scheduler Infrastructure

The Native Scheduler Infrastructure allows to interact with a <<_glossary_ns_native_scheduler,native scheduler>> to deploy ProActive Nodes.
This mechanism is described in <<_deploy_via_other_schedulers>>. This infrastructure must be associated with a <<_native_scheduler_policy>> and cannot be associated with any other policy.
The infrastructure parameters are described hereafter:

 * *RMCredentialsPath* : path to a file which contains the credentials of an administrator user which will own the node source. The ProActive Scheduler Server release contains two admin users credentials files : `config/authentication/rm.cred` and `config/authentication/admin_user.cred`
 * *NSFrontalHostAddress* : the host name or IP address of the cluster <<_glossary_head_node,head node>>.
 * *NSSchedulerHome* : the location of the shared ProActive installation on <<_glossary_ns_node,cluster nodes>> (cluster nodes must be able to access ProActive libraries in order to start ProActive Node). Example `/opt/proactive/activeeon_enterprise-node-linux-x64-8.1.0`.
 * *javaHome* : similarly, cluster nodes must be able to access the java command in order to start ProActive Nodes. ProActive installation includes a Java Runtime Environment under the `jre` subfolder. Example: `/opt/proactive/activeeon_enterprise-node-linux-x64-8.1.0/jre`.
 * *jvmParameters* : additional options which can be passed to the java command.
 * *sshOptions* : additional options which can be passed to the ssh command used to connect to connect to the host name or IP address specified in the NSFrontalHostAddress parameter.
 * *NSNodeTimeoutInSeconds* : timeout to wait for the deployment of ProActive Nodes on the cluster. As the time needed to deploy ProActive Nodes depends on the cluster load, this timeout should be a large value. If the timeout is reached, the ProActive Nodes will be in `"Lost"` <<_node_states,state>>.
 * *ìmpersonationMethod*: when a job is submitted to the native scheduler, the submission is performed under the current <<_glossary_proactive_scheduler_user,ProActive Scheduler user>>. An impersonation is thus performed between the <<_glossary_proactive_scheduler_process_user,scheduler server process>> and the target <<_glossary_cluster_user,cluster user>>.
 This impersonation can be performed using 3 different strategies:
 ** `ssh`: in that case the <<_glossary_head_node,head node>> is contacted using a ssh command with the current <<_glossary_proactive_scheduler_user,ProActive Scheduler user>> and password. User/password combination between the ProActive Scheduler and the head node operating system must match.
 ** `none`: in that case the head node is contacted using a ssh command with the <<_glossary_proactive_scheduler_process_user,ProActive Scheduler process user>> (passwordless ssh). Submission to the native scheduler will be performed with the same account.
 ** `sudo`: similar to `none` regarding the connection to the head node, but a `sudo` command will be initiated to impersonate as the current <<_glossary_proactive_scheduler_user,ProActive Scheduler user>>, before doing a job submission.
 * *alternateRMUrl* : the url used by the ProActive Nodes to contact <<_glossary_resource_manager,ProActive Resource Manager>>. This url is displayed on ProActive server startup. Example: `pnp://myserver:64738`.
 * *sshPort* : port used for ssh connections.
 * *nsPreCommand* : a linux command which can be run before launching ProActive Nodes on the cluster. Can be used as a workaround when some system environment variables are not properly set when starting ProActive Nodes.
 * *nsSubmitCommand* : this is the main command used to start ProActive Nodes on the cluster. Depending on the actual native scheduler implementation, *nsSubmitCommand* will vary, here are examples definitions: +
+
[cols=2*]
|===

|PBS |`qsub -N %NS_JOBNAME% -o %LOG_FILE% -j oe`

|SLURM |`sbatch -J %NS_JOBNAME% -o %LOG_FILE%`

|LSF |`bsub -J %NS_JOBNAME% -o %LOG_FILE% -e %LOG_FILE%`

|===
+
The command can use patterns which will be replaced dynamically by the ProActive Resource Manager. +
+
[cols=2*]
|===

|`%NS_JOBNAME%` |contains a configurable job name dynamically created by the resource manager.

|`%LOG_FILE%` |contains a log file path dynamically created by the resource manager and located in side the NSSchedulerHome installation. This log file is useful to debug errors during <<_glossary_ns_cluster_job,cluster job>> submission.

|`%PA_USERNAME%` |contains the current ProActive Scheduler user.
|===

 * *nsKillCommand* : this is the command used to kill ProActive Nodes started previously by the nsSubmitCommand. Similarly to nsSubmitCommand, *nsKillCommand* will vary for each native scheduler syntax: +
+
[cols=2*]
|===

|PBS |`qdel %NS_JOBID%`

|SLURM |`scancel -n %NS_JOBNAME%`

|LSF |`bkill -J %NS_JOBNAME%`

|===
+
It can use the following patterns: +
+
[cols=2*]
|===

|`%NS_JOBNAME%` |contains a configurable job name dynamically created by the resource manager.

|`%NS_JOBID%` |contains the job id returned by the native scheduler when submitting the job. Currently, job id can only be used with PBS, when the setting `submitReturnsJobId` is set to `true`.

|===

 * *submitReturnsJobId*: is the <<_glossary_ns_cluster_jobid,cluster job id>> returned plainly when calling the nsSubmitCommand. This is the behavior of PBS, and this is why this setting should be set to `true` when using PBS.
 * *nsJobName*: a way to configure the `%NS_JOBNAME%` pattern. The following patterns can be used: +
+
[cols=2*]
|===

|`%PA_TASKID%` |contains the ProActive Task and Job ID associated with the node request.

|`%PA_USERNAME%` |contains the current ProActive Scheduler user.

|===

 * *maxDeploymentFailure*: number of attempts when starting a ProActive Node on the cluster using the nsSubmitCommand, after all attempts failed, the ProActive Node will be declared as `Lost`.

