
NOTE: New Node Source policies can be dynamically plugged into the Resource Manager. In order to do that, it is required to add classes extending the link:../javadoc/org/ow2/proactive/resourcemanager/nodesource/policy/NodeSourcePolicy.html[NodeSourcePolicy class] in the server class path and update the policy implementation list in the configuration file (+PROACTIVE_HOME/config/rm/nodesource/policies+).

==== Static Policy

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.StaticPolicy`

*Static node source policy* starts node acquisition when nodes are added
to the node source and never removes them. Nevertheless, nodes can be
removed by user request. To use this policy, you have to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

==== Restart Down Node Policy

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.RestartDownNodePolicy`

*Restart down node policy* automatically redeploy nodes when they are disconnected from the Resource Manager (for example after a network failure or a machine reboot).
To use this policy, you have to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

-   *checkNodeStateEach* - interval in milliseconds used to check nodes availability. For example, if this interval is set to 30 minutes the policy will redeploy nodes after a disconnection of 30 minutes.

==== Time Slot Policy

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.TimeSlotPolicy`

*Time slot policy* is aimed to acquire nodes for a specific time window with an
ability to do it periodically. To use this policy, you have to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

-   *acquireTime* - Absolute acquire date (e.g. "6/3/10 1:18:45 PM
    CEST").

-   *releaseTime* - Absolute releasing date (e.g. "6/3/10 2:18:45 PM
    CEST").

-   *period* - Period time in millisecond (default is 86400000).

-   *preemptive* - Preemptive parameter indicates the way of releasing
    nodes. If it is true, nodes will be released without waiting the end
    of jobs running on (default is false).

==== Cron Policy

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.CronPolicy`

*Cron policy* is aimed to acquire and remove nodes at a specific time
defined in the cron syntax. To use this policy, you have to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

-   *nodeAcquision* - The time policy will trigger the deployment of
    all nodes (e.g. "0 12 \* \* \*" every day at 12.00).

-   *nodeRemoval* - The time policy will trigger the removal of all
    nodes (e.g. "0 13 \* \* \*" every day at 13.00).

-   *preemptive* - Preemptive parameter indicates the way of releasing
    nodes. If it is true, nodes will be released without waiting the end
    of jobs running on (default is false).

-   *forceDeployment* - If for the example above (the deployment
    starts every day at 12.00 and the removal starts at 13.00) you are
    creating the node source at 12.30 the next deployment will take
    place the next day. If you'd like to force the immediate deployment
    set this parameter to true.

==== Release Resources When Scheduler Is Idle

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.ReleaseResourcesWhenSchedulerIdle`

*"Release resources when scheduler is idle" policy* removes all nodes from
the infrastructure when the scheduler is idle and acquires them when a
new job is submitted. This policy may be useful if there is no need to
keep nodes alive permanently. Nodes will be released after a specified
"idle time". This policy will use a listener of the scheduler, that is
why its URL, its user name and its password have to be specified.
To use this policy, you have to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

-   *schedulerURL* - URL of the Scheduler, e.g. pnp://mymachine:64738, pamr://0, etc

-   *schedulerCredentialsPath* - Path to the credentials used for
    scheduler authentication.

-   *schedulerAwarePolicyNodeSourceRecoveryDelay* - Delay in ms for the Resource Manager to recover a broken node source using the ReleaseResourcesWhenSchedulerIdle policy.

-   *schedulerAwarePolicyNodeSourceRecoveryTrialsNumber* - Number of attempts for the Resource Manager to recover to recover a broken node source using the ReleaseResourcesWhenSchedulerIdle policy.

-   *idleTime* - Idle time in millisecond to wait before removing all
    nodes (default is 60000).


==== Scheduler Loading Policy

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.SchedulerLoadingPolicy`

*Scheduler loading policy* acquires/releases nodes according to the
scheduler loading factor. This policy allows to configure the number of
resources which will be always enough for the scheduler. Nodes are
acquired and released according to scheduler loading factor which is a
number of tasks per node.

It is important to correctly configure maximum and minimum nodes that this
policy will try to hold. Maximum number should not be greater than
potential nodes number which is possible to deploy to underlying
infrastructure. If there are more currently acquired nodes than
necessary, policy will release them one by one after having waited for a
"release period" delay. This smooth release procedure is implemented
because deployment time is greater than the release one. Thus, this
waiting time deters policy from spending all its time trying to deploy
nodes.

To use this policy, you have to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

-   *schedulerURL* - URL of the Scheduler, e.g. pnp://mymachine:64738, pamr://0, etc

-   *schedulerCredentialsPath* - Path to the credentials used for
    scheduler authentication.

-   *schedulerAwarePolicyNodeSourceRecoveryDelay* - Delay in ms for the Resource Manager to recover a broken node source using the SchedulerLoadingPolicy.

-   *schedulerAwarePolicyNodeSourceRecoveryTrialsNumber* - Number of attempts for the Resource Manager to recover to recover a broken node source using the SchedulerLoadingPolicy.

-   *refreshTime* - Time between each calculation of the number of needed
nodes.

-   *minNodes* - Minimum number of nodes to deploy

-   *maxNodes* - Maximum number of nodes to deploy

-   *loadFactor* - Number of tasks per node. Actually, if this number is
*N*, it does not means that there will be exactly *N* tasks executed on
each node. This factor is just used to compute the total number of
nodes. For instance, let us assume that this factor is 3 and that we
schedule 100 tasks. In that case, we will have 34 (= upper bound of
100/3) started nodes. Once one task finished and the refresh time
passed, one node will be removed since 99 divided by 3 is 33. When there
will remain 96 tasks (assuming that no other tasks are scheduled
meanwhile), an other node will be removed at the next calculation time,
and so on and so forth...

-   *refreshCycleWindow* - Number of refresh cycles used to memorize scheduler load. Actual load will be computed as an average.

-   *releaseDelay* - Delay in milliseconds used before removing a free node. This parameter can be used for paid cloud instances such as Amazon, Azure, etc. when their policy define a minimum granularity on instance costs. Example for Amazon : 3600000 ms (1 hour). Default is 0 (not a paid instance, no delay).

-   *threshold* - Nodes can be released if the current time is in interval [releaseDelay - threshold, releaseDelay]. Default is 0 (not a paid instance, no delay). Related to releaseDelay. Example for amazon : 600000 ms (10 minutes).

==== Cron Load Based Policy

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.CronLoadBasedPolicy`

The *Cron load based policy* triggers new nodes acquisition when
scheduler is overloaded (exactly like with *"Scheduler loading" policy*)
only within a time slot defined using crontab syntax. All other time the
nodes are removed from the Resource Manager.
To use this policy, you have to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

-   *schedulerURL* - URL of the Scheduler, e.g. pnp://mymachine:64738, pamr://0, etc

-   *schedulerCredentialsPath* - Path to the credentials used for
    scheduler authentication.

-   *schedulerAwarePolicyNodeSourceRecoveryDelay* - Delay in ms for the Resource Manager to recover a broken node source using the CronLoadBasedPolicy.

-   *schedulerAwarePolicyNodeSourceRecoveryTrialsNumber* - Number of attempts for the Resource Manager to recover to recover a broken node source using the CronLoadBasedPolicy.

-   *refreshTime* - Time between each calculation of the number of
    needed nodes.

-   *minNodes* - Minimum number of nodes to deploy

-   *maxNodes* - Maximum number of nodes to deploy

-   *loadFactor* - Number of tasks per node. Actually, if this number is
*N*, it does not means that there will be exactly *N* tasks executed on
each node. This factor is just used to compute the total number of
nodes. For instance, let us assume that this factor is 3 and that we
schedule 100 tasks. In that case, we will have 34 (= upper bound of
100/3) started nodes. Once one task finished and the refresh time
passed, one node will be removed since 99 divided by 3 is 33. When there
will remain 96 tasks (assuming that no other tasks are scheduled
meanwhile), an other node will be removed at the next calculation time,
and so on and so forth...

-   *refreshCycleWindow* - Number of refresh cycles used to memorize scheduler load. Actual load will be computed as an average.

-   *releaseDelay* - Delay in milliseconds used before removing a free node. This parameter can be used for paid cloud instances such as Amazon, Azure, etc. when their policy define a minimum granularity on instance costs. Example for Amazon : 3600000 ms (1 hour). Default is 0 (not a paid instance, no delay).

-   *threshold* - Nodes can be released if the current time is in interval [releaseDelay - threshold, releaseDelay]. Default is 0 (not a paid instance, no delay). Related to releaseDelay. Example for amazon : 600000 ms (10 minutes).

-   *acquisionAllowed* - The time when the policy starts to work as the
*"scheduler loading" policy* (e.g. "0 12 \* \* \*" every day at 12.00).

-   *acquisionForbidden* - The time policy removes all the nodes from the
Resource Manager (e.g. "0 13 \* \* \*" every day at 13.00).

-   *preemptive* - Preemptive parameter indicates the way of releasing
nodes. If it is true, nodes will be released without waiting the end of
jobs running on (default is false).

-   *allowed* - If true acquisition will be immediately allowed.

==== Cron Slot Load Based Policy

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.CronSlotLoadBasedPolicy`

The *"Cron slot load based" policy* triggers new nodes acquisition when
scheduler is overloaded (exactly like with *"Scheduler loading" policy*)
only within a time slot defined using crontab syntax. The other time it
holds all the available nodes.
To use this policy, you have to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

-   *schedulerURL* - URL of the Scheduler, e.g. pnp://mymachine:64738, pamr://0, etc

-   *schedulerCredentialsPath* - Path to the credentials used for
scheduler authentication.

-   *schedulerAwarePolicyNodeSourceRecoveryDelay* - Delay in ms for the Resource Manager to recover a broken node source using the CronSlotLoadBasedPolicy.

-   *schedulerAwarePolicyNodeSourceRecoveryTrialsNumber* - Number of attempts for the Resource Manager to recover to recover a broken node source using the CronSlotLoadBasedPolicy.

-   *refreshTime* - Time between each calculation of the number of needed
nodes.

-   *minNodes* - Minimum number of nodes to deploy

-   *maxNodes* - Maximum number of nodes to deploy

-   *loadFactor* - number of tasks per node. Actually, if this number is
*N*, it does not means that there will be exactly *N* tasks executed on
each node. This factor is just used to compute the total number of
nodes. For instance, let us assume that this factor is 3 and that we
schedule 100 tasks. In that case, we will have 34 (= upper bound of
100/3) started nodes. Once one task finished and the refresh time
passed, one node will be removed since 99 divided by 3 is 33. When there
will remain 96 tasks (assuming that no other tasks are scheduled
meanwhile), an other node will be removed at the next calculation time,
and so on and so forth...

-   *refreshCycleWindow* - Number of refresh cycles used to memorize scheduler load. Actual load will be computed as an average.

-   *releaseDelay* - Delay in milliseconds used before removing a free node. This parameter can be used for paid cloud instances such as Amazon, Azure, etc. when their policy define a minimum granularity on instance costs. Example for Amazon : 3600000 ms (1 hour). Default is 0 (not a paid instance, no delay).

-   *threshold* - Nodes can be released if the current time is in interval [releaseDelay - threshold, releaseDelay]. Default is 0 (not a paid instance, no delay). Related to releaseDelay. Example for amazon : 600000 ms (10 minutes).

-   *deployAllAt* - Time when all nodes are deployed (crontab format) (e.g. "0 12 \* \* \*" every day at 12.00).

-   *undeployAllAt* - Time when all nodes are removed and the policy starts watching the scheduler load (e.g. "0 13 \* \* \*" every day at 13.00).

-   *preemptive* - Preemptive parameter indicates the way of releasing
nodes. If it is true, nodes will be released without waiting the end of
tasks running on (default is false).

-   *acquireNow* - If true, the policy will acquire all nodes immediately.

==== Native Scheduler Policy

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.NativeSchedulerPolicy`

The Native Scheduler Policy interacts with the <<_glossary_ns_native_scheduler,native scheduler>> to request ProActive nodes deployment dynamically based on the <<_glossary_proactive_scheduler,*ProActive Scheduler*>> pending queue.
This mechanism is described in <<_deploy_via_other_schedulers>>. This policy must be associated with a <<_native_scheduler_infrastructure>> and cannot be associated with any other infrastructure.
To use this policy, you need to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

-   *schedulerUrl* - URL of the Scheduler, e.g. pnp://mymachine:64738, pamr://0, etc

-   *schedulerCredentialsPath* - Path to a file which contains the credentials of an administrator user which will connect to the scheduler. The ProActive Scheduler Server release contains two admin users credentials files : `config/authentication/rm.cred` and `config/authentication/admin_user.cred`

-   *rearrangeTasks* - Currently not implemented.

-   *autoScaling* - If set to `true`, the NativeSchedulerPolicy will scan the Resource Manager activity and Scheduler queue. If the scheduler queue is not empty and all Resource Manager nodes are busy, `autoscaling` will automatically start ProActive Nodes from the NativeSchedulerInfrastructure. This setting cannot be used when multiple NativeScheduler node sources are deployed.

-   *refreshTime* -  The NativeSchedulerPolicy will refresh its status and observe the ProActive Scheduler queue every `refreshTime` milliseconds.

==== Dynamic Policy

*Full name:* `org.ow2.proactive.resourcemanager.nodesource.policy.DynamicPolicy`

The *Dynamic policy* is similar to the <<_scheduler_loading_policy>> in the sense that it can acquire/release nodes according to the
scheduler load. The *Dynamic policy* is more advanced and gives a fuller range of configuration parameters which allow to:

 - filter the scheduler load to consider specific tasks which use the <<../user/ProActiveUserGuide.adoc#_node_source_generic_info,NODE_SOURCE>> or the (deprecated) <<../user/ProActiveUserGuide.adoc#_nodesourcename,NODESOURCENAME>> <<_glossary_generic_information,generic information>>.
 - delay the initial triggering of the policy.
 - release nodes after a configurable idle period.

This policy is very well suited to Cloud infrastructures (Azure, EC2, etc) when cloud instances need to be acquired on-demand.

Similarly to the <<_scheduler_loading_policy>>, it is important to correctly configure the maximum and minimum nodes that this
policy will try to hold in accordance with the underlying infrastructure.
This policy will never acquire more nodes than the infrastructure is capable of (as configured in the infrastructure parameters).
Symmetrically, if an infrastructure defines a minimum number of nodes to acquire, this policy will never go below this number.

The DynamicPolicy will scale up when pending tasks are detected in the ProActive Scheduler queue and the Node Source does not have enough free nodes to assign.

In some cases, the existing free nodes will not match the requirements of these pending tasks. This can happen when the task contains a <<../user/ProActiveUserGuide.adoc#_selection_scripts,Selection Script>> or a <<../user/ProActiveUserGuide.adoc#_generic_information_selection,NODE_ACCESS_TOKEN>> restriction that discards existing free nodes.
In that case, the Dynamic Policy will use a *best-effort strategy* to handle these tasks that remain in pending state. When tasks remain in pending state longer than `initDelay` milliseconds, the node source will trigger a scale-up (one new node per pending task).

The *loadFactor* is the main parameter that allows to control the scale up/down of the DynamicPolicy. It decides how many new ProActive Nodes will be deployed when considering the current number of pending and running tasks. *loadFactor* is a ratio which computes the expected number of Nodes in the Node Source according to the formula:

----
expectedNbNodes = Ceiling((nbPendingTasks + nbBusyNodes) / loadFactor)
----

Two additional parameters allow to fine-tune this formula:

 * loadFactorRoundingUp: when `false`, `Ceiling()` is replaced by `Floor()` in the above formula (rounding down instead of rounding up).
 * loadFactorLegacyBehavior: when `true`, the formula is transformed to:
----
expectedNbNodes = Ceiling(nbPendingTasks / loadFactor + nbBusyNodes)
----
This means that new Nodes will be deployed as long as there are pending tasks remaining, regardless of the *loadFactor* value. In that case, *loadFactor* controls only how many new Nodes are deployed per policy cycle and not how many Nodes are present in the Node Source with regard to the number of tasks to run.

Finally, it is also possible to _fully control_ the scale up/down formula and even link it with the state of other Node Sources. This is achieved by using the *scaleNodesExpression* parameter described in <<_scale_nodes_spel_expression>>.

===== Dynamic Policy Resource Tagging

The DynamicPolicy has the ability to add *tags* to resources acquired by cloud providers.
The tagging mechanism depends on the <<_node_source_infrastructures,infrastructure>> associated with the DynamicPolicy. Currently,
only the <<_azure_scale_set_infrastructure,Azure Scale Set Infrastructure>> supports resource tagging (see <<_scale_set_tagging,Scale Set Tagging>> for more information).

Tagging can be activated by adding the <<../user/ProActiveUserGuide.adoc#_resource_tags,RESOURCE_TAGS>> Generic Information to workflows or workflow tasks that will target this node source.
RESOURCE_TAGS must contain the following json key/value structure:

[source, json]
----
{
  "tagName1" :  "tagValue1",
  "tagName2" :  "tagValue2"
}
----

When the node source scales up due to the scheduling of a task with the above RESOURCE_TAGS generic information example, resources will be tagged with 2 tags `tagName1:tagValue1` and `tagName2:tagValue2`.

===== Dynamic Policy Parameters

To use the *DynamicPolicy*, you have to define the following parameters:

-   *userAccessType* - see <<_policy_common_parameters>>

-   *providerAccessType* - see <<_policy_common_parameters>>

-   *minNodes* - Minimum number of nodes to deploy

-   *maxNodes* - Maximum number of nodes to deploy

-   *schedulerUrl* - URL of the Scheduler, e.g. pnp://mymachine:64738, pamr://0, etc

-   *schedulerCredentialsPath* - Path to the credentials used for
scheduler authentication.

-   *schedulerConnectionTimeout* - Timeout in ms to establish connection with the scheduler.

-   *schedulerConnectionTrialsNumber* - Number of attempts to connect with the scheduler.

-   *refreshTime* - Time in milliseconds between each calculation of the number of
    needed nodes.

-   *loadFactor* - loadFactor allows to control the number of Nodes deployed in relation with the number of pending Tasks to be executed.
loadFactor controls needed Nodes to execute pending Tasks according to the following formula:
`neededNodes = Ceiling((pendingTasks + busyNodes) / loadFactor)`.
The _Ceiling()_ function is used because neededNodes must be an integer. _Ceiling()_ rounds the decimal number to the nearest integer above the value (e.g. `Ceiling(3.5)=4`, see https://en.wikipedia.org/wiki/Floor_and_ceiling_functions).
For example, if _loadFactor_ is `1` (the default), as soon as one task is pending and no Node is free to execute it, the Node Source will deploy one new Node.
When _loadFactor_ is `10`, only 1 Node will be deployed to execute 1 to 10 pending tasks. When 10 tasks are pending, 1 Node will be deployed and execute the first pending task. After this task has finished its execution, the second task will run on the same Node, etc.
If at some point 15 tasks are pending, and 1 Node is busy executing a task, a second Node will be deployed. Because, in that case `neededNodes = Ceiling(15(pending) + 1(busy)/10) = 2`.
+
The behavior of *loadFactor* can be further controlled using *loadFactorRoundingUp* and *loadFactorLegacyBehavior* parameters (see below).

-   *loadFactorRoundingUp* - Boolean value which decides if the https://en.wikipedia.org/wiki/Floor_and_ceiling_functions[_Ceiling()_] function or the https://en.wikipedia.org/wiki/Floor_and_ceiling_functions[_Floor()_] function should be used inside the *loadFactor* formula. When *loadFactorRoundingUp* is `true` (default), the _Ceiling()_ function will be used. This will *round up* the neededNodes calculation (described in the example above).
When *loadFactorRoundingUp* is `false`, the _Floor()_ function will be used instead. This will *round down* the neededNodes calculation.
In that case, when for example _loadFactor_ is `10`, a new Node will be deployed in the Node Source when at least 10 tasks are pending. If 9 Tasks are pending, `neededNodes = Floor(9(pending) + 0(busy)/10) = 0`. If 15 Tasks are pending, `neededNodes = Floor(15(pending) + 0(busy)/10) = 1`.

- *loadFactorLegacyBehavior* - Boolean value which decides if the legacy _loadFactor_ behavior should be enabled (default to `false`). When *loadFactorLegacyBehavior* is `true`, it modifies the *loadFactor* formula to: `neededNodes = Ceiling(pendingTasks / loadFactor) + busyNodes`. In that case, the *loadFactor* applies only to pendingTasks. This means that as long as pending tasks are present, new nodes will be deployed (up to the _maxNodes_ parameter). This behavior does not really satisfy the philosophy of *loadFactor* which aims to deploy a limited number of nodes to handle a larger amount of short-lived tasks. `loadFactorLegacyBehavior = true` should be used only for backward-compatibility reasons.

-   *initDelay* - Delay in milliseconds to initialize the infrastructure (eg. in a azure cloud infrastructure this must
cover the creation of Azure's initial resources such as network, storage account, etc.). This parameter is also used as a trigger for the *best-effort scale-up strategy* described above.

-   *minUptime* - Minimal uptime of a free node to be candidate for deletion (in milliseconds). For example, if this parameter
is set to 10 minutes, a node will not be removed from the node source unless it stays 10 minutes consecutively in state FREE, without executing any new task.

-   *globalScope* - Specifies the scope of the policy: If `true`, all the pending tasks will be taken into account for scaling up.
If `false`, only the pending tasks targeting this specific Node Source will be taken into account. Pending tasks can target a Node Source by defining either the <<../user/ProActiveUserGuide.adoc#_node_source_generic_info,NODE_SOURCE>>
 or the (deprecated) <<../user/ProActiveUserGuide.adoc#_nodesourcename,NODESOURCENAME>> generic information.

-   *scaleNodesExpression*: advanced parameter allowing to fully control the formula used to scale up or scale down Nodes. The parameter expects a SpEL expression with a syntax described below.

===== Scale Nodes SpEL expression

*scaleNodesExpression* is a powerful, advanced parameter which allows full control on the scale up and scale down of a DynamicPolicy.

*scaleNodesExpression* uses the link:https://docs.spring.io/spring-framework/docs/4.3.x/spring-framework-reference/html/expressions.html#expressions-language-ref[Spring Expression Language] syntax

*scaleNodesExpression* must return an integer with the following semantic:

 * a positive integer N will require the Node Source to deploy N new Nodes.
 * a negative integer N will require the Node Source to remove N Nodes (only Nodes that have been in `FREE` state for more than *minUptime* milliseconds can be removed).
 * 0 will keep the current Nodes unchanged.

[[_scalenodes_vars]]
===== Variables and Functions

The expression can use the following given variables:

* *loadFactor*: the currently configured load factor in the DynamicPolicy (see <<_dynamic_policy_parameters>>).
* *globalScope*: the currently configured scope of the DynamicPolicy (see <<_dynamic_policy_parameters>>).
* *aliveNodes* : the current number of alive Nodes in the Node Source when *globalScope* is `false`. Otherwise, *aliveNodes* will be the current number of alive Nodes in the Resource Manager. Alive Nodes are Nodes in the following states: `DEPLOYING`, `CONFIGURING`, `FREE`, `BUSY` or `TO_BE_REMOVED` (see <<_node_states,Node States>>).
* *aliveNodesInNodeSource*: the current number of alive nodes in the Node Source, regardless of the *globalScope* configuration.
* *busyNodes*: the current number of Nodes in BUSY state in the Node Source when *globalScope* is `false`. Otherwise, *busyNodes* will be the current number of Nodes in BUSY state in the Resource Manager.
* *busyNodesInNodeSource*: the current number of Nodes in BUSY state in the Node Source regardless of the *globalScope* configuration.
* *deployingNodes*: the current number of Nodes in DEPLOYING state in the Node Source (does not consider the *globalScope* parameter).
* *neededNodes*: the number of Nodes needed by pending tasks. This value contains all pending tasks when *globalScope* is `true` and only tasks which define the <<../user/ProActiveUserGuide.adoc#_node_source_generic_info,NODE_SOURCE>> or the <<../user/ProActiveUserGuide.adoc#_nodesourcename,NODESOURCENAME>> generic information when *globalScope* is `false`. Also, this value considers multi-node pending tasks (for example, a task can require using 3 Nodes).
* *neededNodesPrevious*: the number of Nodes that were needed at the previous refresh cycle. This value can be used to check if tasks keep staying pending.

* *temp*, *temp2*, *temp3*, *temp4*: each of these variables can be set to a temporary object used in the SpEL expression. Affectations should use the `z()` function described below. e.g. `z(temp=neededNodes+busyNodes)`

* *tempMap*: an empty Hash Map structure which can be populated and used in the SpEL expression. Affectations to the map should use the `z()` function described below. e.g. `z(tempMap.put('key', 'value'))`.
* *nodeSources*: a _key-value_ HashMap which contain the state of all Node Sources in the Resource Manager. The key is the Node Source name and the value contains the following properties:
** *acquiredNodes*: the current number of acquired Nodes in this Node Source. *acquiredNodes* contain Nodes in `CONFIGURING`, `FREE`, `BUSY` or `TO_BE_REMOVED` states. This property is accessed using `nodeSources.get('node-source-name').getAcquiredNodes()`.
** *aliveNodes* : the current number of alive Nodes in the Node Source. Alive Nodes are Nodes in `DEPLOYING`, `CONFIGURING`, `FREE`, `BUSY` or `TO_BE_REMOVED` states. This property is accessed using `nodeSources.get('node-source-name').getAliveNodes()`.
** *deployingNodes*: the current number of Nodes in `DEPLOYING` state in this Node Source. This property is accessed using `nodeSources.get('node-source-name').getDeployingNodes()`.
** *busyNodes*: the current number of Nodes in `BUSY` or `TO_BE_REMOVED` states in this Node Source. This property is accessed using `nodeSources.get('node-source-name').getBusyNodes()`.
** *freeNodes*: the current number of Nodes in `FREE` state in this Node Source. This property is accessed using `nodeSources.get('node-source-name').getFreeNodes()`.
** *totalNodes*: the total number of Nodes in this Node Source. This property is accessed using `nodeSources.get('node-source-name').getTotalNodes()`.
** *descriptor*: a link:../javadoc/org/ow2/proactive/resourcemanager/nodesource/NodeSourceDescriptor.html[NodeSourceDescriptor] object containing all the parameters of this Node Source. This property is accessed using `nodeSources.get('node-source-name').getDescriptor()`.
The NodeSourceDescriptor object contain many methods, among which the `getInfrastructureParameter(name)` and the `getPolicyParameter(name)` will be the most useful.
+
For example, it is possible to access the *maxNodes* parameter of a specific Node Source that uses a DynamicPolicy:
+
----
T(Integer).parseInt(nodeSources.get('node-source-name').getDescriptor().	getPolicyParameter('maxNodes'))
----
+
As _getPolicyParameter_ returns a string, it is necessary to transform it into an integer before doing any integer comparison.

* *previousNodeSources*: this variable contains a similar structure as *nodeSources* but retrieved from the previous refresh cycle. This can be used to ensure that the state of another Node Source remains stable between 2 cycles and take scale up/down decisions accordingly.  Please note that on the first cycle, *previousNodeSources* will contain an empty HashMap. It is thus advised to ensure in the SpEL expression that returned node source information are not `null`. An example usage of *previousNodeSources* is described in <<_scalenodes_examples>>.

*scaleNodesExpression* can also use the following function (in addition to the functions available by default in the SpEL language):

 * `z(expression)`: evaluate the expression and return `zero`.
This is used to allow performing affectations to the `temp` or `tempMap` variables. For example:
----
z(temp=neededNodes+busyNodes)+T(Math).ceil(temp/loadFactor)-aliveNodes
----
In this expression, the `z()` function prevents the affectation from adding its value to the total.

[[_scalenodes_examples]]
===== Example Expressions

Below is an example of a *scaleNodesExpression* that reproduces the default behavior of the DynamicPolicy:

----
T(Math).ceil((neededNodes + busyNodes) / loadFactor) - aliveNodes
----

Explanation:

 * `T(Math).ceil((neededNodes + busyNodes) / loadFactor)`: this computes the expected number of Nodes. When *loadFactor* is 1, the expected number of Nodes is the sum of pending tasks and current Nodes in `BUSY` state. A *loadFactor* greater than 1 will reduce the amount of expected Nodes.
 * `- aliveNodes`: the number of alive Nodes is subtracted to the expected number of Nodes to return the signed difference. The returned value will either be positive (scale up) or negative (scale down).

Below is an example of a *scaleNodesExpression* that will scale up the Node Source when another Dynamic Policy Node Source has reached maximum capacity (this expression considers that *globalScope* is `true` for both Node Sources). The following expression must be defined on the second Node Source:

----
z(temp=nodeSources.get('FirstNS')) + z(temp2=previousNodeSources.get('FirstNS')) + z(temp3=(temp2!=null?T(Math).min(temp.getBusyNodes(),temp2.getBusyNodes()):0)) + (temp3 == T(Integer).parseInt(temp.getDescriptor().getPolicyParameter('maxNodes')) ? T(Math).ceil((neededNodes + busyNodesInNodeSource) / loadFactor) - aliveNodesInNodeSource : -aliveNodesInNodeSource )
----

Explanation:

The main structure of this expression uses the `test ? case1 : case2` syntax, which evaluates `case1` when `test` is `true` and `case2` when `test` is `false`.

 * `z(temp=nodeSources.get('FirstNS'))`: read the state of the first Node Source and store it into a temporary variable. Notice the use of the `z()` function and the `{plus}` sign, so that this affectation does not change the value returned globally by the SpEL expression.
 * `z(temp2=previousNodeSources.get('FirstNS'))`: read the previous state of the first Node Source and store it into another temporary variable.
 * `z(temp3=(temp2!=null?T(Math).min(temp.getBusyNodes(),temp2.getBusyNodes()):0))`: store into another variable the minimum amount of busy nodes that were accounted on the first Node Source in both refresh cycle. If the previous cycle is empty, store `0` into the variable.
 * `temp3 == T(Integer).parseInt(temp.getDescriptor().getPolicyParameter('maxNodes'))` : check if the number of busy Nodes in the Node Source `FirstNS` is equal to the maximum configured for the first Node Source for 2 consecutive refresh periods.
 * `T(Math).ceil((neededNodes + busyNodesInNodeSource) / loadFactor) - aliveNodesInNodeSource`: when the maximum number of nodes is reached on the first Node Source, the second Node Source will scale up based on the default scale up expression (see previous example). Note that we use `busyNodesInNodeSource` and `aliveNodesInNodeSource` to count only Nodes that belong to the second Node Source.
 * `-aliveNodesInNodeSource`: when the first Node Source is not at the maximum, remove all alive Nodes in the second Node Source (in reality, only `FREE` Nodes will be removed after *minUptime* milliseconds).

==== Dynamic Policy Canonical Examples

In this section we will present several canonical use cases that make use of node sources controlled by a *DynamicPolicy*:

 * A single cloud Dynamic Infrastructure
 * A Static Infrastructure (on-premise or cloud) + a single Cloud Dynamic Infrastructure.
 * A primary cloud Dynamic Infrastructure + a secondary Cloud Dynamic Infrastructure.
 * A Static Infrastructure (on-premise or cloud) + a primary Cloud Dynamic Infrastructure + a secondary Cloud Dynamic Infrastructure.

The last two cases are especially suitable when one is looking to *optimize the cloud cost* of one's infrastructure using preemptive Spot VMs. For example, your Jobs can be executed in priority on the low cost Spot VMs or static infrastructure (that can be lower-cost pre-reserved instances). Only when Spot VMs are becoming scarce or your workload exceeds your Spot quota, you will be automatically using the Standard VMs.

===== Single Cloud Dynamic Infrastructure

In this use case, the Resource Manager defines a single cloud infrastructure with a *DynamicPolicy* attached. For the type of cloud infrastructure, we can use for example an <<_azure_scale_set_infrastructure,Azure Scale Set Infrastructure>>, an <<_aws_autoscaling_infrastructure,AWS Autoscaling Infrastructure>>, a <<_gce_infrastructure,GCE Infrastructure>>, etc...

The configuration of the DynamicPolicy in this use case is fairly simple, we only need to set *globalScope* to `true` to ensure that Nodes will be dynamically deployed whenever `PENDING` tasks appear in the ProActive Scheduler queue. We control the cloud node source capacity using the *minNodes* and *maxNodes* parameters. And *minUptimes* will be adjusted to determine how long a `FREE` Node will remain before scaling down.

===== Static Infrastructure and single Cloud Dynamic Infrastructure

In this use case, the Resource Manager defines two Node Sources:

 * A first Node Source _'StaticNS'_ which will be controlled by a *StaticPolicy*. This Node Source can either use an on-premise infrastructure such as <<_ssh_infrastructure_v2,SSH Infrastructure V2>> or a cloud infrastructure such as  <<_azure_scale_set_infrastructure,Azure Scale Set Infrastructure>>).
 * A second Node Source _'DynamicNS'_ which uses a cloud infrastructure and a *DynamicPolicy*.

The general configuration mentioned in the previous scenario will be applied to _'DynamicNS'_. *globalScope* must be set to `true`. In addition, a *scaleNodesExpression* must be defined to ensure that the dynamic cloud Node Source will scale up when all Nodes of _'StaticNS'_ are used.

The maximum number of Nodes of _'StaticNS'_ cannot be easily determined by reading the parameters of its infrastructure, as it highly depends on the type of infrastructure selected (e.g. LocalInfrastructure, SSHInfrastructureV2). On the other hand, we can assume that _'StaticNS'_ is fully used when the number of `FREE` and `DEPLOYING` Nodes is zero and the number of `BUSY` Nodes is greater than zero.

*scaleNodesExpression* defined in _'DynamicNS'_:

----
z(temp=nodeSources.get('StaticNS')) + z(temp2=previousNodeSources.get('StaticNS')) + z(temp3=(temp2!=null?temp2.getFreeNodes()+temp2.getDeployingNodes()+temp.getFreeNodes()+temp.getDeployingNodes():-1)) + z(temp4=(temp2!=null?T(Math).min(temp2.getBusyNodes(),temp.getBusyNodes()):0)) + (temp3 == 0 && temp4 > 0 ? T(Math).ceil((neededNodes + busyNodesInNodeSource) / loadFactor) - aliveNodesInNodeSource : -aliveNodesInNodeSource )
----

We query _'StaticNS_' info through 2 successive cycles (using *nodeSources* and *previousNodeSources*) to ensure that the observed state is stable. In _temp3_ we assign a number that will be zero when no `DEPLOYING` or `FREE` Node was observed. This number will be a non-zero value on the very first cycle. In _temp4_ we assign the minimum number of `BUSY` Nodes observed.

_'DynamicNS'_ will scale up when _temp3_ is zero and _temp4_ is greater than zero. It will scale down otherwise.


===== Cloud Dynamic Infrastructure and Secondary Cloud Dynamic Infrastructure

In this use case, the Resource Manager defines two Node Sources:

 * A first Node Source _'DynamicNS'_ which will be controlled by a *DynamicPolicy* and using cloud infrastructure such as  <<_azure_scale_set_infrastructure,Azure Scale Set Infrastructure>>).
 * A second Node Source _'DynamicNSExtended'_ which uses a similar setup.

The configuration of _'DynamicNS'_ will be a basic configuration as described in <<_single_cloud_dynamic_infrastructure>> with *globalScope* `true`.

The configuration of _'DynamicNSExtended'_ will also use *globalScope* `true` and add a *scaleNodesExpression* to ensure that the secondary Node Source will scale up only when _'DynamicNS'_ is fully used.

*scaleNodesExpression* defined in _'DynamicNSExtended'_:

----
z(temp=nodeSources.get('DynamicNS')) + z(temp2=previousNodeSources.get('DynamicNS')) + z(temp3=(temp2!=null?T(Math).min(temp.getBusyNodes(),temp2.getBusyNodes()):0)) + (temp3 == T(Integer).parseInt(temp.getDescriptor().getPolicyParameter('maxNodes')) ? T(Math).ceil((neededNodes + busyNodesInNodeSource) / loadFactor) - aliveNodesInNodeSource : -aliveNodesInNodeSource )
----

We query _'DynamicNS_' info through 2 successive cycles (using *nodeSources* and *previousNodeSources*) to ensure that the observed state is stable. In _temp3_ we assign the minimum busy nodes amount in _'DynamicNS_' the two cycles.

_'DynamicNS'_ will scale up when there are pending tasks in ProActive Scheduler queue. _'DynamicNSExtended'_ will scale up when the amount of `BUSY` Nodes in _'DynamicNS'_ is equal to the maximum number of Nodes configured, and will scale down otherwise.


===== Static Infrastructure, Primary Cloud Dynamic Infrastructure and Secondary Cloud Dynamic Infrastructure

In this use case, the Resource Manager defines three Node Sources:

 * A first Node Source _'StaticNS'_ which will be controlled by a *StaticPolicy*. This Node Source can either use an on-prem infrastructure such as <<_ssh_infrastructure_v2,SSH Infrastructure V2>> or a cloud infrastructure such as  <<_azure_scale_set_infrastructure,Azure Scale Set Infrastructure>>, or pre-reserved instances).
 *  A second Node Source _'DynamicNS'_ which will be controlled by a *DynamicPolicy* and using cloud infrastructure such as  <<_azure_scale_set_infrastructure,Azure Scale Set Infrastructure>>, typically Spot VMs).
* A third Node Source _'DynamicNSExtended'_ which uses a similar setup, but typically using Standards VMs.

This use case is a combination of <<_static_infrastructure_and_single_cloud_dynamic_infrastructure>> and <<_cloud_dynamic_infrastructure_and_secondary_cloud_dynamic_infrastructure>>:

 * In _'DynamicNS'_, we set *globalScope* to `true` and define a *scaleNodesExpression* as following:
+
----
z(temp=nodeSources.get('StaticNS')) + z(temp2=previousNodeSources.get('StaticNS')) + z(temp3=(temp2!=null?temp2.getFreeNodes()+temp2.getDeployingNodes()+temp.getFreeNodes()+temp.getDeployingNodes():-1)) + z(temp4=(temp2!=null?T(Math).min(temp2.getBusyNodes(),temp.getBusyNodes()):0)) + (temp3 == 0 && temp4 > 0 ? T(Math).ceil((neededNodes + busyNodesInNodeSource) / loadFactor) - aliveNodesInNodeSource : -aliveNodesInNodeSource )
----

 * In _'DynamicNSExtended'_, we set *globalScope* to `true` and define a *scaleNodesExpression* as following:
+
----
z(temp=nodeSources.get('DynamicNS')) + z(temp2=previousNodeSources.get('DynamicNS')) + z(temp3=(temp2!=null?T(Math).min(temp.getBusyNodes(),temp2.getBusyNodes()):0)) + (temp3 == T(Integer).parseInt(temp.getDescriptor().getPolicyParameter('maxNodes')) ? T(Math).ceil((neededNodes + busyNodesInNodeSource) / loadFactor) - aliveNodesInNodeSource : -aliveNodesInNodeSource )
----

////
prevents the following from being in a source block (bug?)
////

_'DynamicNS'_ will scale up when _'StaticNS'_ has reached its maximum capacity.

_'DynamicNSExtended'_ will scale up when _'DynamicNS'_ has reached its max capacity.