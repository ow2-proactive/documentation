:docinfo:
:toc:
:toc-title: PWS Admin Guide
= ProActive Workflows & Scheduling -- Admin Guide
include::../common-settings.adoc[]
include::../all-doc-links.adoc[]

== Overview

include::../Overview.adoc[]

The administration guide covers cluster setup and cluster administration. Cluster setup includes two main steps:

* The installation and configuration of the <<_glossary_proactive_scheduler,*ProActive Scheduler*>>.
* The set up of <<_glossary_proactive_node,*ProActive Nodes*>>.

=== Glossary

include::../Glossary.adoc[]

[[_run_the_proactive_scheduler]]
== Get Started

ProActive Scheduler can be installed and started either manually or automatically.

For *manual installation*, you can install and start ProActive:

* as a Java process, see section <<_process_Start_ProActive_Manually_as_a_Java_process>>.
* as a system service, see section <<_service_Instal_and_start_ProActive_as_a_system_service>>.

For *automatic installation*, you can install and start ProActive:

* using Docker Compose _(applicable to any machine with Docker and Docker Compose installed)_, see section <<compose_Automated_ProActive_Installation_with_Docker_Compose>>.
* using Kubernetes _(applicable to any On-premises or AKS Kubernetes clusters)_ , see section <<kube_Automated_ProActive_Installation_with_Kubernetes>>.
* using Shell Scripts _(applicable to both physical and cloud virtual machines)_, see section <<shell_Automated_ProActive_Installation_with_shell>>.

We recommend using one of the *automated installation methods*, as they are faster and simpler.

=== Minimum requirements

The minimal hardware requirements for the ProActive server is 4 Cores and 8 GB RAM.

ProActive server is compatible with any 64 bits Linux, Windows or Mac operating system. The server process uses at minimum 4 GB of RAM.

ProActive Nodes are compatible with any Linux, Windows or Mac systems (both 64 bits and 32 bits versions). ProActive Nodes can also run on some Unix systems (such as Solaris, HP-UX), by installing and using a Java Runtime Environment compatible with these systems.

Sizing the ProActive server correctly depends on the number of jobs and tasks executed and submitted per day, the number of nodes connected to the server and the number of concurrent users.

A medium-sized ProActive Scheduler installation uses 12 cores, 24 GB RAM. Large setups use 24 cores and 100 GB RAM.

Additionally, using local ProActive Nodes (deployed directly inside the ProActive server) or having other services running on the same machine as the server can impact the sizing.

[[_process_Start_ProActive_Manually_as_a_Java_process]]
=== Start ProActive Manually as a Java process

First, https://www.activeeon.com/community-downloads[Download^] ProActive Scheduler and *unzip* the archive. The extracted folder will be referenced as `PROACTIVE_HOME` in the rest of the documentation.
The archive contains all required dependencies with no extra prerequisites.

The first way to start a ProActive Scheduler is by running the `proactive-server` script. To do so, you can use the following command with no extra configuration.

[source]
----
$PROACTIVE_HOME/bin/proactive-server
----

NOTE: It is possible to start the server with a specific number of nodes by adding the *-ln* parameter. For example, PROACTIVE_HOME/bin/proactive-server -ln 1 will start the proactive server with a single node.
In case this parameter is not specified, the number of nodes corresponds to the number of cores of the machine where ProActive server is installed.

....
The router created on localhost:33647
Starting the scheduler...
Starting the resource manager...
The resource manager with 4 local nodes created on pnp://localhost:41303/
The scheduler created on pnp://localhost:41303/
The web application /automation-dashboard created on http://localhost:8080/automation-dashboard
The web application /studio created on http://localhost:8080/studio
The web application /job-analytics created on http://localhost:8080/job-analytics
The web application /notification-service created on http://localhost:8080/notification-service
The web application /cloud-automation-service created on http://localhost:8080/cloud-automation-service
The web application /catalog created on http://localhost:8080/catalog
The web application /proactive-cloud-watch created on http://localhost:8080/proactive-cloud-watch
The web application /scheduler created on http://localhost:8080/scheduler
The web application /scheduling-api created on http://localhost:8080/scheduling-api
The web application /connector-iaas created on http://localhost:8080/connector-iaas
The web application /rm created on http://localhost:8080/rm
The web application /rest created on http://localhost:8080/rest
The web application /job-planner created on http://localhost:8080/job-planner
*** Get started at http://localhost:8080 ***
....

The following ProActive Scheduler components and services are started:

* <<_glossary_resource_manager,*Resource Manager*>>
* 4 local <<_glossary_proactive_node,*ProActive Nodes*>>
* <<_glossary_scheduler,*Scheduler*>>
* <<_glossary_automation_dashboard,*Automation Dashboard*>>
* <<_glossary_workflow_studio,*Workflow Studio*>>
* <<_glossary_job_analytics,*Job Analytics*>>
* <<_glossary_notification_service,*Notification Service*>>
* <<_glossary_cloud_automation_service,*Service Automation*>>
* <<_glossary_catalog,*Catalog*>>
* <<_glossary_cloud_watch,*Event Orchestration*>>
* <<_glossary_scheduler_web_interface,*Scheduler Web Interface*>>
* <<_glossary_scheduling_api,*Scheduling Api*>>
* <<_glossary_connector_iaas,*Connector IaaS*>>
* <<_glossary_rm_web_interface,*Resource Manager Web Interface*>>
* <<_glossary_rest_api,*REST API*>>
* <<_glossary_job_planner,*Job Planner*>>

The URLs of the Scheduler, Resource Manager, REST API and Web Interfaces are displayed in the output.

The ProActive server can also be started in a specific state by using the command line options `--stopped`, `--paused` or `--frozen`.

When one of these options is used, the scheduler will start in the provided state, which can be changed afterwards using the ProActive Scheduler portal.

See <<_scheduler_life_cycle_management>> for an explanation of the different scheduler states.

[[_service_Instal_and_start_ProActive_as_a_system_service]]
=== Install and start ProActive as a system service

First, https://www.activeeon.com/community-downloads[Download^] ProActive Scheduler and *unzip* the archive. The extracted folder will be referenced as `PROACTIVE_HOME` in the rest of the documentation.
The archive contains all required dependencies with no extra prerequisites.

The second way to start a ProActive Scheduler is to install it as a system service. Depending on your operating system, the installation of ProActive can be done as follows.

==== How to install ProActive on Windows

Under Windows, it is possible to use the https://nssm.cc/‎[nssm^] service manager tool to manage a running script as a service. You can configure nssm to absolve all responsibility for restarting it and let Windows take care of recovery actions.
In our case, you need to provide to nssm the Path to this script `$PROACTIVE_HOME/bin/proactive-server.bat` to start ProActive as a service.

==== How to install ProActive on Linux

Under Linux, the user has to run the `install.sh` script located under `$PROACTIVE_HOME/tools` folder as *root* and follow the installations steps provided in an interactive mode.
Generally, it is recommended to keep the default configuration (internal account, protocol used, port , etc) unless you would like to setup a particular one.

....
$ sudo $PROACTIVE_HOME/tools/install.sh
Directory where to install the scheduler: /opt/proactive
Name of the user starting the ProActive service: proactive
Group of the user starting the ProActive service: proactive
ProActive use internal system accounts which should be modified in a production environment.
Do you want to regenerate the internal accounts ? [Y/n] n
ProActive can integrate with Linux PAM (Pluggable Authentication Modules) to authenticate users of the linux system.
Warning: this will add the proactive account to the linux "shadow" group
Do you want to set PAM integration for ProActive scheduler? [y/N]  n
Protocol to use for accessing Web apps deployed by the proactive server: [http/https] http
Port to use for accessing Web apps deployed by the proactive server: 8080
Number of ProActive nodes to start on the server machine: 4
The ProActive server can automatically remove from its database old jobs. This feature also remove the associated job logs from the file system.
Do you want to enable automatic job/node history removal? [Y/n]  y
Remove history older than (in days) [30]: 30
Here are the network interfaces available on your machine and the interface which will be automatically selected by ProActive:
Available interfaces:
Eligible addresses:
/xxx.xxx.xxx.xxx
/127.0.0.1

Elected address : /xxx.xxx.xxx.xxx
Elected address :
Do you want to change the network interface used by the ProActive server? [y/N]  n
Start the proactive-scheduler service at machine startup? [Y/n]  y
Restrict the access to the ProActive installation folder to user proactive? [y/N]  n
Restrict the access to the configuration files containing sensitive information to user proactive? [Y/n]  y
Do you want to start the scheduler service now? [Y/n]  y
If a problem occurs, check output in /var/log/proactive/scheduler
....

Once the server is up, ProActive Web Portals are available at `\http://localhost:8080/`. The user can access to any of the four portals using the default credentials: *admin/admin*

Your ProActive Scheduler is ready to execute <<_glossary_job,*Jobs*>>!

==== How to upgrade ProActive on Linux
Before you upgrade to a new version of ProActive (ex 10.0), you first need to back up the configuration and data that you want to restore (DB, workflows, etc) from the old ProActive installation (ex 8.4). Basically, you have to back up the content of the following folders:

*  `$PROACTIVE_HOME/data` which contains all databases (scheduler, catalog, rm) and workflows of the different users.
*  `$PROACTIVE_HOME/config` which contains configuration files required to setup the server initial settings, define the network properties, manage the authentication, internal accounts, access permissions, etc.

To upgrade ProActive, the user has to run the `install.sh` script located under `$PROACTIVE_HOME/tools` folder as *root* and follow the installations steps provided in an interactive mode.
It is important to mention that the user has to choose the same directory as the old version where to make the ProActive Scheduler installation. This will enable the detection of the existing installation and then copying addons, data files and the configuration changes from the previous installation to the new one.
In some cases, this operation can induce some conflicts that can be solved easily using Git tools.

....
$ sudo $PROACTIVE_HOME/tools/install.sh
This will install the ProActive scheduler 10.1.0-SNAPSHOT as a service.
Once the installer is started, it must process until the end otherwise the installation may be corrupted.
Do you want to continue? [Y/n]  y
Directory where to install the scheduler: /opt/proactive
Previous installation was owned by user proactive with group proactive
Detected an existing ProActive Scheduler installation at /opt/proactive/8.4
Copying addons and data files from the previous installation...
Do you want to port all configuration changes to the new version (do not do this from a version prior to 8.4.0 before executing the patch)? [Y/n]  y
Porting configuration into new installation...
[master 61aaa19] Commit all changes before switching from 8.4 to 10.1
 4 files changed, 8 insertions(+), 8 deletions(-)
warning: no common commits
Depuis /opt/proactive/8.4/
 * [nouvelle branche] master     -> old-version/master

Cherry-picking all changes to new installation
Here is the list of JAR files in the new installation 'addons' folder.
If there are duplicates, you need to manually remove outdated versions.
Restrict the access to the ProActive installation folder to user proactive? [y/N]  n
Restrict the access to the configuration files containing sensitive information to user proactive? [Y/n]  y
Resource Manager credentials are used by remote ProActive Agents to register to the scheduler.
Do you want to start the scheduler service now? [Y/n]  y
If a problem occurs, check output in /var/log/proactive/scheduler
....

WARNING: Once the upgrade is performed and the new version started for the first time, the ProActive server will upgrade <<_glossary_catalog,Catalog>> objects to the new version. This procedure ensures that any object modified manually by a user will not be upgraded.

[[compose_Automated_ProActive_Installation_with_Docker_Compose]]
=== Automated ProActive Installation with Docker Compose
ProActive could be easily deployed and started within a fully containerized environment. For this purpose, we provide a ready-to-use Docker Compose configuration that enables you running ProActive Workflows and Scheduling (PWS) as a containerized service.
It starts the following services:

*  ProActive server container
*  (Optional) ProActive PostgreSQL database container. The database container is not started when using our embedded HSQL Database.

The Docker Compose configuration pulls the ProActive server and database images from the https://dockerhub.activeeon.com/[Private Activeeon Docker Registry^].

These images can be used out of the box, on any operating system. You can customize the ProActive server and database settings, located in the `.env` file according to your requirements.

To be able to access the https://dockerhub.activeeon.com/[Private Activeeon Docker Registry^], a ProActive enterprise licence and access credentials are required.
Please contact us at contact@activeeon.com to request access.

Here are the installation steps you need to follow in order to have ProActive running in a fully containerized environment:

==== Download the Docker Compose Installation Package
We offer a public Docker Compose installation package, enabling you to easily configure and initiate the Docker installation within your environment.

The Docker Compose installation package includes:

- The Docker Compose input file (`.env`): Used to set the environment variables that are then read by the Docker Compose configuration files.

- The Docker Compose configuration files: Define and configure the ProActive server and database containers using YAML definitions:
** `docker-compose-hsqldb.yml`: Used to have ProActive with an embedded HSQL database.
** `docker-compose-pg.yml`: Used to have ProActive with a Postgres database.

You can proceed to donwload it using the following command:

'''
++++
<pre><code><span id="composeInstaller"/></code></pre>
++++
'''



==== Configure the Docker Compose Input File

Before starting the ProActive installation, you need first to configure the installation input parameters using the Docker Compose environment file `.env`.

'''
.*[underline blue]#Click here to display the .env input file#*
[example%collapsible]
====
link:./references/automatedInstallScripts/.env[.env, title="Click to download"^]:
[source,yaml]
----
include::./references/automatedInstallScripts/.env[]
----
====
'''

===== Required configuration
The `.env` file mentioned above is ready for use in a minimal installation. However, it does require the definition of five mandatory parameter values:

- `DOCKER_IMAGE_PROACTIVE_NAMESPACE`: Defines the ProActive Docker image namespace (or repository) in the Activeeon private registry.
- `DOCKER_IMAGE_PROACTIVE_TAG`: Defines the ProActive release version.
- `DOCKER_IMAGE_POSTGRES_NAMESPACE`: Defines the ProActive Postgres image namespace.
- `DOCKER_IMAGE_POSTGRES_TAG`: Defines the Postgres version to be used.
- `PWS_HOST_ADDRESS`: Defines the internal hostname or IP address of the machine where ProActive will be installed.

[[_compose_advanced_parameters]]
===== Advanced configuration
For more advanced configuration of the ProActive server, please refer to the parameters listed below:

[[_compose_PWS_Server_Configuration_Properties]]
*_PWS Server Configuration Properties_*

This section outlines the global configuration options available to configure the ProActive server.

- `PWS_PROTOCOL`: The internal http protocol to use (http or https). By default, http.

- `PWS_PORT`: The internal port used (e.g., 8080 for http, 8443 for https). By default, 8080.

- `PWS_PAMR_PORT`: The internal port to use for <<_proactive_message_routing,PAMR>> communication. By default, 33647.

- `PWS_JOB_CLEANUP_DAYS`: Remove job history older than the specified number of days. By default, 30.

- `PWS_USER_ID`: ID of the user starting the ProActive server. By default, 1001.

- `PWS_USER_GROUP_ID`: Group ID of the user starting the ProActive server. By default, 1001.

- `PWS_USER_NAME`: Name of the user starting the ProActive server. By default, activeeon.

- `PWS_USER_GROUP_NAME`: Group of the user starting the ProActive server. By default, activeeon.

- `PWS_LOGIN_ADMIN_PASSWORD`: Password of the ProActive 'admin' user. By default, activeeon.

- `PWS_WORKER_NODES`: Number of ProActive agent nodes to deploy on the ProActive server machine. By default, 4.

[[_compose_Public_Access_Configuration_Properties]]
*_Public Access Configuration Properties_*

This section describes the public REST Urls to be set when you are running the ProActive server behind a reverse proxy or in a cloud instance (e.g., Azure VM)

- `ENABLE_PUBLIC_ACCESS`: Set to true when the ProActive server is deployed behind a reverse proxy or inside a cloud instance. By default, false.

- `PWS_PUBLIC_HOSTNAME`: Public hostname used (e.g. public.myserver.mydomain) or IP address (e.g. 192.168.12.1).

- `PWS_PUBLIC_SCHEME`: Public protocol used (http or https).

- `PWS_PUBLIC_PORT`: Public port used (e.g. 8080, 8443).

[[_compose_TLS_Certificate_Configuration_Properties]]
*_TLS Certificate Configuration Properties_*

In case you would like to set up a TLS certificate for the ProActive server, you can customize it in this section:


- `ENABLE_TLS_CERTIFICATE_CONFIGURATION`: Set to true in case you have a valid certificate for the host address. By default, false.

- `TLS_CERTIFICATE_PATH`: Path of the certificate full chain or PFX keystore (accepted formats are crt, pem or pfx).

- `TLS_CERTIFICATE_KEY_PATH`: If the certificate is provided in pem/crt format, path to the server private key (`TLS_CERTIFICATE_KEY_PATH` is ignored if the certificate is provided as a pfx keystore).

- `TLS_CERTIFICATE_KEYSTORE_PASSWORD`: Password of the server private key (if applicable) or pfx keystore. By default, activeeon.

NOTE: The pem/crt/pfx certificate as well as the key have to be placed in the backup folder and their absolute paths need to be provided. See section <<_compose_required_configuration>> to learn more about the backup folder to be created beforehand.

[[_compose_LDAP_Configuration_Properties]]
*_LDAP Configuration Properties_*

In case you would like to set up the LDAP authentication for the ProActive server, you can enable the LDAP configuration in this section.

The given parameter values are only examples and actual parameters must match your LDAP organization. This section will require two accounts:

[circle]
- An `LDAP_BIND_LOGIN` (given as a LDAP distinguished name) that will be used by the ProActive server to execute LDAP queries (search queries only).

- An `LDAP_TEST_LOGIN` (given as a LDAP user login name) that will be used to validate the configuration and guarantee that the connection to the LDAP server is working properly. This test account is only used during the installation.

You can then set up the right LDAP configuration using the following properties:

- `ENABLE_LDAP_AUTHENTICATION`: Set to true if you would like to enable the LDAP authentication to the ProActive server. By default, false.

- `LDAP_SERVER_URLS`: Url of the LDAP server. Multiple urls can be provided separated by commas. Example: `ldaps://ldap-server.company.com`

- `LDAP_USER_SUBTREE`: Scope in the LDAP tree where users can be found. Example: `ou=users,dc=activeeon,dc=com`

- `LDAP_GROUP_SUBTREE`: Scope in the LDAP tree where groups can be found. Example: `ou=groups,dc=activeeon,dc=com`

- `LDAP_BIND_LOGIN`: Login that will be used when binding to the LDAP server (to search for a user and its group). The login should be provided using its LDAP distinguished name. Example: `cn=admin,dc=activeeon,dc=com`

- `LDAP_BIND_PASSWORD`: Password associated with the bind login.

- `LDAP_USER_FILTER`: LDAP filter used to search for a user. %s corresponds to the user login name that will be given as parameter to the filter. Example: `(&(objectclass=inetOrgPerson)(uid=%s))`

- `LDAP_GROUP_FILTER`: LDAP filter used to search for all groups associated with a user. %s corresponds to the user distinguished name found by the user filter. Example: `(&(objectclass=groupOfUniqueNames)(uniqueMember=%s))`

- `LDAP_TEST_LOGIN`: A login name that will be used to test the user and group filter and validate the configuration. The test login name must be provided as a user id. Example: `activeeon`

- `LDAP_TEST_PASSWORD`: Password associated with the test user.

- `LDAP_ADMIN_ROLES`: List of existing LDAP groups that will be associated with a ProActive administrator role. Multiple groups can be provided separated by commas. Example: `activeeon-admins`

- `LDAP_USER_ROLES`: List of existing LDAP groups that will be associated with a ProActive standard user role. Multiple groups can be provided separated by commas. Example: `activeeon-operators`

[[_compose_PWS_Users_Configuration_Properties]]
*_PWS Users Configuration Properties_*

In case you would like to create internal user accounts for the ProActive server, you can enable the users configuration in this section:

- `ENABLE_USERS_CONFIGURATION`: Set to true if you would like to create internal ProActive users via login/password. By default, false.

- `PWS_USERS`: A json formatted array where each object defines a set of LOGIN, PASSWORD, and GROUPS of a ProActive internal user. The list of ProActive predefined GROUPS that can be associated with a user are listed and explained following this link: <<_user_permissions>>

[[_compose_External_Database_Configuration_Properties]]
*_External Database Configuration Properties_*

These parameters are only used when ProActive is started with a Postgres database. It is recommended to not change these parameters unless you would like to change the passwords.

- `ENABLE_EXTERNAL_DATABASE_CONFIGURATION`: Set to true if you would like to start ProActive with a Postgres database. By default, false.

- `POSTGRES_PASSWORD`: Password of the Postgres database user. By default, postgres.

- `DB_DNS`: DNS of the machine hosting the database. By default, it is hosted in the same network as the ProActive server container, hence, the database container name is referenced.

- `DB_PORT`: Database port. By default, 5432.

- `DB_DIALECT`: Database dialect to be used for hibernate communication. By default, PostgreSQL94Dialect.

- `DB_RM_PASS`: Password of the `rm` user in the RM database. By default, rm.

- `DB_SCHEDULER_PASS`: Password of the `scheduler` user in the SCHEDULER database. By default, scheduler.

- `DB_CATALOG_PASS`: Password of the `catalog` user in the CATALOG database. By default, catalog.

- `DB_PCA_PASS`: Password of the `pca` user in the PCA database. By default, pca.

- `DB_NOTIFICATION_PASS`: Password of the `notification` user in the NOTIFICATION database. By default, notification.

==== Docker Compose Configuration with an Embedded HSQL Database
Once you set the required configuration options in the `.env` file, you can use now the Docker Compose configuration provided in the installation package (`docker-compose-hsqldb.yml`) to run the ProActive server container with an embedded HSQL database.

It will start one single container related to the ProActive server.

'''
.*[underline blue]#Click here to display the docker-compose-hsqldb.yml file#*
[example%collapsible]
====
link:./references/automatedInstallScripts/docker-compose-hsqldb.yml[docker-compose-hsqldb.yml, title="Click to download"^]:
[source,yaml]
----
include::./references/automatedInstallScripts/docker-compose-hsqldb.yml[]
----
====
'''

===== Required configuration

[[_compose_persistent_volumes]]
You can use the YAML Docker Compose definition as it is. However, to persist containers’ data,
it is necessary to establish local mounted volumes on the ProActive server and database VMs. Hence, you need to create 3 empty folders on the host machine named `default`, `previous`, and `backup`.

----
$ mkdir -p /opt/proactive/default /opt/proactive/previous /opt/proactive/backup
----


Then, you have to configure the absolute paths to these 3 folders in the `docker-compose-hsqldb.yml` file using the `device:` property (e.g., in the example above 3 folders are created and configured: `/opt/proactive/default`, `/opt/proactive/previous`, and `/opt/proactive/backup`).

----
...
volumes:
  proactive-default:
    ...
      device: '/opt/proactive/default'
  proactive-previous:
    ...
      device: '/opt/proactive/previous'
  proactive-backup:
    ...
      device: '/opt/proactive/backup'
----

These folders will host the current installation files (`default`), the upgraded one (`previous`), and all previous upgraded installation files (`backup`).

==== Docker Compose Configuration with a PostgreSQL Database

To use the Docker Compose configuration with a PostgreSQL database, ensure that the correct configuration options are set in the `.env` file beforehand.
Additionally, enable the advanced property `ENABLE_EXTERNAL_DATABASE_CONFIGURATION`.

Once done, you can then proceed with the Docker Compose configuration provided in the installation package `docker-compose-pg.yml` to launch the ProActive server container alongside an external PostgreSQL database container.

It will start two containers, one for the server, and one for the database.

'''
.*[underline blue]#Click here to display the docker-compose-pg.yml file#*
[example%collapsible]
====
link:./references/automatedInstallScripts/docker-compose-pg.yml[docker-compose-pg.yml, title="Click to download"^]:
[source,yaml]
----
include::./references/automatedInstallScripts/docker-compose-pg.yml[]
----
====
'''

[[_compose_required_configuration]]
===== Required configuration

Same as for a Docker Compose installation using an embedded database (See section <<_compose_persistent_volumes, persistent volumes with compose>>), you need to create and link an empty folder to host the Postgres `data` in addition to the basic three `default`, `previous`, and `backup` folders that host the ProActive installation(s).

----
$ mkdir -p /opt/proactive/default /opt/proactive/previous /opt/proactive/backup /opt/proactive/data
----

You then need to specify the folder paths in the Docker Compose configuration file `docker-compose-pg.yml`:

----
...
volumes:
  proactive-default:
    ...
      device: '/opt/proactive/default'
  proactive-previous:
    ...
      device: '/opt/proactive/previous'
  proactive-backup:
    ...
      device: '/opt/proactive/backup'
  proactive-data:
    ...
      device: '/opt/proactive/data'
----

==== Start ProActive Using Docker Compose

Login into the private Activeeon registry:
----
$ docker login dockerhub.activeeon.com
----

To start the ProActive server with a default HSQL database, run:
----
$ docker-compose -f docker-compose-hsqldb.yml up -d
----

To start the ProActive server with a PostgreSQL database, run:
----
$ docker-compose -f docker-compose-pg.yml up -d
----

To monitor the ProActive server startup, run:
----
$ docker logs -f proactive-scheduler
----

Wait until the command output shows that ProActive server is started:
----
....
Loading workflow and utility packages...
Packages successfully loaded
ProActive started in 246.476727488 seconds
----

You can then access the ProActive Web Portals at `<PWS_PROTOCOL>://<PWS_HOST_ADDRESS>:<PWS_PORT>/`. The URL property values are specified in the `.env` file.

The user can access any of the four portals using the default credentials: `admin/<PWS_LOGIN_ADMIN_PASSWORD>`. `PWS_LOGIN_ADMIN_PASSWORD` is, by default, `activeeon` but you can customize it as well in the `.env` file.

[[_compose_purge]]
==== Database Purge Operation in Docker Compose

In Postgres database, `vacuum` operations do not by default remove orphaned Large  Objects from the database. Identification and removal of these objects is performed by the `vacuumlo` command.
As the ProActive database contains many large objects, it is essential to apply both `vacuumlo` and `vacuum` operations in order to control the disk space usage over time.
The purge operation is automatically integrated and performed within the Postgres container using a `cron.daily` process.
Though, prior to executing the purge operation, a preliminary configuration step is necessary (this preliminary step is done once for a given ProActive installation).
The following commands allow to prepare the database for the automatic purge operations performed within the Postgres database container.

Stop the ProActive server container:
----
$ docker-compose stop proactive-server
----

Run the embedded script for table configuration within the database container:
----
$ docker exec -it proactive-database /bin/bash /tables-configuration.sh
----

Start the ProActive server again:
----
$ docker-compose start proactive-server
----

The installation of ProActive using Docker Compose is now over ! Your ProActive Scheduler is now running as a set of container services !

[[_compose_upgrade]]
==== Upgrade/Downgrade/Backup ProActive Using Docker Compose

ProActive can be easily upgraded using Docker Compose. We support automatic upgrade, downgrade, and backup of ProActive server installations in Docker.

Docker Compose configuration relies on Docker container volumes to host ProActive installation directories:

- The `proactive-default` volume hosts the current and default installation.

- The `proactive-previous` volume hosts the previous installation after upgrade.

- The `proactive-backup` volume hosts all previous installations after multiple upgrades.

In the following, we are going to explain each procedure.

===== ProActive upgrade
In order to upgrade the ProActive server to a newer version, you have to edit the `.env` config file to provide a newer version. The current ProActive version is specified using the `DOCKER_IMAGE_PROACTIVE_TAG` option in the input installation file.

For example, you can replace `DOCKER_IMAGE_PROACTIVE_TAG=13.0.0` by `DOCKER_IMAGE_PROACTIVE_TAG=13.0.8` to get a maintenance release of ProActive. It is also applicable for minor and major release versions.

Now that a newer version is provided, you can restart the container:

----
$ docker-compose -f docker-compose-hsqldb.yml down
$ docker-compose -f docker-compose-hsqldb.yml up -d
----

The new version will be detected and the upgrade process will start at container startup.
In fact, the current version will be automatically placed on the `proactive-previous` volume and the newer version will be placed on the `proactive-default` volume and started from there.

===== ProActive backup
The backup of previous ProActive versions is also automatic and it is triggered on every upgrade. When a new upgrade is started, the `proactive-previous` volume is checked and if it is not empty, its content is automatically moved to the backup volume `proactive-backup`.
Afterwards, the upgrade process starts as previously described.

===== ProActive downgrade
Using Docker Compose we also support one-level automatic downgrade of ProActive installations located in `proactive-default` and `proactive-previous` volumes.
To do so, you should provide the previous version of ProActive, as it is specified in the `proactive-previous` volume, to the `DOCKER_IMAGE_PROACTIVE_TAG` option in the `.env` file. This will allow to automatically swap the content of `proactive-default` and `proactive-previous` volumes.
ProActive will be then started from the `proactive-default` volume which will contain the last previous version.

[[kube_Automated_ProActive_Installation_with_Kubernetes]]
=== Automated ProActive Installation with Kubernetes

ProActive could be easily deployed and started on `AKS` (Azure) or `bare-metal` (On-premises) Kubernetes clusters. For this purpose, we provide a ready-to-use Kubernetes configuration that enables you running ProActive Workflows and Scheduling (PWS) as a set of pods.
It allows you to start the following pods:

*  ProActive server pod
* (Optional) ProActive static nodes pods
* (Optional) ProActive dynamic nodes pods
* (Optional) ProActive PostgreSQL database pod. The database pod is not started when using an embedded HSQL database.

The Kubernetes installation pulls the ProActive server, node, and database images from the https://dockerhub.activeeon.com/[Private Activeeon Docker Registry^].

To be able to pull images from the https://dockerhub.activeeon.com/[Private Activeeon Docker Registry^], a ProActive enterprise licence and access credentials are required. Please contact us at contact@activeeon.com to request access and support.

Here are the installation steps you need to follow in order to have ProActive running in a Kubernetes cluster:

==== Download the Helm Installation Package

https://helm.sh[Helm^] is a Kubernetes deployment tool for automating the creation, packaging, configuration, and deployment of applications and services to Kubernetes clusters.

ProActive uses Helm Charts (i.e., Helm package definition) to create and configure the kubernetes deployments. Thus, https://helm.sh[Helm^] must be pre-installed in order to deploy the ProActive pods.

Please refer to Helm's https://helm.sh/docs/[documentation^] to get started.

Once Helm is set up properly, proceed to download the Helm installation package of ProActive:

'''
++++
<pre><code><span id="helmInstaller"/></code></pre>
++++
'''

The next step is to configure the Helm Chart by setting up the right configuration in the input `values.yml` file of the Chart.

==== Configure the Helm Input File

The `values.yaml`, which is included in the installation package, is necessary to provide the right configuration values before installing the Chart.

'''
.*[underline blue]#Click here to display in Helm input file#*
[example%collapsible]
====
link:./references/automatedInstallScripts/values.yml[values.yaml, title="Click to download"^]:
[source,yaml]
----
include::./references/automatedInstallScripts/values.yml[]
----
====
'''

===== Required configuration

You can use the default parameter values provided in the `values.yaml` file. However, the following specific properties must be configured as outlined below:

- `target.cluster`: The target Kubernetes cluster for the installation. Possible values: `aks` or `on-prem`.
- `imageCredentials`: Contains the required credentials to be able to pull the ProActive images from the private Activeeon registry.
- `images`:
** `scheduler.repository`: A namespace of the ProActive scheduler image located in the Activeeon private registry.
** `scheduler.tag`: The ProActive scheduler image version or tag.
** `node.repository`: A namespace of the ProActive node image located in the Activeeon private registry.
** `node.tag`: The ProActive node image version or tag.
** `db.postgres.repository`: A namespace of the ProActive Postgres image located in the Activeeon private registry.
** `db.postgres.tag`: The ProActive Postgres image version or tag.
- To ensure having dynamic ProActive Kubernetes nodes after server startup, you need to copy and paste the target (`aks` or `on-prem`) cluster configuration into the `files/kube.config` file of the Helm installation package.


For an on-premises installation, the Kubernetes node name to host the ProActive scheduler and database pods:

- `persistentVolumes.localVolumeConfig`:
** `scheduler.persistentVolume.nodeAffinity.nodeName`: Node name
** `database.persistentVolume.nodeAffinity.nodeName`: Node name

For an on-premises installation, a directory must be created beforehand on the node defined by the `nodeAffinity` (see `nodeAffinity` above) to host the ProActive scheduler and database pods' data:

- `persistentVolumes.localVolumeConfig`:
** `scheduler.persistentVolume.localPath`: Absolute path to the empty directory created on the node
** `database.persistentVolume.localPath`: Absolute path to the empty directory created on the node

===== Advanced configuration

In addition to the required parameters, you can set more advanced parameters to your Helm chart. Many of the advanced parameters are already defined in <<_compose_advanced_parameters, Docker Compose advanced parameters section>>.

We will enumerate below few (but not all) important parameters that could be useful to customize your Kubernetes installation:

- `namespace`: Namespace to host all ProActive installation resources. By default, ae-dev.
- `ports`:
** `httpPort`: Http port. By default, 8080.
** `nodePort`: For on-premises installation, you can customize the Node Port. By default, 32000.
- `database`: You can enable or not using a separate Postgres pod for the database. By default, true.
- `usersConfiguration`: For more details, see section <<_compose_PWS_Users_Configuration_Properties, PWS Users Configuration Properties>>.
- `ldapConfiguration`: For more details, see section <<_compose_LDAP_Configuration_Properties, LDAP Configuration Properties>>.
- `nodeSources`:
** `dynamicK8s`: To enable or not having dynamic ProActive Kubernetes nodes. A minimum (`minNodes`) and maximum (`maxNodes`) number of nodes have to be defined. By default, it is enabled.
** `staticLocal`: To enable or not having static ProActive Kubernetes nodes. A static number of nodes (`numberNodes`) has to be defined. By default, it is enabled.
- `persistentVolumes`:
** `localVolumeConfig`: Set the local node volumes settings according to your needs if you are targeting on-premises clusters. By default, local node spaces are provisioned.
** `aksDiskVolumeConfig`: Set the Azure disk settings according to your needs if you are targeting AKS clusters. By default, Azure disks are provisioned for the scheduler and the database.
- `resources`: Set the resources requests for the scheduler, node, and database pods. By default, minimal settings are defined.
- To use https with TLS encryption, enable the use of ingress service. Note that you must first install or enable the Ingress and Cert Manager controllers in your cluster (refer to the commands below).
Then, within the same configuration block, enable and configure one of the three TLS certificates to be used: either a provided, a self-signed, or a Let's Encrypt certificate:
- `ingress`:
** `enabled`: Set to true to enable the ingress service. By default, false.
** `ingressClassName`: ingress class. Use `kubectl get ingressclass` to get the ingress class.
** `hostname`: hostname of the machine running the ProActive scheduler pod.
** `providedCertificate.enabled`: Set to true if you provide your own certificate.
*** `providedCertificate.tls.crt`: The actual content of the TLS certificate file (encoded in Base64). Use for example `cat fullchain.pem | base64` to get the encrypted certificate file content.
*** `providedCertificate.tls.key`: The actual content of the TLS private key file (encoded in Base64). Use for example `cat privkey.pem | base64` to get the encrypted private key file content.
** `clusterIssuer.selfSigned.enabled`: Set to true if you want to use a self-signed certificate.
** `clusterIssuer.letsEncrypt.enabled`: Set to true if you want to use a valid Let's Encrypt certificate.
*** `clusterIssuer.letsEncrypt.server`: The Let's Encrypt ACME server URL.
*** `clusterIssuer.letsEncrypt.email`: The email address to be associated with the Let's Encrypt account.

Set ingress controller:
----
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.0/deploy/static/provider/cloud/deploy.yaml
----
Set Cert manager controller:
----
$ kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.3/cert-manager.yaml
----

==== Start ProActive Using Helm

After setting the right configuration options in the `values.yaml` file, you can access the Helm installation package and then proceed to install the chart:

----
$ helm install pa-helm-chart .
----

After installing the Chart, all ProActive pods are deployed. Using the default configuration, you can monitor pods creation as follows:

----
$ kubectl get pod -n ae-dev
NAME                                                              READY   STATUS             RESTARTS   AGE
proactive-db-6f886f98f-cdcvs                                      1/1     Running            0          5h34m
proactive-deployment-657b5dc546-sv4v5                             1/1     Running            0          5h19m
dynamic-kube-linux-nodes-node-d4ka127srv-deployment-64bd6dklnj9   2/2     Running            0          5h15m
dynamic-kube-linux-nodes-node-kmrice1asv-deployment-59c87584vtv   2/2     Running            0          5h15m
dynamic-kube-linux-nodes-node-wrbhjijggy-deployment-7f49c4kw9rt   2/2     Running            0          5h15m
dynamic-kube-linux-nodes-node-0pgisrxfqd-deployment-fc6dc5l8jwn   2/2     Running            0          5h15m
node-deployment-74f979d95c-txd8w                                  2/2     Running            0          5h34m
----

To monitor ProActive server logs, use:
----
$ kubectl logs -f -n ae-dev proactive-deployment-657b5dc546-sv4v5
----
You can access the ProActive Web Portals, for example:

On `on-premises` installation:
----
http://on-prem-cluster-ip-or-dns:32000
----
(`32000` is the default `NodePort`)

On `AKS` installation:
----
http://aks-cluster-load-balancer-ip-or-dns:8080
----

The user can access any of the four portals using the default credentials: `admin/<proactiveAdminPassword>`. `proactiveAdminPassword` is defined in the `values.yaml` input file.


==== Database Purge Operation in Kubernetes
The Postgres database purge process is required on any fresh ProActive installation using the Postgres database, and it explained in section: <<_compose_purge, Docker Compose Purge Operation>>.

The following commands allow to prepare the database for the automatic purge operations performed within the Postgres database pod:

Delete the ProActive scheduler deployment in the namespace that was given to the `values.yaml` file. By default, `ae-dev` is used:
----
$ kubectl delete deploy -n ae-dev proactive-deployment
----

Run the embedded database pod script for table configuration. You need to replace `<proactive-db-id>` with your current ProActive database pod id:
----
$ kubectl exec -it -n ae-dev <proactive-db-id> -- /tables-configuration.sh
----

Start the Helm chart again:
----
$ helm upgrade pa-helm-chart . -f values.yaml
----

The installation of ProActive using Helm is now over ! Your ProActive Scheduler is now running as a set of pods !

==== Upgrade ProActive Using Helm
The process of upgrading ProActive in a containerized environment is described in section <<_compose_upgrade>>.

In this section, we will see how to upgrade ProActive in Kubernetes using Helm.

First, the new release version has to be provided in the `values.yaml` file:

- `images`:
** `scheduler.tag`: The new ProActive scheduler image version or tag.
** `node.tag`: The new ProActive node image version or tag.

Once set, we upgrade the installed Helm release using the following command:

----
helm upgrade pa-helm-chart . -f values.yaml
----

The Scheduler and node pods will be recreated to download the new images and apply the upgrade progress during their startup.

[[shell_Automated_ProActive_Installation_with_shell]]
=== Automated ProActive Installation with Shell Scripts
ProActive could be easily deployed and started automatically using Shell scripts. For this purpose, we provide a ready-to-use Shell installer that enables you running ProActive Workflows and Scheduling (PWS) as a Linux service.

Here are the installation steps you need to follow in order to have ProActive running in your environment:

==== Download the Shell Installation Package
We provide public access to the ProActive Shell installation package that you can easily use to configure and start ProActive as a service in your environment.

The installation package contains 4 files:

- The `inputConfig.json` file that includes the required parameters needed for the configuration tuning of the ProActive server. It is formatted as a json file.
- The `proactiveInstaller.sh` file, which is the main installation script. It parses the configuration provided in the `inputConfig.json` file, proceeds to the configuration, and finally starts up the ProActive server.
- The `apiHealthcheck.sh`, called by the `proactiveInstaller.sh` (at the end), to run REST API health check tests to verify the correct behavior of the ProActive server after its startup.
- The `install_configure_postgresql.sh` file, which installs the Postgres database and configures the ProActive server to support the installed database.

You can proceed to download the Shell installation package using the following command:

'''
++++
<pre><code><span id="shellInstaller"/></code></pre>
++++
'''

In order to install the ProActive server on your environment:

- You need first to configure the input parameters defined in the `inputConfig.json` (to better know how to configure the parameters, see next section: <<shell_Configure_the_installation_input_file, Configure the installation input file>>)
- Run the `proactiveInstaller.sh` installation script (see section <<_shell_Starting_ProActive_using_Shell_scripts, Starting ProActive using Shell scripts>>)

[[shell_Configure_the_installation_input_file]]
==== Configure the Installation Input File

The `inputConfig.json`, which is included in the installation package, is necessary to provide the right configuration values before installing ProActive.

'''
.*[underline blue]#Click here to display in Shell input file#*
[example%collapsible]
====
link:./references/automatedInstallScripts/inputConfig.json[inputConfig.json, title="Click to download"^]:
[source,json]
----
include::./references/automatedInstallScripts/inputConfig.json[]
----
====
'''

===== Prerequisites
In this section, we show how to tune the input configuration parameters needed for this installation. The configuration options are defined in the `inputConfig.json` file.
However, you should make sure to encrypt all password beforehand using the gpg encryption library with a passphrase (see the example below) and provide the encrypted passwords to the `inputConfig.json` file.
In this latter, the configuration properties with value `<encrypted_password>` needs to be replaced with the corresponding encrypted password value.
For sections that are not enabled in the configuration (for example, if you set `ENABLE_LDAP_AUTHENTICATION` or `ENABLE_USERS_CONFIGURATION` to `false`), there is no need to encrypt the passwords that these settings demand.
Though, make sure to use the same `PASSPHRASE` to encrypt all passwords. It will be used later on to run the ProActive installation (explained in section <<_shell_Starting_ProActive_using_Shell_scripts, Starting ProActive using Shell scripts>>)

----
$ ENCRYPTED_PWD=$(printf "clear_text_password" | gpg -c --batch --passphrase PASSPHRASE | base64)
$ echo $ENCRYPTED_PWD
$ jA0ECQMC1V3aYdEkNj3/0joBq9DUbBi2dnCCDtJPMosRdWh72KZ7Gm1cpctqOJnEKFGX1wO0v1qK 32b4aqmeyt6ylLmn5Y72GdTw

----

Now, we will provide a detailed explanation of the ProActive configuration parameters as they are specified in the input script `inputConfig.json`.

===== Required configuration
You can the use default parameter values as they are given in the `inputConfig.json` file. However, the following specific properties must be configured as described below:

- `INSTALLATION_CONFIGURATION`
** `ARCHIVE_LOCATION`: The archive location could be an absolute file path of the ProActive archive or an http(s) link.
** `INSTALLATION_DIRECTORY`: Path of the folder where ProActive will be installed. You can change the path according to your preference. This path will be created by the Shell installer. No need to create it in advance.

- `PWS_SERVER_CONFIGURATION`
** `PWS_HOST_ADDRESS`: Defines the internal hostname or IP address of the machine where ProActive will be installed.

===== Advanced configuration

For more advanced configuration of the ProActive server, please refer to the parameters listed below:

- `INSTALLATION_CONFIGURATION`
** `NEW_INSTALL_OR_UPGRADE`: Set to true when it is a first ProActive server installation or an installation upgrade.
** `LIBRARIES`: The libraries to be installed on the ProActive server machine. The `proactiveInstaller.sh` script will install them. Default libraries mentioned are necessary for the installation.

- `HEALTH_CHECK_CONFIGURATION`
** `ENABLE_POST_INSTALLATION_HEALTH_CHECK`: If enabled, the health check tests will be executed at the end of the `proactiveInstaller.sh` installation script. It is recommended to enable the health check tests.
** `RUN_HEALTH_CHECK_ONLY`: If enabled, only health check tests will be executed. It supposes that the installation is already done and that the ProActive server is already running. It is recommended to not activate it unless you would like to run the health checks separately.

- `PWS_SERVER_CONFIGURATION`

See details of `PWS_SERVER_CONFIGURATION` in this section: <<_compose_PWS_Server_Configuration_Properties, PWS Server Configuration Properties>>.

- `PUBLIC_ACCESS_CONFIGURATION`

See details of `PUBLIC_ACCESS_CONFIGURATION` in this section: <<_compose_Public_Access_Configuration_Properties, Public Access Configuration Properties>>.

- `TLS_CERTIFICATE_CONFIGURATION`

See details of `TLS_CERTIFICATE_CONFIGURATION` in this section: <<_compose_TLS_Certificate_Configuration_Properties, TLS Certificate Configuration Properties>>.

NOTE: The pem/crt/pfx certificate as well as the key have to be provided with their absolute paths. No need to place them in the backup folder.

- `LDAP_CONFIGURATION`

See details of `LDAP_CONFIGURATION` in this section: <<_compose_LDAP_Configuration_Properties, LDAP Configuration Properties>>.

- `PWS_USERS_CONFIGURATION`

See details of `PWS_USERS_CONFIGURATION` in this section: <<_compose_PWS_Users_Configuration_Properties, PWS Users Configuration Properties>>.

- `EXTERNAL_DATABASE_CONFIGURATION`

This section allows you to change the ProActive server database configuration in order to connect to an external database. If set to false, it will use the embedded HSQL database.
ProActive requires the following components to be mapped to different database schemas: Resource Manager, Scheduler, Catalog, Service Automation and Notification Service.

- `EXTERNAL_DATABASE_CONFIGURATION`
** `ENABLE_EXTERNAL_DATABASE_CONFIGURATION`: Set to true if you want to connect to an external Postres database.
** `DB_TYPE`: The external database to be used by ProActive, namely `postgresql`.
** `DB_DRIVER_LOCATION`: The Postgres driver location. It is recommended to keep the default value.
** `DB_APPLY_SCHEDULER_CONFIG_TO_ALL`: If set to true, the 5 ProActive components will be configured using the Scheduler DB configuration described below.
** `DB_SCHEDULER_CONFIG`: Configuration of the 'scheduler' component database
*** `DB_SCHEDULER_HOSTNAME`: Database hostname used (e.g. localhost, myserver) or IP address (e.g. 127.0.0.1, 192.168.12.1).
*** `DB_SCHEDULER_PORT`: Database port used (e.g. 9001, 5432).
*** `DB_SCHEDULER_USERNAME`: Database user name.
*** `DB_SCHEDULER_PASSWORD`: Database user encrypted password.
*** `DB_SCHEDULER_DIALECT`: Override default database vendor dialect. Defaults are: {hsqldb=org.hibernate.dialect.HSQLDialect, postgresql=org.hibernate.dialect.PostgreSQL94Dialect}.
*** `DB_SCHEDULER_SCHEMA_NAME`: Override the default database schema name for the selected component. Defaults names are { Resource Manager : "rm", Scheduler : "scheduler", Catalog : "catalog", Service Automation : "pca", Notification Service : "notification" }.
*** `DB_SCHEDULER_URL`: Sets the JDBC url that will be used to access the target component database. If set to default, the url will be inferred from --hostname, --port and optionally --schema-name options.

The description of the configuration options for the remaining ProActive components (`DB_RM_CONFIG`, `DB_CATALOG_CONFIG`, `DB_PSA_CONFIG`, `DB_NOTIFICATION_CONFIG`) is the same as described for the scheduler database above.

[[_shell_Starting_ProActive_using_Shell_scripts]]
==== Starting ProActive Using Shell Scripts

===== ProActive server installation

After configuring the appropriate settings as outlined in the previous section, execute the `proactiveInstaller.sh` script along with the `PASSPHRASE` used to encrypt the passwords.

----
$ chmod +x proactiveInstaller.sh
$ sudo ./proactiveInstaller.sh -p PASSPHRASE
----

===== Server monitoring and troubleshooting
While running the installation script, the script will continuously display logs about the in-progress configuration and installation. When the server is starting, you can access the ProActive service logs located at `/var/log/proactive/scheduler` to monitor the ProActive server startup status:
----
$ tail -f /var/log/proactive/scheduler
----

For advanced troubleshooting, you can access the detailed server logs located at the ProActive installation directory `INSTALLATION_DIRECTORY/default/logs/Scheduler.log` in order to investigate or report any issues. The `INSTALLATION_DIRECTORY` is set in the `inputConfig.json` file, default is `/opt/proactive`.

----
$ tail -f INSTALLATION_DIRECTORY/default/logs/Scheduler.log
----

Wait until the logs show:

----
....
Loading workflow and utility packages...
Packages successfully loaded

ProActive started in 246.476727488 seconds
----
===== ProActive web portals access

When the ProActive server is up, you can access the ProActive Web portals at:

If you are using a public address:
`PWS_PUBLIC_SCHEME://PWS_PUBLIC_HOSTNAME:PWS_PUBLIC_PORT`

If you are using a private address only:
`PWS_PROTOCOL://PWS_HOST_ADDRESS:PWS_PORT`

The login is `admin` and the password is given in the `inputConfig.json` file referenced as `PROACTIVE_ADMIN_PASSWORD`. You can also use the internal ProActive users configured in the input file if you have activated `ENABLE_USERS_CONFIGURATION` property (same, for the LDAP configuration).

===== Manage the ProActive Linux service lifecycle
When the server is up and running, it will start as a Linux service. Hence, you can manage its lifecycle using these basic commands:
----
# status of the ProActive Linux service
systemctl status proactive-scheduler

# stop the ProActive Linux service
systemctl stop proactive-scheduler

# start the ProActive Linux service
systemctl start proactive-scheduler
----

==== Upgrade ProActive Using Shell Scripts
The process of upgrading the ProActive server is described in this section: <<_compose_upgrade>>.

In this section, we will see how to upgrade the ProActive server using the provided Shell scripts.

A link to the new ProActive release has to be provided in the `inputConfig.json` file:

- `"ARCHIVE_LOCATION"`: Replace the old archive link with a new one.

Once set, we upgrade the installed version using the following command:

----
$ sudo ./proactiveInstaller.sh -p PASSPHRASE
----

The new version will be automatically detected, and then the upgrade process will be triggered.

=== Scheduler Life-Cycle Management

Under the Admin menu from the scheduler portal, the following administrator actions are proposed:

image::SchedulerPortalAdminMenu.png[Admin Menu,423,342,role=center]

* `Start` the scheduler: Jobs can be submitted. Getting the jobs results is possible. The scheduler can be stopped, frozen, paused or killed.
* `Stop` the scheduler: Jobs cannot be submitted anymore. Already running jobs run to completion, but not pending jobs. Getting the jobs results is possible. The scheduler can be started or killed.
* `Freeze` the scheduler: Every running tasks run to completion, but the running jobs wait for the scheduler to resume. The scheduler can be stopped, resumed or killed.
* `Resume` the scheduler: If the scheduler is Paused, Stopped or Frozen.
* `Pause` the scheduler: Every running jobs run to completion. The scheduler can be stopped, resumed or killed.
* `Kill` the scheduler: Similar to Ctrl-C. No interaction can be done anymore.
* `Shutdown` the scheduler: Every running tasks run to completion before terminating the scheduler.

== ProActive Scheduler configuration

All configuration files of ProActive Scheduler can be found under `PROACTIVE_HOME`.

=== Java Virtual Machine configuration

Various command-line tools shipped with ProActive (bin/proactive-server, bin/proactive-node) start a Java Virtual Machine.
The parameters of the JVM can be modified by editing DEFAULT_JVM_OPTS= in the corresponding script both in Linux and Windows.

For example, to set the maximum heap capacity on the JVM to 6GB in Linux:

Change the line
`DEFAULT_JVM_OPTS='"-server" "-Dfile.encoding=UTF-8" "-Xms4g"'`

into

`DEFAULT_JVM_OPTS='"-server" "-Dfile.encoding=UTF-8" "-Xms4g" "-Xmx6g"'`


=== General configuration

.ProActive Scheduler configuration files

|===
|Component | Description|File |Reference

.1+^.^|Scheduler
|Scheduling Properties|config/scheduler/settings.ini|<<_scheduler_properties>>

.1+^.^|Resource Manager
|Node management configuration|config/rm/settings.ini|<<_resources_manager_properties>>

.1+^.^|Web Applications
|REST API and Web Applications configuration|config/web/settings.ini|<<_rest_api_properties>>

.1+^.^|Networking
|Network, firewall, protocols configuration|config/network/node.ini, config/network/server.ini|<<_network_properties>>

.4+^.^|Security
|User logins and passwords|config/authentication/login.cfg|<<_file>>
|User group assignments|config/authentication/group.cfg|<<_file>>
|User permissions|config/security.java.policy-server|<<_user_permissions>>
|LDAP configuration|config/authentication/ldap.cfg|<<_ldap>>
|Keycloak configuration|config/authentication/keycloak.cfg|<<_keycloak>>

.6+^.^|Database
|Scheduler configuration|config/scheduler/database.properties|<<Database configuration>>
|Resource Manager configuration|config/rm/database.properties|<<Database configuration>>
|Scheduling-api microservice|dist/war/scheduling-api/WEB-INF/classes/application.properties|<<Database configuration>>
|Job-planner microservice|dist/war/job-planner/WEB-INF/classes/application.properties|<<Database configuration>>
|Catalog microservice|dist/war/catalog/WEB-INF/classes/application.properties|<<Database configuration>>
|Cloud-Watch microservice|dist/war/proactive-cloud-watch/WEB-INF/classes/application.properties|<<Database configuration>>

|===

=== Database configuration

Scheduler, Resource Mananger, and some microservices (*catalog*, *cloud-automation-service*, *notification-service*) require to have direct access to the database.
Thus, they must each have a correct configuration for each supported database (HSQLDB, Postgres, MySQL, Oracle).

To configure Scheduler or Resource Mananger, one have to modify *PROACTIVE_HOME/config/scheduler/database.properties*
and *PROACTIVE_HOME/config/rm/database.properties* respectively. For microservices, you have also to modify files in: *PROACTIVE_HOME/config/MICROSERVICE-NAME/database.properties*.

TIP: The `PROACTIVE_HOME/tools/configure-db` command can be used to apply a database configuration. See <<_configure_database_command>>

For *HSQLDB*, the following five properties must be configured:
[source]
----
hibernate.connection.driver_class=org.hsqldb.jdbc.JDBCDriver

#DB_NAME could be scheduler, rm, or one of the microservices names
#9001 is the defaut DB port of the embedded PA database
hibernate.connection.url=jdbc:hsqldb:hsql://localhost:9001/DB_NAME

hibernate.dialect=org.hibernate.dialect.HSQLDialect

# Username and password
hibernate.connection.username=username
hibernate.connection.password=password
----

For *Postgres 12.2*, the following template must be configured:
[source]
----
hibernate.connection.driver_class=org.postgresql.Driver

#DB_NAME could be scheduler, rm, or one of the microservices names
#PORT should correspond to your DB port
hibernate.connection.url=jdbc:postgresql://localhost:PORT/DB_NAME

#PostgreSQLDialect is recommended for Postgres 12.2 version
hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect

# Username and password
hibernate.connection.username=username
hibernate.connection.password=password
----

For *MySQL 8.0*, the following template must be configured:
[source]
----
hibernate.connection.driver_class=com.mysql.jdbc.Driver

#DB_NAME could be scheduler, rm, or one of the microservices names
#PORT should correspond to your DB port
hibernate.connection.url=jdbc:mysql://localhost:PORT/DB_NAME

#MySQL5InnoDBDialect is recommended for MySQL 8.0 version
hibernate.dialect=org.hibernate.dialect.MySQL5InnoDBDialect

# Username and password
hibernate.connection.username=username
hibernate.connection.password=password
----

For *Oracle 12.2*, the following template must be configured:
[source]
----
hibernate.connection.driver_class=oracle.jdbc.driver.OracleDriver

#DB_NAME could be scheduler, rm, or one of the microservices names
#PORT should correspond to your DB port
hibernate.connection.url=jdbc:oracle:thin:@localhost:PORT:DB_NAME

#Oracle12cDialect is recommended for Oracle 12.2 version
hibernate.dialect=org.hibernate.dialect.Oracle12cDialect

# Username and password
hibernate.connection.username=username
hibernate.connection.password=password
----

The *job-planner*, *catalog*, *scheduling-api*, *cloud-automation-service*, *notification-service* and *proactive-cloud-watch* microservices must contain a database configuration.
For each microservice, a configuration file can be found in: *PROACTIVE_HOME/dist/war/MICROSERVICE-NAME/WEB-INF/classes/application.properties*.
In their *application.properties* file, you need to set the following five properties for the target database as configured just above:

* `spring.datasource.url`
* `spring.datasource.username`
* `spring.datasource.password`
* `spring.datasource.driverClassName`
* `spring.jpa.database-platform`

Moreover, it is mandatory to put the proper database driver in *PROACTIVE_HOME/addons* folder.

For example:

* For HSQLDB 2.4, the drive `hsqldb-2.4.1.jar` is already embedded in *PROACTIVE_HOME/addons*
* For MySQL 8.0, you have to get `mysql-connector-java-8.0.19.zip` driver an place it in *PROACTIVE_HOME/addons*
* For Postgres 12.2, you have to get `postgresql-42.2.11.jar` driver.
* For Oracle 12.2, you have to get `ojdbc8-19.3.0.0.jar` driver.

TIP: If you plan to use another DB version, it is important to get the corresponding database driver version and set up the right dialect version

Moreover, it is also important to notice that the Scheduler and 3 microservices (*scheduling-api*, *job-planner*, and *proactive-cloud-watch*) have to point to the same scheduler database since they use that one.

==== Example configuration with PostgreSQL
Configuring ProActive with an external database such as PostgreSQL is recommended in a production environment.
In the following, we show an example of PostgreSQL installation and configuration to be used along with ProActive.

===== PostgreSQL version
PostgreSQL versions 12.0 to 12.11 are fully compatible with ProActive. More recent versions of PostgreSQL have not been fully tested yet for compatibility.
PostgreSQL server can be installed on the same machine as the ProActive server or on a separate machine.

===== Initialize the database
To initialize the database, some SQL queries should be executed in the PostgreSQL server machine.
In order to do so, the following SQL queries can be defined in a SQL initialization script (default users passwords should be modified):

.init.sql
[source,sql]
----
-- Create users
CREATE USER rm WITH PASSWORD 'rm';
CREATE USER scheduler WITH PASSWORD 'scheduler';
CREATE USER catalog WITH PASSWORD 'catalog';
CREATE USER pca WITH PASSWORD 'pca';
CREATE USER notification WITH PASSWORD 'notification';

-- Creating empty databases
CREATE DATABASE rm OWNER rm;
CREATE DATABASE scheduler OWNER scheduler;
CREATE DATABASE catalog OWNER catalog;
CREATE DATABASE pca OWNER pca;
CREATE DATABASE notification OWNER notification;

-- Grant access
GRANT CONNECT ON DATABASE scheduler TO scheduler;
GRANT USAGE ON SCHEMA public TO scheduler;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO scheduler;
GRANT CONNECT ON DATABASE rm TO rm;
GRANT USAGE ON SCHEMA public TO rm;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO rm;
GRANT CONNECT ON DATABASE catalog TO catalog;
GRANT USAGE ON SCHEMA public TO catalog;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO catalog;
GRANT CONNECT ON DATABASE pca TO pca;
GRANT USAGE ON SCHEMA public TO pca;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO pca;
GRANT CONNECT ON DATABASE notification TO notification;
GRANT USAGE ON SCHEMA public TO notification;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO notification;

ALTER SYSTEM SET max_connections ='500';
----

When the PostgreSQL server is installed on a separate machine, the `listen-addresses` setting should be set to allow remote connections:

[source, bash]
----
sudo vim /opt/postgres/data/postgresql.conf
listen-addresses = ‘IP_ADDRESS’
----

Then, execute the initialization script inside the PotsgreSQL database:

[source, bash]
----
psql -f init.sql
----

===== Download the Postgres driver
The Postgres driver has to be downloaded and added inside the addons folder of ProActive (SCHEDULER_PATH/addons/).

The link to download a Postgres driver version compatible with Postgres 12.X is: `https://jdbc.postgresql.org/download/postgresql-42.2.11.jar`

===== Configure ProActive
To configure ProActive to connect to the Postgres DB, we need to edit the database configuration files as follows.

NOTE: The database passwords should be modified according to your chosen passwords. Note that passwords can be encrypted in configuration files as explained in section <<_encrypting_configuration_passwords>>

[source, bash]
----
#To easily get the configuration file names, you can execute the following command:
cd SCHEDULER_PATH/dist/war
find . -name ‘database.properties’
----

[source, bash]
----
#Scheduler config (SCHEDULER_PATH/config/scheduler/database.properties)
hibernate.connection.driver_class=org.postgresql.Driver
hibernate.connection.url=jdbc:postgresql://PG_SERVER_DNS:5432/scheduler
hibernate.dialect=org.hibernate.dialect.PostgreSQL94Dialect
hibernate.connection.username=scheduler
hibernate.connection.password=scheduler

#RM config (SCHEDULER_PATH/config/rm/database.properties)
hibernate.connection.driver_class=org.postgresql.Driver
hibernate.connection.url=jdbc:postgresql://PG_SERVER_DNS:5432/rm
hibernate.dialect=org.hibernate.dialect.PostgreSQL94Dialect
hibernate.connection.username=rm
hibernate.connection.password=rm

#Catalog config (SCHEDULER_PATH/dist/war/catalog/WEB-INF/classes/application.properties)
spring.datasource.driverClassName=org.postgresql.Driver
spring.datasource.url=jdbc:postgresql://PG_SERVER_DNS:5432/catalog
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQL94Dialect
spring.datasource.username=catalog
spring.datasource.password=catalog

#Scheduling config (SCHEDULER_PATH/dist/war/scheduling-api/WEB-INF/classes/application.properties)
spring.datasource.driver-class-name=org.postgresql.Driver
spring.datasource.url=jdbc:postgresql://PG_SERVER_DNS:5432/scheduler
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQL94Dialect
spring.datasource.username=scheduler
spring.datasource.password=scheduler

#Job planner config (SCHEDULER_PATH/dist/war/job-planner/WEB-INF/classes/application.properties)
spring.datasource.driver-class-name=org.postgresql.Driver
spring.datasource.url=jdbc:postgresql://PG_SERVER_DNS:5432/scheduler
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQL94Dialect
spring.datasource.username=scheduler
spring.datasource.password=scheduler

#PCW config (SCHEDULER_PATH/dist/war/proactive-cloud-watch/WEB-INF/classes/application.properties)
spring.datasource.driver-class-name=org.postgresql.Driver
spring.datasource.url=jdbc:postgresql://PG_SERVER_DNS:5432/scheduler
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQL94Dialect
spring.datasource.username=scheduler
spring.datasource.password=scheduler

#PCA config (SCHEDULER_PATH/dist/war/cloud-automation-service/WEB-INF/classes/application.properties)
spring.datasource.driverClassName=org.postgresql.Driver
spring.datasource.url=jdbc:postgresql://PG_SERVER_DNS:5432/pca
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQL94Dialect
spring.datasource.username=pca
spring.datasource.password=pca

#Notification config (SCHEDULER_PATH/dist/war/notification-service/WEB-INF/classes/application.properties)
spring.datasource.driverClassName=org.postgresql.Driver
spring.datasource.url=jdbc:postgresql://PG_SERVER_DNS:5432/notification
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQL94Dialect
spring.datasource.username=notification
spring.datasource.password=notification
----

===== Database purge operation
Additionally, a `vacuum` operation does not by default remove orphaned Large Objects from the database.
Identification and removal of these objects is performed by the `vacuumlo` command.

As the ProActive database contains many large objects, it is essential to apply both `vacuumlo` and `vacuum` operations in order to control the disk space usage over time.

In addition, prior to execute the purge operation, a preliminary configuration step is necessary (this preliminary step is done once for a given ProActive installation).

In the following paragraphs, we describe the step by step procedure to perform the database purge operations:

- **Start the ProActive server to fill up the database**

Initially, the ProActive server has to be started **at least once** to fill up the ProActive database.

- **Stop the ProActive server**

Then, once the ProActive server is up and the web portals become accessible, you have to stop the ProActive server.

PotgreSQL database must remain running in order to perform the next operations.

- **Prepare the database purge operation**

While the ProActive server is stopped and the Postgres server is running, the following SQL queries have to be executed in order to alter the type of certain columns in several database tables (this is needed due to a PotsgreSQL driver issue):

.tables-configuration.sql
[source,sql]
----
# rm database
\c rm
alter table nodesourcedata alter column infrastructuretype TYPE oid USING infrastructuretype::oid;
\q


# scheduler database
\c scheduler
alter table environment_modifier_data alter column value TYPE oid USING value::oid;
alter table job_data alter column input_space TYPE oid USING input_space::oid;
alter table job_data alter column out_space TYPE oid USING out_space::oid;
alter table job_data alter column global_space TYPE oid USING global_space::oid;
alter table job_data alter column user_space TYPE oid USING user_space::oid;
alter table job_data alter column description TYPE oid USING description::oid;
alter table job_data_variable alter column variable_value TYPE oid USING variable_value::oid;
alter table job_data_variable alter column variable_model TYPE oid USING variable_model::oid;
alter table job_data_variable alter column variable_description TYPE oid USING  variable_description::oid;
alter table script_data alter column script TYPE oid USING script::oid;
alter table script_data alter column url TYPE oid USING url::oid;
alter table selection_script_data alter column script TYPE oid USING script::oid;
alter table selection_script_data alter column url TYPE oid USING url::oid;
alter table task_data alter column description TYPE oid USING description::oid;
alter table task_data alter column java_home TYPE oid USING java_home::oid;
alter table task_data alter column working_dir TYPE oid USING working_dir::oid;
alter table task_data_variable alter column variable_value TYPE oid USING variable_value::oid;
alter table task_data_variable alter column variable_model TYPE oid USING variable_model::oid;
alter table task_data_variable alter column variable_description TYPE oid USING variable_description::oid;
alter table plannedworkflow alter column failure_message TYPE oid USING failure_message::oid;
alter table rule_error alter column error_message TYPE oid USING error_message::oid;
alter table submittedrule alter column failure_message TYPE oid USING failure_message::oid;
\q

# pca database
\c pca
alter table variable alter column name TYPE oid USING name::oid;
alter table variable alter column value TYPE oid USING value::oid;
\q
----

- **Restart the ProActive server**

After editing the table columns as described above, you have to start again the ProActive server to take into account the new modifications made to the Postgres database.

- **Database purge operation**

Now, while the ProActive server is running, it is possible to execute a full `vacuum` by using the following commands:

[source,sql]
----
# rm database
vacuumlo -v rm
vacuumdb -f -v -d rm -t pg_largeobject

# scheduler database
vacuumlo -v scheduler
vacuumdb -f -v -d scheduler -t pg_largeobject

# pca database
vacuumlo -v pca
vacuumdb -f -v -d pca -t pg_largeobject

# check autovacuum
SHOW autovacuum;
----

NOTE: The command `vacuumdb` is executed implicitly if `AutoVacuum` is enabled.
The command `vacuumlo` consumes a lot of RAM and disk space depending on the number of large objects to be removed.

=== Data Spaces configuration

link:../user/ProActiveUserGuide.html#_data_spaces[ProActive Data Spaces] is an integrated file-transfer mechanism used by ProActive workflows.

Data Spaces are used to transfer files between the server and <<_glossary_proactive_node,ProActive Nodes>> during workflows tasks execution.

ProActive Data Spaces implementation is based on link:https://commons.apache.org/proper/commons-vfs/[Apache VFS].

The ProActive server defines and starts automatically the following data spaces:

 * A *Global Space* where anyone can read/write files.
 * An *User Space* which is a personal user data storage.

Global or User Spaces can be configured individually to use one of the following Data Space protocols.

==== ProActive protocol

This is the *default* protocol used by the Global and User Spaces. The ProActive Data Space protocol is able to transfer files either by:

 * a direct copy operation on the file system
 * a network copy operation through the currently configured <<_available_network_protocols,ProActive Network Protocol>>

Accordingly, a ProActive Node connected to the ProActive server and sharing the same file system (e.g. NFS) will be able to transfer files directly through the file system.
A ProActive Node connected to the server from a different environment (virtual machine, cloud instance, docker container, etc) will also be able to transfer files to/from the server thanks to the ProActive Data Space protocol.

The ProActive Data Space protocol will use the network protocol configured for the ProActive server (see <<_network_properties>>).

In order to configure the data spaces paths on the ProActive server, you can set the following properties in the `PROACTIVE_HOME/config/scheduler/settings.ini` file:

....
pa.scheduler.dataspace.defaultglobal.localpath=/path/to/global/space
pa.scheduler.dataspace.defaultuser.localpath=/path/to/user/space/root
....

The user space will use the path defined in `pa.scheduler.dataspace.defaultuser.localpath` as its root and create sub-folders according to each user login name.

When these properties are not configured, global and user spaces default locations are:

 * Global Space : `PROACTIVE_HOME/data/defaultglobal`
 * User Space : `PROACTIVE_HOME/data/defaultuser`

==== SFTP protocol

The SFTP protocol can be used instead of the ProActive protocol to deploy the User or Global spaces.

ProActive User Space mapped using SFTP has the following benefits:

 * It allows to map each User Space to the *HOME directory* of the user on the SFTP server.
 * It allows *file impersonation* : a file copied to the User Space will belong to the user system account and not to the ProActive server account.

It has the following constraints and limitations:

 * Every user's credentials of the ProActive server must match system users credentials inside the SFTP server. This is typically achieved by enabling LDAP or PAM authentication on the ProActive server (see <<_user_authentication>>).
 * Every file transfer will use the SFTP protocol when it's configured. Note that SFTP protocol is less efficient than the ProActive protocol when transferring files onto and from the local file system.

The SFTP protocol can also be used for the Global Space. In that case, all file transfers will be accomplished by a single system account.
Motivations to enable SFTP on the Global Space can be to access a folder:

 * on a remote server, reachable via SSH.
 * on the local file system, in case this folder belongs to a different user from the ProActive server user.

In order to configure the SFTP protocol for the User Space, the following properties in the `PROACTIVE_HOME/config/scheduler/settings.ini` file must be defined:

....
pa.scheduler.dataspace.defaultuser.url=sftp://your-server
pa.scheduler.dataspace.defaultuser.impersonation=true
....
The user spaces will always be mapped to each user HOME directory on the SFTP server. In order to have more flexible mappings, consider using the <<_vsftp_protocol>>.

In order to configure the SFTP protocol for the Global Space, the following property must be set:

....
pa.scheduler.dataspace.defaultglobal.url=sftp://username:password@your-server/path
....

==== VSFTP protocol

The VSFTP protocol is a proprietary extension inside ProActive of the SFTP protocol.
It complements the SFTP protocol by adding the ability to configure SFTP root directories with a set of predefined environment variables.

By default, only the `HOME` environment variable is included. Other environment variables can be added using the java property `proactive.vfsprovider.vsftp.var_names`.

Each environment variable must represent a directory path inside the SFTP server. Each variable must contain a value for all users of the SFTP server.

For example, suppose that `proactive.vfsprovider.vsftp.var_names=HOME,WORK,SCRATCH` and that these environment variables are resolved for `user1` as follows:

....
HOME=/home/user1
WORK=/work
SCRATCH=/scratch/user1
....

The VSFTP protocol will then be able to resolve inside the SFTP server the following urls:

....
vsftp://your-server/$HOME/file1 => /home/user1/file1
vsftp://your-server/$WORK/file2 => /work/file2
vsftp://your-server/$SCRATCH/file3 => /scratch/user1/file3
....

Additionally, when listing the contents of the server root directory, it will return a list corresponding to the configured variables:

....
LIST vsftp://your-server/ => [$HOME, $WORK, $SCRATCH]
....

`proactive.vfsprovider.vsftp.var_names` thus defines the different folder roots of the SFTP protocol for all users.

In order to configure the VSFTP protocol for the User Space, the following properties in the `PROACTIVE_HOME/config/scheduler/settings.ini` file must be defined:

....
pa.scheduler.dataspace.defaultuser.url=vsftp://your-server
pa.scheduler.dataspace.defaultuser.impersonation=true
....

Additionally, the following Java properties must be added to the ProActive server and all ProActive Nodes connected to the server:

 * `proactive.vfsprovider.vsftp.var_names` : a comma-separated list of environment variables which will be used as vsftp root folders for all users. Default value `HOME`.
 * `proactive.vfsprovider.vsftp.var_command` (optional) : the configured command will be executed as the current user inside the SFTP server to retrieve the environment variable(s) value (s).
Depending on how the environment variable is defined on the SFTP server (.bashrc, .profile or .ssh/environment files), it is necessary to adapt the command to make sure the appropriate environment script is sourced.
The property uses the %VAR% pattern which will be replaced dynamically by the variable name. Default value is `echo $%VAR%`. Example definition `proactive.vfsprovider.vsftp.var_command='source $HOME/.bashrc & echo $%VAR%'`.

In order to add the above properties to the ProActive server and local Nodes, you can edit the following files and include these properties:

 * `PROACTIVE_HOME/config/network/server.ini`
 * `PROACTIVE_HOME/config/network/node.ini`

In order to add these properties to remote ProActive Nodes, the procedure will depend on the deployment strategy:

 * Nodes started manually: you can add these properties in the command line. For example, `./proactive-node -Dproactive.vfsprovider.vsftp.var_names=HOME,WORK (...)`.
 * Nodes started using ProActive Linux Agents: the agent configuration file `/opt/proactive-agent/config.xml` needs to be edited to include the properties in the `<jvmParameters>` section.
For example:
```xml
        <jvmParameters>
            <param>-Dproactive.useIPaddress=true</param>
            <param>-Dproactive.vfsprovider.vsftp.var_names=HOME,WORK</param>
        </jvmParameters>
```
 * Nodes started using the ProActive Windows Agents: using the Windows Agent GUI, the properties must be added in the *JVM option* section. See <<_configure_agents_on_windows>>.
 * Nodes started using ProActive Resource Manager <<_glossary_node_source_infrastructure, Infrastructures>>: the properties needs to be added in the corresponding infrastructure parameter which allows to define java properties. See <<_node_source_infrastructures>> for further details.

[[_backup]]
=== Configure automated backup

ProActive Workflows & Scheduling features an automated backup mechanism which can be used to save the server state at regular intervals. Usage of this mechanism will depend on the database in use by the server:

* In case the default database is used (HSQLDB), the backup mechanism can store the database contents, data and log files of the server.
* In case another database is used (MySQL, Oracle, etc), the integrated backup mechanism can only
store data and log files. Backup of the database contents will need
to be activated separately, using the database vendor backup mechanism.

To use the automated backup, edit the file `PROACTIVE_HOME/config/shared/settings.ini`.
Enable backup mechanism by setting `pa.server.backup` to true.
Backup will be performed according to the provided cron expression in `pa.server.backup.period`.
Set file/folders you want to backup into `pa.server.backup.targets`.
Backup archives will appear in the folder specified by `pa.server.backup.destination`.
`pa.server.backup.windows` controls how many backup archives needs to be kept in the destination folder.

NOTE: The backup mechanism automatically freezes the scheduler
while performing the backup to guarantee data integrity.
It expects currently running tasks to terminate in a reasonable amount of time
(configurable using the pa.server.backup.possible.delay property). In case a currently running
task does not terminate after the expected delay, backup will not be performed.

=== Encrypting configuration passwords
Some configuration settings such as *database passwords* should be encrypted to prevent their exposition.
In order to encrypt any configuration setting, the command-line `PROACTIVE_HOME/tools/encrypt` or `PROACTIVE_HOME\tools\encrypt.bat` can be used.

Here is an example of usage:

----
encrypt -d mypass
Encrypted value (to use in configuration files):
ENC(/b0izhUHO846/GmAn8Hbhg==)
----

The interactive mode can also be used:

----
encrypt -i
Data to encrypt:*****
Encrypted value (to use in configuration files):
ENC(xtlMWGd0ZK/EzTMyrgPiPw==)
----
The encrypted value can then be used as displayed by the `encrypt` command, for example, in *PROACTIVE_HOME/config/scheduler/database.properties*:
`hibernate.connection.password=ENC(xtlMWGd0ZK/EzTMyrgPiPw==)`

=== Configure Global Variables and Generic Information

ProActive Server allows to configure workflow variables or generic information that will apply to all submitted workflows or to some categories of workflows (e.g. workflows with a given name).

This configuration is available in the file *PROACTIVE_HOME/config/scheduler/global_variables.xml*.


The default configuration is the following:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<globalvariables
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns="urn:proactive:globalvariables:3.13" xsi:schemaLocation="urn:proactive:globalvariables:3.13 http://www.activeeon.com/public_content/schemas/proactive/globalvariables/3.13/globalvariables.xsd">
        <filter>
            <select>
                <!-- Xpath expressions are used to filter workflows affected by global variables.
                Example: to select workflows belonging to the basic-examples bucket
                <xpath><![CDATA[/job/genericInformation/info[@name='bucketName' and @value='basic-examples']]]></xpath>
                -->
                <xpath><![CDATA[.]]></xpath>
                <!-- more than one xpath expression can be added. In that case, global variables are applied when all xpath expressions match -->
            </select>
            <!-- add global variables or generic information below -->
            <variables>
            </variables>
            <genericInformation>
            </genericInformation>
        </filter>
        <!-- Multiple filters can be added, to apply different set of global variables to different workflows -->
</globalvariables>
```

This configuration file is composed of `<filter>` sections. Each filter section describes the global variables and generic information that will be added to a certain category of workflows.
Multiple filter sections allow to add different lists of variables to different types of workflows.

A `<filter>` section contains the following subsections:

 * `<select>`: it contains one or more https://en.wikipedia.org/wiki/XPath[xpath] expressions to filter workflow according to xml rules.
In order to select a given workflow, the xpath expression evaluated on this workflow must return a non-empty node set (see xpath documentation). The default xpath expression `<xpath><![CDATA[.]]></xpath>` selects any workflow.
If multiple xpath expressions are included, a given workflow must satisfy all expressions in order to be selected. Examples of `<select>` section:
+
.Select all workflows with name="CronJob"
[source,xml]
----
<select>
    <xpath><![CDATA[/job[@name='CronJob']]]></xpath>
</select>
----
+
.Select all workflows in the "basic-examples" catalog bucket
[source,xml]
----
<select>
    <xpath><![CDATA[/job/genericInformation/info[@name='bucketName' and @value='basic-examples']]]></xpath>
</select>
----

 * `<variables>`: it contains the list of variables that will be added to the selected workflows. The format of this section corresponds to the same format as the `<variables>` section in the ProActive workflow XML definition. See link:../user/ProActiveUserGuide.html#_job_variables[Job Variables] for a description of this format.

 * `<genericInformation>`: it contains the list of generic information that will be added to the selected workflows. The format of this section corresponds to the same format as the `<genericInformation>` section in the ProActive workflow XML definition. See link:../user/ProActiveUserGuide.html#_generic_information[Generic Information] for a description of this format.

The following important rules apply to global variables and generic information:

 * *Global variables reload*: the definition of global variables and generic information is reloaded every 10 minutes by the ProActive server. It is thus possible to modify the *PROACTIVE_HOME/config/scheduler/global_variables.xml* configuration and have the modifications taken into account without restarting the server.
 * *Masking rule*: if a global variable named `VAR1` is defined and a workflow which contains the same variable is submitted, then the definition of `VAR1` inside the workflow will take precedence over the global variable.
 * *Variable references*: a global variable can reference another global variable using the `${VAR_NAME}` syntax. A global generic information can also reference a global variable (but a global generic information cannot reference another global generic information). Examples:
+
[source,xml]
----
            <variables>
                <variable name="var1" value="value" model=""/>
                <variable name="var2" value="some_${var2}" model=""/> <!-- var2 references var1 -->
            </variables>
            <genericInformation>
                <info name="ginfo1" value="${var1}"/> <!-- ginfo1 references var1 -->
                <info name="ginfo2" value="${ginfo1}"/> <!-- INVALID, a generic information cannot reference another generic info-->
            </genericInformation>
----

=== Manage job labels

A label is a property that can be attached to a job, and changed dynamically by end-users.
The *Labels*  give the possibility to tag Jobs, group them and manipulate them by filtering based on these custom labels.
These Labels act as a custom status for the Jobs. For example, a company can label jobs which are having errors to be reviewed by operators (label "to-be-reviewed"), and later mark them as "reviewed" when done.
Job labels can be used for this specific purpose, but for any other purpose.

In order to use Labels, the Administrators have to define a list of labels that can be applied to jobs.
This has to be done in the *Admin* menu of the <<_glossary_scheduler_web_interface,*Scheduler Portal*>>. In practice, possible labels are stored in the scheduler database.
An Admin can add, change, remove authorized labels.

Once a label is applied to a job, it will be stored alongside the job information in the scheduler database. If, in the future, the administrator changes the label name or deletes it, this operation will not be applied retroactively (the original label attached to the job will not be modified).

image::manage_labels.png[Manage Labels]

In User Guide, link:../user/ProActiveUserGuide.html#_adding_labels_to_jobs[*Adding Labels to Jobs*]

The constraints of a label name can be configured in the `PROACTIVE_HOME/config/scheduler/settings.ini` file.

The maximum length is specified by the property `pa.scheduler.label.max.length`.
If this property is not specified, the default value is ``20``.

The regular expression constraint is specified by the `property pa.scheduler.label.regex`.
If this property is not specified, the default value is ``^[a-zA-Z0-9_/-]*$``.

=== Custom logo configuration

image::generic_logo.png[align="center"]

To be able to edit the default logo displayed in ProActive portals, you have to replace the following file `PROACTIVE_HOME/dist/war/getstarted/assets/image/custom-logo.png` with your company custom logo.

You can use the command SCP to copy the file from your local machine to the ProActive server machine. The dimensions of the custom logo are automatically adapted.

Please note that there's no need to restart the server, just clear the cache and refresh the Web page.

image::custom_logo.png[align="center"]

=== Configuration Tools

==== Configure Http Server Command

The command line `PROACTIVE_HOME/tools/configure-http` allows configuring the main parameters of the *http server*.

 * The _protocol_ (scheme).
 * The _port_.
 * The _hostname_ or _IP address_ used by internal http communications (when a specific hostname must be configured rather than localhost).
 * a _public url_ (when the ProActive server is behind a reverse proxy or installed in a cloud instance).

The command options with several usage examples are described below:

----
$> ./configure-http -h
usage: configure-http [-d] [-h] [-H <HOSTNAME>] [-P <PORT>] [--public-hostname <PUBLIC-HOSTNAME>] [--public-port <PUBLIC-PORT>]
[--public-scheme <PUBLIC-SCHEME>] [-S <SCHEME>]

Change ProActive Jetty (Web) Server parameters and apply it to all configuration files.
This command can be used, for example, to change the ProActive server HTTP port or switch the server to the HTTPS protocol.

 -d,--debug                               Debug mode (prints modified files and properties)
 -h,--help                                Display this help
 -H,--hostname <HOSTNAME>                 Hostname used (e.g. localhost, myserver) or IP address (e.g. 127.0.0.1, 192.168.12.1). When
                                          this option is not provided, the hostname appearing in existing configuration files urls will
                                          be unchanged.
 -P,--port <PORT>                         Port used (e.g. 8080, 8443). When this option is not provided, the port configured for the
                                          current scheme (http or https) will be used.
    --public-hostname <PUBLIC-HOSTNAME>   Public Hostname used (e.g. myserver.mydomain) or IP address (e.g. 192.168.12.1). This setting
                                          should be used when the ProActive server is deployed behind a reverse proxy or inside a cloud
                                          instance. This option must be defined when any of public-scheme or public-port is set.
    --public-port <PUBLIC-PORT>           Public port used (e.g. 8080, 8443). This setting should be used when the ProActive server is
                                          deployed behind a reverse proxy or inside a cloud instance. This option must be defined when
                                          any of public-scheme or public-hostname is set.
    --public-scheme <PUBLIC-SCHEME>       Public protocol used (http or https). This setting should be used when the ProActive server
                                          is deployed behind a reverse proxy or inside a cloud instance. This option must be defined
                                          when any of public-hostname or public-port is set.
 -S,--scheme <SCHEME>                     Http protocol to use (http or https). When this option is not provided, the currently
                                          configured protocol will remain unchanged.

Examples:
# Configure scheme to https with port 8444 and default hostname. Debug output to see all modifications.
configure-http -S https -P 8444 -d
# Configure scheme to http with default port and specific hostname
configure-http -S http -H myserver
# Configure public address to use the server behind a reverse-proxy
configure-http --public-scheme=https --public-hostname=myproxy.mycompany.com --public-port=8443
----

See <<_rest_api_properties>> for the full description of the http server configuration.

==== Configure Database Command

The command line `PROACTIVE_HOME/tools/configure-db` allows configuring the ProActive server to use an external database.

ProActive uses by default an embedded database called link:https://hsqldb.org/[HSQLDB].
In order to use a different database such as PostgreSQL, MySQL or Oracle, the `configure-db` command can be used to modify the ProActive server configuration files
and allow interaction with this external database.

NOTE: `configure-db` configures the connection from ProActive components to the database and not the database itself.

WARNING: Each ProActive component (*Resource Manager*, *Scheduler*, *Catalog*, *Service Automation* and *Notification Service*) that uses an external database needs to be configured separately. Accordingly, the `configure-db` command should be run multiple times.

The command options with several usage examples are described below:

----
$> ./configure-db -h
usage: configure-db [-c <COMPONENT>] [-d] [-h] [-H <HOSTNAME>] [-p <PASSWORD>] [-P <PORT>] [-s <SCHEMA-NAME>] [-U <URL>] [-u
       <USERNAME>] [-v <DATABASE-VENDOR>] [-z <TIMEZONE>]

Change ProActive server database configuration.
ProActive can either use the integrated HSQLDB database or an external database (PostgreSQL MySQL or Oracle).
ProActive requires the following components to be mapped to different database schemas:
Resource Manager, Scheduler, Catalog, Service Automation and Notification Service.

 -c,--component <COMPONENT>               Target component to configure. Component name can be "rm", "scheduler", "catalog",
                                          "service-automation", "notification" or "all" (to configure all components with a common
                                          configuration).
 -d,--debug                               Debug mode (prints modified files and properties)
 -h,--help                                Display this help
 -H,--hostname <HOSTNAME>                 Database hostname used (e.g. localhost, myserver) or IP address (e.g. 127.0.0.1,
                                          192.168.12.1). This option cannot be used in conjunction with --url.
 -p,--password <PASSWORD>                 Database user password. Password can be provided already encrypted using the 'encypt' command
                                          or in clear text. The password will always be encrypted when stored in configuration files.
 -P,--port <PORT>                         Database port used (e.g. 9001, 5432). If this option is not provided, the database vendor
                                          default port will be used. This option cannot be used in conjunction with --url.
 -s,--schema-name <SCHEMA-NAME>           Override the default database schema name for the selected component. Defaults names are {
                                          Resource Manager : "rm", Scheduler : "scheduler", Catalog : "catalog", Service Automation :
                                          "pca", Notification Service : "notification" }. This option cannot be used when
                                          --component=all or when --database-vendor=hsqldb
 -U,--url <URL>                           Sets the JDBC url that will be used to access the target component database. If not used, the
                                          url will be inferred from --hostname, --port and optionally --schema-name options. This
                                          option cannot be used when --component=all. Note: this parameter is the only way to use the
                                          TNS URL Format with Oracle database.
 -u,--username <USERNAME>                 Database user name
 -v,--database-vendor <DATABASE-VENDOR>   Target database vendor to configure ("hsqldb", "postgresql", "mysql" or "oracle")
 -z,--timezone <TIMEZONE>                 Specific to MySQL, sets the timezone used by the MySQL server. The default is "UTC". This
                                          option cannot be used in conjunction with --url.

Examples:
# Configure HSQLDB for all components with single user and password, default hostname and port. Debug output to see all modifications.
configure-db -c all -v hsqldb -u user -p pwd -d
# Configure PostgreSQL for the Resource Manager component with a specific hostname and port
configure-db -c rm -v postgresql -H myserver -P 5434 -u user -p pwd
# Configure PostgreSQL for the Resource Manager component with a specific hostname, port and schema name
configure-db -c rm -v postgresql -H myserver -P 5434 -s RESOURCE_MANAGER -u user -p pwd
# Configure Oracle for the Resource Manager component with a TNS URL format
configure-db -c rm -v oracle -U
"jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=myserver)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=rm))" -u user -p pwd
----

See <<_database_configuration>> for a complete explanation of different database vendor configurations.

[_configure_ldap_command]
==== Configure LDAP / Active Directory Command

The command line `PROACTIVE_HOME/tools/configure-ldap` allows configuring the ProActive server to use LDAP or Active Directory authentication.

`configure-ldap` uses the following process in order to configure authentication:

1. It *modifies* the various configuration files using the parameters received (LDAP server url, bind login, user/group subtree, etc).
2. It *checks* the connection to the LDAP server and *installs* inside the ProActive Java Runtime Environment *SSL certificates* in case of _LDAPS_ usage.
3. It *tests* authentication with a provided _test user_'s credentials to validate that authentication works properly.
4. If it succeeds, the configuration is *persisted*. Otherwise, the configuration is *reverted*.

The command options with several usage examples are described below:

----
$> ./configure-ldap -h
usage: configure-ldap [-a] [-d] [--group.attr <GROUP.ATTR>] [--group.filter <GROUP.FILTER>] [--group.subtree <GROUP.SUBTREE>] [-h]
       [-l <LOGIN>] [-o <OUTPUT.FILE>] [-p <PASSWORD>] [-s] [--start.tls.disable.check] [--tenant.attribute <TENANT.ATTRIBUTE>]
       [--test.pwd <TEST.PWD>] [--test.user <TEST.USER>] [--truststore.path <TRUSTSTORE.PATH>] [--truststore.pwd <TRUSTSTORE.PWD>] [-u
       <URL>] [--user.filter <USER.FILTER>] [--user.subtree <USER.SUBTREE>]

Configure ProActive server for LDAP authentication.
ProActive can either use an Active Directory or a classical LDAP server for authentication.

 -a,--auth.disable                          Use this option to disable authentication to the LDAP server (only when the server supports
                                            anonymous connections).
 -d,--debug                                 Debug mode (prints modified files and properties)
    --group.attr <GROUP.ATTR>               The attribute in the group entry that matches the jaas' group name. Default is "cn".
    --group.filter <GROUP.FILTER>           LDAP filter executed when searching for all groups associated with a given user. By
                                            default, the search takes as parameter the user LDAP entry. Option group.filter.use.uid can
                                            be enabled to give as parameter the user identifier instead. The default group filter,
                                            depending on the LDAP server type and group.filter.use.uid option is  :
                                            {LDAP=(&(objectclass=groupOfUniqueNames)(uniqueMember=%s)),
                                            LDAPwithUID=(&(objectclass=posixGroup)(memberUID=%s)),
                                            AD=(&(objectclass=group)(member:1.2.840.113556.1.4.1941:=%s))}
    --group.subtree <GROUP.SUBTREE>         Scope in the LDAP tree where groups can be found. If not set, the value of option
                                            user.subtree will be used.
 -h,--help                                  Display this help
 -l,--login <LOGIN>                         login name used to connect (bind) to the ldap server when executing queries. The login is
                                            usually a LDAP distinguished name (e.g. uid=janedoe,ou=People,dc=activeeon,dc=com).
 -o,--output.file <OUTPUT.FILE>             Ldap configuration file that will be modified. It will be created by copying the default
                                            configuration file if it does not exist.
 -p,--password <PASSWORD>                   Password associated with the login name. Password can be provided already encrypted using
                                            the 'encypt' command or in clear text.The password will always be encrypted when stored in
                                            the ldap configuration file.
 -s,--start.tls                             Enable StartTLS mode.
    --start.tls.disable.check               If StartTLS mode is enable, disable certificate verification check.
    --tenant.attribute <TENANT.ATTRIBUTE>   An LDAP attribute such as "department" or "project" can be used to group categories of
                                            users in an abstract organization called "Tenant".
    --test.pwd <TEST.PWD>                   (Required) Password of the user that will be searched using configured filters. This
                                            parameter is required to validate the configuration and guarantee that the connection and
                                            filters are working properly.
    --test.user <TEST.USER>                 (Required) Identifier of the user that will be searched using configured filters. This
                                            parameter is required to validate the configuration and guarantee that the connection and
                                            filters are working properly.
    --truststore.path <TRUSTSTORE.PATH>     Path to the truststore that will be used to store LDAPS certificates. If the file does not
                                            exist it will be created. The default value is config/authentication/truststore
    --truststore.pwd <TRUSTSTORE.PWD>       Password used to encrypt the truststore that will be created if the LDAP server is using
                                            LDAPS protocol. If not provided the default password will be "activeeon".
 -u,--url <URL>                             (required) Url(s) of the LDAP server(s). Multiple servers can be configured using a
                                            comma-separated list.
    --user.filter <USER.FILTER>             LDAP filter executed when searching for a LDAP user entry which corresponds to a given user
                                            identifier. The default user filter, depending on the LDAP server type is :
                                            {LDAP=(&(objectclass=inetOrgPerson)(uid=%s)), AD=(&(objectclass=user)(sAMAccountName=%s))}
    --user.subtree <USER.SUBTREE>           (required) Scope in the LDAP tree where users can be found.

Examples:
# Configure Ldap authentication for a ldaps server and default filters. Debug output to see all modifications.
configure-ldap --url ldaps://ldap-server1.activeeon.com --user.subtree "ou=users,dc=activeeon,dc=org" --group.subtree
"ou=groups,dc=activeeon,dc=org" --login "cn=admin,dc=activeeon,dc=org" --password "my_bind_pwd" --test.user user1 --test.pwd
"my_test_pwd" --debug
----

See <<_ldap>> for a full explanation of LDAP authentication parameters.

==== Copy Role Command

The command line `PROACTIVE_HOME/tools/copy-role` can be used to duplicate a role defined in the `PROACTIVE_HOME/config/security.java.policy-server` file.

As described in <<_user_permissions>>, the file `security.java.policy-server` is used to store the definitions of users groups and their rights within the ProActive server.

The `copy-role` command is used when a new group of users needs to be created with the same rights as an existing group.
It can also be used after configuring <<_ldap>>, to map existing _ldap groups_ to one of the default roles defined in `security.java.policy-server`.

For example, in order to import the ldap group `it-admin` as an administrator of the ProActive server:

----
copy-role --source server-admins --destination it-admin
----

See <<_default_groupsroles>> for a description of all roles defined by default in the ProActive server.

WARNING: Any modification to the `security.java.policy-server` file is taken into account only after the ProActive server is restarted.

The command options with several usage examples are described below:

----
$> ./copy-role -h
usage: copy-role [-d] [-D <DESTINATION>] [-h] [-S <SOURCE>] [-y]

Create a new JaaS role by copying an existing one.
JaaS roles are used to define authorization inside the ProActive server.

 -d,--debug                       Debug mode (prints modified files and properties)
 -D,--destination <DESTINATION>   (required) Name of the destination role (group).
 -h,--help                        Display this help
 -S,--source <SOURCE>             (required) Name of the role (group) that will be copied.
 -y,--overwrite                   Overwrite if the destination role already exists.

Examples:
# Copy existing 'server-admins' role into new role 'my-admin-group'. Debug output to see all modifications.
copy-role --source server-admins --destination my-admin-group --debug
----

== User Authentication

In order to log into ProActive server, every user must have a registered account.

A user *login information* is associated with the following attributes:

* a *login* name: when the user authenticate himself into the ProActive server, he/she will use his registered login.
* a *domain*: when the server runs on a _Windows operating system_ or when the authentication method is configured as <<_ldap_with_multiple_domains>>, a domain name may be associated with the account.
* a *password*.
* a list of *groups*. A user may belong to one or more groups. Each group has its own set of authorizations as described in chapter <<_user_permissions>>.
* a *tenant*: a tenant name may be associated with the user account. Tenants represent the isolation of a set of users within the ProActive server. Tenants must first be configured either using <<_tenants_file,Tenant File Configuration>>,
<<_configure_tenant_attribute,Tenant LDAP attribute configuration>> or <<_ldap_with_multiple_domains>>.
When <<_ldap_with_multiple_domains>> is set, the tenant name is equal to the domain name.

User accounts are defined with one of the following *authentication methods*:

* <<_file,*File-based*>>
* <<_ldap,*LDAP*>>
* <<_ldap_with_multiple_domains,*LDAP with Multiple Domains*>>
* <<_pam,*PAM*>>
* <<_keycloak,*Keycloak*>>

=== Configure Domains / Tenants

==== Domains

Within ProActive, a *domain* is part of a user login information and can either represent:

 * A link:https://en.wikipedia.org/wiki/Windows_domain[Windows domain].
 * The name of a LDAP server in case of <<_ldap_with_multiple_domains>>.
 * Both when Windows domain names and LDAP/Active Directory server names match.

The domain is used either:

 * to select which _LDAP/Active Directory server_ will be queried in case of <<_ldap_with_multiple_domains>>.
 * to select which _Windows domain_ will be used when the user executes <<../user/ProActiveUserGuide.adoc#_run_as_me,RunAsMe>> workflow tasks inside a <<_glossary_proactive_node,ProActive Node>> running on a Windows Operating System.

Before being able to use *domains* in login information, the list of allowed domains must be configured on the ProActive server.

IMPORTANT: When configuring allowed domains, it is important to include an empty domain name. This is because ProActive uses internal accounts that do not belong to any domain.
Failing to do so would prevent the ProActive server from starting. An empty domain is included by using the syntax `",domain1,domain2"`.

There are two properties to configure:

 * in `PROACTIVE_HOME/config/rm/settings.ini`
----
pa.rm.allowed.domains=,domain1,domain2
----
* in `PROACTIVE_HOME/config/scheduler/settings.ini`
----
pa.scheduler.allowed.domains=,domain1,domain2
----

=> Allows using either domain1, domain2 or an empty domain name.

NOTE: When the ProActive server runs on a Windows operating system, allowed domains properties default to `",SERVER_USER_DOMAIN"`, where _SERVER_USER_DOMAIN_ corresponds to the Windows domain name of the user that starts the ProActive server.

After _allowed domains_ have been configured, the login page of the various portals will display a drop-down list which allows to select the login domain.

Example:

image::LoginSchedDomain.png[Scheduler Portal login using Domains,237,220]

==== Tenants

*Tenants* are organization units that regroup class of users in an isolated environment within the ProActive server.

For example, _user1_ belonging to _tenant1_ will not be able to see *jobs* submitted by _user2_ belonging to _tenant2_.
It is also possible to restrict access to *ProActive Nodes* based on tenants (a *Node Source* can restrict its access to a specific tenant name).

Tenants can thus improve the management and governance of an organization's hybrid compute infrastructure.

User *tenants* are different from user *groups*:

 * A user can belong to *several groups* but can only belong to a *single tenant*.
 * User groups does not allow the same level of *isolation* as tenants allow (for example, it is not possible to prevent users from seeing jobs that belong to a different group).
 * User groups allow defining *roles* (administrators, standard users, etc.) inside the ProActive server. A user tenant does not correspond to a role.

Tenant user membership can be configured using one of the following methods:

 * a <<_tenants_file,file-based tenant configuration>>.
This file is a group -> tenant association which allows to define a collection of groups as tenant.
 * a <<_configure_tenant_attribute,LDAP attribute configuration>>.
This method can be used when a single LDAP/Active Directory server is configured as authentication method. In that case, a scalar LDAP attribute defined for all users can be used as tenant name (For example _Organization Name_, _Division_, etc.).
 * a <<_ldap_with_multiple_domains,Multi-Domain LDAP configuration>>. When multi-domain LDAP configuration is used as authentication method, the tenant name will be automatically set to the domain name used during authentication. For example, if _user1_ logs in from _domain1_, _user1_ will also belong to the _domain1_ tenant.
This is the only case where domain name and tenant name will be equal.

A *tenant* can either be a collection of groups (if defined using a file), an organization unit (if defined using an _LDAP_ attribute) or a _LDAP domain_ (if the authentication method is configured to LDAP with multiple domains).

After tenants definition is configured, it is necessary to configure in the ProActive Server tenant isolation.

===== Tenants Job Isolation

*Tenants Job isolation* is the ability to hide submitted <<_glossary_job,jobs>> across tenants.

For example, when _user1_ belonging to _tenant1_ submits a job to the ProActive Scheduler, then _user2_ belonging to _tenant2_ will not see this job activity on the <<_glossary_scheduler_web_interface,Scheduler>> or <<_glossary_workflow_automation_portal,Workflow Execution>> portals.

In order to configure Tenants Job isolation, the setting `pa.scheduler.core.tenant.filter` must be set to `true` in `PROACTIVE_HOME/config/scheduler/settings.ini`.

When this configuration is enabled, standard users will only see jobs according to the user's tenant. Admin users will still be able to see jobs coming from all tenants.

NOTE: It is not mandatory to configure tenant isolation. Tenants can simply be used as additional information associated with each user submitted jobs.

.Scheduler Portal for "Users" tenant
image::TenantSPUsers.png[Scheduler Portal for "users" tenant]

.Scheduler Portal for "Experts" tenant
image::TenantSPExperts.png[Scheduler Portal for "experts" tenant]

.Scheduler Portal for Admin users
image::TenantSPAdmins.png[Admins Tenant View]

===== Tenants Node Source Isolation

Apart from job isolation, it is also possible to restrict <<_glossary_node_source,Node Sources>> across tenants.

Restricting a Node Source to a specific tenant can be done by using the `nodeUsers` parameter which is common to all <<_glossary_node_source_policy,Node Source Policies>>.

See <<_policy_common_parameters,Policy Common Parameters>> for more information.

=== Select authentication method

By default, the ProActive server is configured to use *file-based* authentication and has some default accounts ('demo', 'admin', 'user') that
can be used to first log into the server.

The default accounts passwords (demo:demo, admin:admin and user:pwd) should be modified using the <<_command_line,proactive-users>> command.

If you would like to change the authentication mechanism to use your *LDAP* server or use Linux PAM, you need to modify two configuration files:

* *Resource Manager* configuration (`PROACTIVE_HOME/config/rm/settings.ini`)
+
[source]
----
#Property that defines the method that has to be used for logging users to the Resource Manager
#It can be one of the following values:
#  - "RMFileLoginMethod" to use file login and group management
#  - "RMLDAPLoginMethod" to use LDAP login management
#  - "RMMultiLDAPLoginMethod" to use LDAP with multiple domains
#  - "RMPAMLoginMethod" to use PAM login management
#  - "RMKeycloakLoginMethod" to use Keycloak login management
pa.rm.authentication.loginMethod=RMFileLoginMethod
----

* *Scheduler* configuration (`PROACTIVE_HOME/config/scheduler/settings.ini`)
+
[source]
----
#Property that define the method that have to be used for logging users to the Scheduler
#It can be one of the following values:
#  - "SchedulerFileLoginMethod" to use file login and group management
#  - "SchedulerLDAPLoginMethod" to use LDAP login management
#  - "SchedulerMultiLDAPLoginMethod" to use LDAP with multiple domains
#  - "SchedulerPAMLoginMethod" to use PAM login management
#  - "SchedulerKeycloakLoginMethod" to use Keycloak login management
pa.scheduler.core.authentication.loginMethod=SchedulerFileLoginMethod
----

[[_file]]
=== File authentication

By default, the ProActive server stores users accounts, passwords, group memberships (user or admins), and tenant memberships (group agglomerate or organization), in two files.

The default file path to store users, group and tenant memberships can be modified by changing the following settings:

.PROACTIVE_HOME/config/rm/settings.ini
----
pa.rm.defaultloginfilename=config/authentication/login.cfg
pa.rm.defaultgroupfilename=config/authentication/group.cfg
pa.rm.defaulttenantfilename=config/authentication/tenant.cfg
----

.PROACTIVE_HOME/config/scheduler/settings.ini
----
pa.scheduler.core.defaultloginfilename=config/authentication/login.cfg
pa.scheduler.core.defaultgroupfilename=config/authentication/group.cfg
pa.scheduler.core.defaulttenantfilename=config/authentication/tenant.cfg
----

==== Users

Users and passwords accounts are stored in `PROACTIVE_HOME/config/authentication/login.cfg`.
Each line has to follow the format *user:encrypted password*.

==== Groups

Users group membership is stored in `PROACTIVE_HOME/config/authentication/group.cfg`. For each user registered in login.cfg,
a group membership has to be defined in this file. Each line has to look like *user:group*. Group has to be
`user` to have user rights, `scheduleradmins` to have administrator rights on the scheduler, `rmcoreadmins` to have administrator rights on the resource manager, etc. Below is an example `group.cfg` file:

.group.cfg
----
admin:scheduleradmins
admin:rmcoreadmins
demo:scheduleradmins
demo:rmcoreadmins
citizen-ds:citizen-ds
expert-ds:expert-ds
guest:guests
nsadmin:nsadmins
provider:providers
rm:admin
rm:nsadmins
scheduler:user
scheduler:scheduleradmins
user:user
watcher:watchers
----

ProActive contains a set of predefined groups such as `user` and `providers`. Group rights are defined in `PROACTIVE_HOME/config/security.java.policy-server` as described in chapter <<User Permissions>>.

[[_tenants_file]]
==== Tenants

<<_tenants,Tenants>> are organization units that regroup class of users in an isolated environment within the ProActive server.

When defined via a file, tenant organization is stored in `PROACTIVE_HOME/config/authentication/tenant.cfg`.
Each line of this file contains a couple _(group_name, tenant_name)_, using the syntax `group_name:tenant_name`.
To define tenants, an administrator of the platform need to associate one or more groups to each tenant.

IMPORTANT: Each user can belong to only one tenant. Thus, if a user is a member of multiple groups belonging to different tenants, only the first tenant found will be associated.

By default, the tenant configuration file is empty, below is an example tenant configuration:

.tenant.cfg
----
scheduleradmins:admins
rmcoreadmins:admins
nsadmin:admins
citizen-ds:citizens
expert-ds:experts
user:users
----

==== Command Line

In order to create new users, delete an existing user, or modify the groups or password for an existing user, a command-line tool is available.

This command is available inside the *tools* folder: `PROACTIVE_HOME/tools/proactive-users`.
You can check the command syntax using the -h option.

----
$ proactive-users -h
usage: proactive-users [-C | -D | -U]  [-g <GROUPS>] [-gf <GROUPFILE>] [-h] [-kf <KEYFILE>] [-l <LOGIN>] [-lf <LOGINFILE>] [-p
       <PASSWORD>] [-sgf <SOURCEGROUPFILE>] [-slf <SOURCELOGINFILE>]
----

Here are some example usages:

* Creating users
+
----
$ proactive-users -C -l user1 -p pwd1 -g user
Created user user1 in H:\Install\scheduling\tools\..\config/authentication/login.cfg
Added group user to user user1
----
The user with login "user1", password "pwd1" and group "user" was created

* Updating users
+
----
$ proactive-users -U -l user1 -p pwd2 -g nsadmins,admin
Changed password for user user1 in H:\Install\scheduling\tools\..\config/authentication/login.cfg
Added group nsadmins to user user1
Added group admin to user user1
----
User "user1" now has password pwd2 and groups nsadmins & admin (group "user" was removed).

* Deleting users
+
----
$ proactive-users -D -l user1
Deleted user user1 in H:\Install\scheduling\tools\..\config/authentication/login.cfg
----
User "user1" does not exist any more.

* Creating multiple users
+
It is also possible to create multiple users at once using a source login file and a source group file.
In that case, the source login file contains, for each user, a line with the format "login:unencrypted_password".
The source group file has the same structure as the `PROACTIVE_HOME/config/authentication/group.cfg` file.
This can be used, for example, to convert login files used by ProActive Scheduler versions prior to 7.19.0.
+
----
$ proactive-users -C -slf source_login.cfg -sgf source_group.cfg
Adding group admin to user admin1
Created user admin1
Adding group user to user user2
Created user user2
Adding group user to user user1
Created user user1
Stored login file in H:\Install\scheduling\tools\..\config/authentication/login.cfg
Stored group file in H:\Install\scheduling\tools\..\config/authentication/group.cfg
----

* Updating multiple users
+
Similarly, it is possible to update existing users with source login or group files. It is possible to update only group membership for existing users, or only passwords, or both.
+
The example below shows how to update only groups for existing users:
+
----
proactive-users -U -sgf source_group_2.cfg
Adding group admin to user user1
Updated user user1
Adding group admin to user user2
Updated user user2
Stored login file in H:\Install\scheduling\tools\..\config/authentication/login.cfg
Stored group file in H:\Install\scheduling\tools\..\config/authentication/group.cfg
----

[[_ldap]]
=== LDAP authentication

The ProActive server is able to connect to an existing *LDAP* server, to check users' logins, passwords and verify users group
memberships.

In order to use the LDAP authentication method, change the default authentication method as described in <<_select_authentication_method,Select authentication method>>.

Additionally, multiple LDAP parameters must be configured in a dedicated LDAP configuration file, such as *path in LDAP tree users*, LDAP *groups* that define
user and admin group membership, *URL* of the LDAP server, LDAP *binding method* used by connection and configuration
of SSL/TLS if you want a secured connection between the ProActive Resource Manager and LDAP.

TIP: The `PROACTIVE_HOME/tools/configure-ldap` command can be used to configure LDAP and test the configuration. See <<_configure_ldap_active_directory_command>>.

We assume that the existing LDAP server is configured in a way that:

* all existing users and groups are located under a _single domain_
* user and group name is defined in _cn_ (Common Name) attribute

[source]
----
# EXAMPLE of user entry
#
# dn: cn=jdoe,dc=example,dc=com
# cn: jdoe
# firstName: John
# lastName: Doe
# objectClass: inetOrgPerson

# EXAMPLE of group entry
#
# dn: cn=mygroup,dc=example,dc=com
# cn: mygroup
# firstName: John
# lastName: Doe
# uniqueMember: cn=djoe,dc=example,dc=com
# objectClass: groupOfUniqueNames
----

The LDAP configuration is defined in `PROACTIVE_HOME/config/authentication/ldap.cfg`. You need to:

==== Set the LDAP server URL

First, you have to define the LDAP URL of your organization. This address corresponds to the property: `pa.ldap.url`.
You have to put a standard LDAP-like URL, for example *ldap://myLdap*.
You can also set a URL with secure access: *ldaps://myLdap:636*.

Finally, it is possible to define additional URLs that will be used as backup if the primary URL is not responding.
----
pa.ldap.url=ldap://myLdapServer1 ldap://myLdapServer2
----

==== Define user and group filters

The authentication module must be able to find a user ldap entry, given its login name (or any other appropriate ldap attribute).
For each user, the module must also be able to find all group ldap entries associated with the user.

The LDAP module uses ldap filters to search for users and groups.

===== User filters

The user filter is defined using the property `pa.ldap.user.filter`.
The search is performed according to a configurable ldap subtree, controlled by the property `pa.ldap.userssubtree`.

`pa.ldap.userssubtree` must be defined according to the base distinguished name of the ldap server.
It can be further refined to restrict the search to a specific subtree where the users are located.

Example:

----
pa.ldap.userssubtree=dc=mycompany,dc=com
----

`pa.ldap.user.filter` must be defined according to the ldap server version.

For an _OpenLDAP_ server (V2 or V3), use:

----
pa.ldap.user.filter=(&(objectclass=inetOrgPerson)(uid=%s))
----

For an _Active Directory_ server, use:

----
pa.ldap.user.filter=(&(objectclass=user)(sAMAccountName=%s))
----

The link:https://linux.die.net/man/1/ldapsearch[ldapsearch] command can be used to test the execution of user filters.

For example:

----
ldapsearch -x -b "dc=mycompany,dc=com" -H ldap://myLdapServer1 -D "cn=admin,dc=mycompany,dc=com" -W '(&(objectclass=user)(sAMAccountName=user1))'
----

Can be used to search that the configured user filter is able to locate the ldap entry for _user1_ on an Active Directory server



===== Group filters

The group filter is defined using the property `pa.ldap.group.filter`.
The search is performed according to a configurable ldap subtree, controlled by the property `pa.ldap.groupssubtree`.

`pa.ldap.groupssubtree` can be left undefined. In that case, the value configured for `pa.ldap.userssubtree` will be used.

As group membership search can be time-consuming for large organizations, it is recommended to set `pa.ldap.groupssubtree` to a subtree that contains groups that will be mapped as ProActive server roles.

Example:

----
pa.ldap.groupssubtree=ou=groups,ou=mydepartment,dc=mycompany,dc=com
----

`pa.ldap.group.filter` must be defined according to the ldap server version.

For an _OpenLDAP_ server V2, use:

----
pa.ldap.group.filter=(&(objectclass=groupOfUniqueNames)(uniqueMember=%s))
----

For an _OpenLDAP_ server V3, use:

----
pa.ldap.group.filter=(&(objectclass=posixGroup)(memberUID=%s))
----

The following setting must also be enabled for an open ldap server V3:

----
pa.ldap.group.search.use.uid=true
----

This tells the authentication module to use the login name instead of distinguished name as parameter of the group filter.

For an _Active Directory_ server, use:

----
pa.ldap.group.filter=(&(objectclass=group)(member:1.2.840.113556.1.4.1941:=%s))
----

Finally, the following property controls which LDAP attribute contains the group name in the group ldap entry (it is normally not necessary to change the default value):

----
pa.ldap.group.name.attr=cn
----

The link:https://linux.die.net/man/1/ldapsearch[ldapsearch] command can be used to test the execution of group filters.

For example:

----
ldapsearch -x -b "dc=mycompany,dc=com" -H ldap://myLdapServer1 -D "cn=admin,dc=mycompany,dc=com" -W '(&(objectclass=group)(member:1.2.840.113556.1.4.1941:=cn=user1,ou=users,dc=mycompany,dc=com))'
----

Can be used to search that the configured group filter is able to find all groups associated with _user1_ (contrary to the user filter, the user distinguished name (dn) is given as parameter)


==== Configure LDAP authentication parameters

By default, the ProActive Scheduler binds to LDAP in anonymous mode. You can change this authentication
method by modifying the property `pa.ldap.authentication.method`. This property can have several values:

* none (default value) - the ProActive Resource Manager performs connection to LDAP in anonymous mode.
* simple - the ProActive Resource Manager performs connection to LDAP with a specified user/password (see below for user password setting).

You can also specify a SASL mechanism for LDAPv3. There are many SASL available mechanisms: cram-md5, digest-md5, kerberos4. Just set this property to *sasl* to let the ProActive server choose a SASL authentication mechanism.

If you specify an authentication method different from 'none' (anonymous connection to LDAP), you must specify a login/password for authentication.

There are two properties to set in LDAP configuration file:

** `pa.ldap.bind.login` - sets user name for authentication.
** `pa.ldap.bind.pwd` - sets password for authentication.

The login used should be the _distinguished name_ (dn) of a user with read access on the LDAP server.
The password can be encrypted using the <<_encrypting_configuration_passwords,encryption command line tool>>.

==== Set SSL/TLS parameters

A secured SSL/TLS layer can be useful if your network is not trusted, and critical information is transmitted between the Resource Manager server and LDAP, such as user passwords.

Encryption of the communication to the ldap server can be performed in two ways:

* by using the _LDAPS_ protocol (<<_ssl_encryption>>).
* by using <<_starttls_encryption>>.

===== SSL encryption

First, set the LDAP URL property `pa.ldap.url` to a URL of type *ldaps://myLdap*. Then, set `pa.ldap.authentication.method` to *none* so as to delegate authentication to SSL.

For using SSL properly, it is required to specify a certificate and authorities (public keys) for the SSL handshake.
Java stores certificates inside a keyStore and public keys inside a trustStore.
In most of the cases, adding authority public keys in a trust store is sufficient.

====== Generate trustStore manually

The following steps show how to create a keystore and trustStore manually using the keytool command (included in the ProActive server installation):

----
PROCTIVE_HOME/jre/bin/keytool -import -alias myAlias -file myCertificate -keystore myKeyStore
----

where:

* `PROCTIVE_HOME` is the path to the ProActive server installation
* `myAlias` is the alias name of your certificate.
* `myCertificate` is your private certificate file.
* `myKeyStore` is the new keyStore file produced in output.

This command asks you to enter a password for your keyStore.

Put the LDAP certificate's public key in a trustStore, with the following keytool command:

----
PROCTIVE_HOME/jre/bin/keytool -import -alias myAlias -file myPublicKey -keystore myTrustStore
----

where:

* `PROCTIVE_HOME` is the path to the ProActive server installation
* `myAlias` is the alias name of your certificate's public key.
* `myPublicKey` is your certificate's public key file.
* `myTrustore` is the new trustStore file produced in output.

This command asks you to enter a password for your trustStore.

Finally, in `config/authentication/ldap.cfg`, set keyStore and trustStore created before to their respective passwords:

* Set `pa.ldap.keystore.path` to the path of your keyStore.
* Set `pa.ldap.keystore.passwd` to the password defined previously for keyStore.
* Set `pa.ldap.truststore.path` to the path of your trustStore.
* Set `pa.ldap.truststore.passwd` to the password defined previously for trustStore.

====== Download the ldap server certificate

Alternatively to the above procedure, the following command can be used to automatically download the ldap server certificates and install it inside the ProActive server Java Runtime Environment default trustStore:

----
PROCTIVE_HOME/jre/bin/java -jar PROCTIVE_HOME/tools/installcert-usn-20140115.jar ldap_server_hostname_or_ip:ldap_port
----

where:

* `PROCTIVE_HOME` is the path to the ProActive server installation.
* `ldap_server_hostname_or_ip` is the hostname or ip address of the ldap server.
* `ldap_port` is the port used to connect to the ldap server.

The command is interactive and will ask the java trustStore password. The default password is `changeit`.


===== StartTLS encryption

StartTLS is an alternative encryption method to SSL. StartTLS works by opening a connection using an unencrypted protocol, then upgrading this connection to an encrypted channel.
The LDAP server must support StartTLS in order to use it.

In order to configure StartTLS inside the ProActive LDAP module, the keystore/trustStore operations described in <<_generate_truststore_manually>> or <<_download_the_ldap_server_certificate>> must be executed.

Additionally, the following properties must be set in the LDAP configuration file:

----
pa.ldap.starttls=true

pa.ldap.connection.pooling=false
----

==== Configure tenant attribute

A LDAP attribute can be used to associate a *tenant name* to an existing user.

<<_tenants,Tenants>> are organization units that regroup class of users in an isolated environment within the ProActive server.

If your LDAP structure contains an user attribute that seems suitable for tenants isolation, set the `pa.ldap.tenant.attr` property to the name of this attribute. Example atributes: `division`, `department`, `company`, etc.

See <<_tenants>> for more information.

==== Use fall back to file authentication

You can use simultaneously file-based authentication and LDAP-based authentication.
In this scenario, ProActive Scheduler checks at first user/password and group memberships in
the login and group files, as performed in the <<_file,File login method>>.
If the user or group attribution is not found in the login/group files, login or groups will be searched via LDAP.

There are two configuration options to control this mechanism (present in the `ldap.cfg` configuration file):

* `pa.ldap.authentication.fallback`: if set to `true`, the user login/password will be searched first in the login file, and then on the LDAP server. If set to `false`, only the LDAP server will be queried.
* `pa.ldap.group.membership.fallback`: if set to `true`, group memberships will be read from the LDAP server if the user exists, and then from the group file. In that sense, a user will be affected to groups defined in both file and LDAP server. If set to `false`, only the LDAP server will be queried.
* `pa.ldap.tenant.membership.fallback`: if set to `true`, tenant memberships will be read from the tenant file if:
** The user does not exist in the LDAP server.
** The pa.ldap.tenant.attr configuration is not defined.
** The pa.ldap.tenant.attr configuration is defined, but the configured attribute is empty for the user.

==== Specific example of LDAP configuration

Below is an example of LDAP configuration which summarizes the LDAP properties frequently configured in the file `config/authentication/ldap.cfg`.
[source,properties]
----
# URL of a ldap used for authentication
pa.ldap.url=ldaps://ldap-server.activeeon.com

# path in the LDAP tree users
pa.ldap.userssubtree=ou=users,dc=activeeon,dc=org

# path in the LDAP tree groups
# if empty, then the value for pa.ldap.userssubtree is used
pa.ldap.groupssubtree=ou=groups,dc=activeeon,dc=org

# the filter that allows to find the user dn given its scheduler login
# %s is replaced by the user-submitted login
pa.ldap.user.filter=(&(objectclass=inetOrgPerson)(uid=%s))

# retrieves the groups the user dn belongs to
# the '%s' parameter is the user dn previously retrieved
pa.ldap.group.filter=(&(objectclass=groupOfUniqueNames)(uniqueMember=%s))

# the attribute in the group entry that matches the jaas' group name
pa.ldap.group.name.attr=cn

# authentication method used to connect to LDAP : none (for anonymous connection), simple or a SASL method
pa.ldap.authentication.method=simple

# login name used to perform ldap's binding
# ex uid=narf,ou=People,dc=activeeon,dc=com
pa.ldap.bind.login=cn=admin,dc=activeeon,dc=org

# password used to perform ldap binding
pa.ldap.bind.pwd=change-pwd

# check user password and group membership in login and group files
pa.ldap.authentication.fallback=true

# check user group membership in group file
pa.ldap.group.membership.fallback=true

# check user tenant membership in tenant file
pa.ldap.tenant.membership.fallback=true
----

[[_ldap_with_multiple_domains]]
=== LDAP authentication with Multiple Domains

The ProActive server can be configured to authenticate users according to *multiple LDAP domains*.

It means that when a user logs in, he/she will need to select which LDAP <<_domains,domain>> will be used for the authentication.

image::LoginSchedDomain.png[Scheduler Portal login using Domains,237,220][align="center"]

NOTE: In that configuration, the user <<_domains, domain name>> will be automatically recorded as <<_tenants, tenant>> name.

Following is a step-by-step procedure to configure it.

1. Copy the file `PROACTIVE_HOME\config\authentication\ldap.cfg` into multiple files, one for each domain.
+
For example:
+
----
config\authentication\ldap-domain1.cfg
config\authentication\ldap-domain2.cfg
----
+
2. Using the procedure described in chapter <<_ldap>>, configure each ldap file according to the domain used (each domain will have its own ldap server, bind login name, user subtree, etc.).
+
3. Follow the steps described in chapter <<_select_authentication_method>> to configure `RMMultiLDAPLoginMethod` and `SchedulerMultiLDAPLoginMethod`.
+
4. In `config\rm\settings.ini`, uncomment the property `pa.rm.multi.ldap.config` and define it according to the ldap configuration files you created and their corresponding domain names:
+
----
pa.rm.multi.ldap.config=domain1:config/authentication/ldap-domain1.cfg,domain2:config/authentication/ldap-domain2.cfg
----
+
Uncomment also the property `pa.rm.allowed.domains` and configure with the list of domains that will be available.
+
----
pa.rm.allowed.domains=,domain1,domains2
----
+
This expression must start with a comma, it allows login with an empty domain, necessary for internal ProActive users.
+
5. In `config\scheduler\settings.ini`, uncomment the property `pa.scheduler.multi.ldap.config` and define it according to the ldap configuration files you created and their corresponding domain names:
+
----
pa.scheduler.multi.ldap.config=domain1:config/authentication/ldap-domain1.cfg,domain2:config/authentication/ldap-domain2.cfg
----
+
Uncomment also the property `pa.scheduler.allowed.domains` and configure with the list of domains that will be available.
+
----
pa.scheduler.allowed.domains=,domain1,domains2
----
+
This expression must start with a comma, it allows login with an empty domain, necessary for internal ProActive users.

Once all the steps above are completed, the ProActive server can be (re)started in order to make the multi-domain ldap configuration effective.

[IMPORTANT]
====
In multi-domain mode, when a user logs in to any portal of the ProActive server, the user must specify the domain used.
A drop-down list of configured domains will appear when login into ProActive portals.

For other methods of login (such as command line or REST api), the domain can be specified using the notation `domain\user`.

It is essential to allow the ProActive server select the adequate ldap domain configuration where the user is registered.
====

[[_pam]]
=== PAM authentication

The ProActive Scheduler & Resource Manager are able to interact with *Linux PAM (Pluggable Authentication Modules)* to check users login/password.
It is not currently possible to retrieve linux system group memberships for users. Thus, groups must be managed using the `PROACTIVE_HOME/config/authentication/group.cfg` file.

In order to enable PAM authentication, follow the steps below:

1. Configure the PAM login methods in the Scheduler and RM settings:
+
.PROACTIVE_HOME/config/rm/settings.ini
----
pa.rm.authentication.loginMethod=RMPAMLoginMethod
----
+
.PROACTIVE_HOME/config/scheduler/settings.ini
----
pa.scheduler.core.authentication.loginMethod=SchedulerPAMLoginMethod
----
+

2. Copy the file `PROACTIVE_HOME/config/authentication/proactive-jpam` to `/etc/pam.d` (you must be root to perform this operation).
+
You can modify this default PAM configuration file if you need specific PAM policies.


3. Add the user which will start the scheduler process `PROACTIVE_HOME/bin/proactive-server` to the *shadow* group
+
[source]
----
sudo usermod -aG shadow your_user
----
+
After this command, the user, called "your_user" here in this example, will be added to group *shadow*. You may need to logoff/logon before this modification can be effective.

4. Associate, for each Linux system users which will connect to the Scheduler, a ProActive group.
+
Following the procedure described in chapter <<_file>>, groups must be associated to existing PAM users in the ProActive group file `PROACTIVE_HOME/config/authentication/group.cfg`.
+
In case of PAM users, the login file should not be modified as password authentication will be performed at the Linux system level. Accordingly, you should not use the *proactive-users* command to associate groups to PAM users.

NOTE: Similarly to the LDAP authentication, there is a fallback mechanism for login authentication, with the difference that it is always activated.
A user defined in the ProActive login file always has precedence over the same user defined on the Linux system.

[[_keycloak]]
=== Keycloak-based access control
https://www.keycloak.org/[Keycloak] is an access control solution that allows single sign-on with identity and access management aimed at modern applications and services.
ProActive is able to connect to an existing Keycloak server, and perform user authentication and authorization against it.

ProActive uses the OpenID Connect (OIDC) protocol to communicate with Keycloak and acquire an access token for a given user.
It further parses the obtained access token and extracts from it the user information, including the user role.
Based on the user role, ProActive determines the set of privileges and authorizations to be granted to the user.

In order to perform ProActive access control against a Keycloak server, two steps must be accomplished: (i) deploying and configuring a Keycloak server and (ii) configuring ProActive to connect to the Keycloak server and perform access control against it.

==== Configuring Keycloak server

WARNING: The configuration steps mentioned below provide an example of the basic steps needed to configure ProActive access control against Keycloak.
For advanced configurations, please refer to the https://www.keycloak.org/documentation[Keycloak documentation].

===== Pre-requisites
1. A Keycloak server must be up and in operational mode so ProActive can interact with it.
2. The Keycloak server's administrator should select a security realm (or create a new one if needed).

===== Configuring a new Keycloak client for ProActive
The Keycloak server's administrator has to configure a new client for ProActive in the chosen realm.
A client is an external software application for which the access control is managed via Keycloak.
In the section *General Settings* of the configuration, the parameters following must be properly set (as shown in the screenshot below):

* The property `Client type` specifies the security protocol of the client application. It must be set to `OpenID Connect`.
* The property `Client ID` specifies the ID of the client application (e.g., ProActive).
* The property `Name` specifies the displayed name of the client  (e.g., ProActive Workflows and Scheduling).
* The property `Description` specifies the description of the client (e.g., ProActive workflows engine).

.General settings of Keycloak client
image::KeycloakClientGeneralConfig.png[Keycloak Client General Configuration,600,400]

===== Configuring the authentication mechanism of the client
In the section *Capability config* of the client configuration, the following parameters must be properly set (as shown in the screenshot below):

* The property `Client authentication` must be enabled (i.e., set to `On`).
* The checkbox `Standard flow` must be ticked.
* The checkbox `Direct access grants` must be ticked.

.Authentication settings of the Keycloak client
image::KeycloakClientAuthConfig.png[Keycloak Client Authentication Configuration,600,400]

[[_keycloak_client_roles]]
===== Configuring the client roles
In the *Roles* tab of the client configuration, click on the button *Create role* and add the following parameters for each user role needed in ProActive.

* The property `Role name` specifies name of the role. It will be used later on when <<_keycloak_proactive_privileges, configuring ProActive privileges>>.
* The property `Composite` should be set to `false`.
* The property `Description` specifies description of the role.

.Roles to be associated to the Keycloak client
image::KeycloakClientRolesConfig.png[Keycloak Client Roles Configuration,600,400]

===== Binding client roles to user groups
* Once the client configuration is fulfilled, go to the section *Groups* which lists the groups of users available in Keycloak.
* For each group of users that require access to ProActive, select the tab *Role mapping* and click on the button *Assign role*.
* In the list of roles that will be displayed, select the set of roles to be granted for the considered group of users.

TIP: In the search menu, select *Filter by client* and enter the client name (e.g., ProActive) in order to easily find the roles defined above.

.Binding client roles to user groups in Keycloak
image::KeycloakGroupToRolesBinding.png[Keycloak Role-to-Group Binding,600,400]

==== Configuring ProActive server

In order to enable ProActive access control against a Keycloak server, the following configuration steps must be accomplished at ProActive side:

===== Configuring the authentication method for ProActive scheduler and Resource Manager
The Keycloak authentication method must be selected for the scheduler and resource manager, as described in <<_select_authentication_method,Select authentication method>>.

===== Configuring ProActive properties for Keycloak
The configuration file *config/authentication/keycloak.cfg* must be edited in order the set the proper values of the following properties:

* The property `keycloak.url` should be set to the URL of the Keycloak server (already configured for ProActive).
* The property `keycloak.https.allow_any_certificate` must be set to `true` when the keycloak server operates in TLS mode and uses a non-trusted (e.g., self-signed) TLS/SSL certificate. Otherwise, this property is set to `false`.
* The property `keycloak.realm` must take the name of the Keycloak realm in which the ProActive client application is configured.
* The property `keycloak.proactive.client_id` must take as value the name of the ProActive client application in Keycloak.
* The property `keycloak.proactive.client_secret` must take as value the secret credential of the ProActive client application in Keycloak.
* The property `keycloak.proactive.scope` must take as value the name of a scope configured in Keycloak. A scope defines the set of user information/parameters to be included in a Keycloak access token. If the value is empty, the default scope in Keycloak is used.
* The property `keycloak.public.key` must have as value the content of public encryption key of Keycloak (e.g., public RSA key). This key is used to decrypt the access token returned by Keycloak.
* The property `keycloak.principal_attribute` is the parameter in Keycloak access token based on which the user name is determined (e.g., `name` or `given_name`).
* The property `keycloak.use_resource_role_mappings` is set to `true` when the user roles are determined based on the information block `resource_access` in Keycloak access token. When it is set to `false`, the user roles are determined based on the information block `realm_access` in Keycloak access token.
* Using the fallback to file authentication: ProActive can be configured to use simultaneously file-based authentication and Keycloak-based access control. In this scenario, ProActive Scheduler checks at first user credentials (login/password) and group memberships in
the login and group files, as performed in the <<_file,File login method>>. If the user or group attribution is not found in the login/group files, the user credentials and roles will be checked via Keycloak. There are three configuration options to control the fallback mechanism:
** `pa.keycloak.authentication.fallback`: If set to `true` (which is the default value), the user credentials will be checked first in the login file, and then in the Keycloak server. If set to `false`, only the Keycloak server will be queried.
** `pa.keycloak.group.membership.fallback`: If set to `true` (which is the default value), group memberships will be read from the Keycloak server if the user exists, and then from the group file. In that sense, a user will be affected to groups defined in both file and Keycloak server. If set to `false`, only the Keycloak server will be queried.
** `pa.keycloak.tenant.membership.fallback`: Must be always set to `false` for the current version of ProActive.

[[_keycloak_proactive_privileges]]
===== Configuring the granted privileges for user roles
ProActive has a set of predefined roles (also called groups), each one of them grants certain access rights and privileges to the users having the considered role.
If the roles configured in Keycloak (for the ProActive client) do not already exist, the ProActive server's administrator has to create these roles with the appropriate privileges, as explained in the section <<_creating_new_groups,creating new groups>>.

WARNING: The name of the roles to be created in ProActive must be the same as those configured in <<_keycloak_client_roles, Keycloak client for ProActive>>.
For example, if the role `proactive-workflow-engineer` is configured in Keycloak, the role/group to be created in proActive must be as follows:

----
grant principal org.ow2.proactive.authentication.principals.GroupNamePrincipal "proactive-workflow-engineer" {
  // list of granted privileges
}
----

===== Specific example of Keycloak configuration

Below is an example of Keycloak configuration which provides sample values for the Keycloak properties to be configured in the file `config/authentication/keycloak.cfg`.
[source,properties]
----
# URL of a Keycloak server configured for ProActive authentication
keycloak.url=https://keycloak.activeeon.com:9443

# When Keycloak URL uses the HTTPS protocol, this parameter indicates which kind of TLS certificate to consider for the Keycloak server.
# True means that any certificate can be used including the self-signed ones.
# False means that only valid certificates generated by trusted Certification Authorities are allowed.
keycloak.https.allow_any_certificate=true

# Keycloak realm in which ProActive client is configured
keycloak.realm=activeeon

# Keycloak client credentials (note that ProActive is the client application of Keycloak)
keycloak.proactive.client_id=ProActive
keycloak.proactive.client_secret=w9jvqPPTMvxxxyyyrnessKxt7nCs75NpDZ

# Scope of the information to be returned by Keycloak for the ProActive client.
# An empty value considers the scopes defined for ProActive client in Keycloak.
keycloak.proactive.scope=

# Keycloak RSA public key used to decrypt the access tokens
keycloak.public.key=MIIBIjANBgkutvbS5w0BAQEFAAOCAQ8AMGGBCgKCAQEAsmVExiiZgtnrN7DrPRu6Cx/H1Fx4wOCy1b9sT4w7YjSLfX0d5GN51bDiz5olcvgBNvNIXGXm3Vax8OoOJdIYaq2EiVa0p0ATpzYFIZkKFOkiuriDJd3TpxuJyWIoEg1zUZHNPw0zvFftZ24HByZr51qm/QMBQNvj2tMcPvBgm5tUbIe1bzR3VURXA41+p0Cs4I1kjhWV5B95+SLbxcj/+TMkUt5BVDxPy0nJixM+4zZ/R48OCTpPUnLrGJw+/z5vgbqwwH+ekpI+L08v0wUYmMkqfnykZxOyhtoCUD66pojDoPO60JPDj5qtbiydHwV7DsULvbc406wEe0pN8hKO5QIDAQAB

# The attribute in Keycloak access token to be used to extract the user/principal name
keycloak.principal_attribute=name

# Use the resource_access block in Keycloak access token to extract the user/principal roles
keycloak.use_resource_role_mappings=true

# Authentication fallback Properties
# check user password and group membership in login and group files
# (as performed in FileLogin method), if user is not found in Keycloak.
#
# It use pa.[scheduler.core|RM].defaultloginfilename to authenticate user and
# pa.[scheduler.core|RM].defaultgroupfilename files to check user membership.
# In any case, no fallback is performed if user is found in Keycloak but has entered an incorrect password).
# true or false.
pa.keycloak.authentication.fallback=true

# check user group membership in group file (as performed in FileLogin method)
# if user is not found in its corresponding requested Keycloak group.
# true or false.
pa.keycloak.group.membership.fallback=true

# check user tenant membership in tenant file (as performed in FileLogin method)
# if tenant attribute is not defined or is empty for the corresponding requested Keycloak user.
# true or false.
pa.keycloak.tenant.membership.fallback=false
----
[[_user_permissions]]
== User Permissions

All users authenticated in the ProActive server have their own role according to granted permissions.
ProActive uses the standard
https://www.oracle.com/technetwork/java/javase/jaas/index.html[Java Authentication and Authorization Service (JAAS)^]
to address these needs.

The file `PROACTIVE_HOME/config/security.java.policy-server` allows configuring fine-grained access for all users, e.g. who has the right to:

* Deploy ProActive Nodes
* Execute jobs
* Pause the Scheduler
* etc

=== Default Groups/Roles

The following groups are defined by default in the `security.java.policy-server` file:

* `user`: can submit a job to the scheduler and control this job execution (pause, kill, etc). Cannot interact with other users jobs.
* `guests`: can only log into the scheduler/resource manager, but cannot interact with it.
* `scheduleradmins`: has the same permissions as `user` but can also control other users' job execution. Can manage the scheduler life-cycle such as pausing or stopping the scheduler.
* `nsadmins`: this group gives the permission to create, edit or remove Node Sources in the resource manager.
* `providers`: this group gives only the permission to add nodes to the resource manager or to an existing Node Source.
* `rmcoreadmins` : this group gives full interaction capability with the resource manager (create, edit, remove Node Sources, add nodes, etc).
* `server-admins` : this group combine the rights of `rmcoreadmins` and `scheduleradmins` groups.
* `admin` : this group gives full permission on every aspect, this group is [.underline]#deprecated# and is kept for backward compatibility purpose only (use the above `server-admins` for a global administrator instead).
* `watchers` : this group allows receiving notification events from the scheduler and resource manager, but cannot perform any action. It is mostly used internally.
* `citizen-ds` : this group gives enough permissions to software power users who can do moderate data analysis tasks (studio, catalog-portal, workflow-execution, job-analytics).
* `expert-ds` : this group gives permissions to expert data scientists to collect and clean a wide range of data, and to analyze it to solve business problems (studio, scheduler, catalog-portal, workflow-execution, service-automation, job-analytics, job-gantt, job-planner-calendar-def, job-planner-calendar-def-workflows, job-planner-execution-planning, job-planner-gantt-chart, notification-portal).

[_creating_new_groups]
=== Creating New Groups

New groups can be defined by copying an existing group definition and edit its permissions.

TIP: The `PROACTIVE_HOME/tools/copy-role` command can be used to duplicate an existing group inside the `security.java.policy-server` file. See <<_copy_role_command>>.

WARNING: Any modification to the `security.java.policy-server` file is taken into account periodically by the ProActive server (by default every 30 seconds). This period can be configured in
`PROACTIVE_HOME/config/rm/settings.ini:pa.rm.auth.policy.refreshperiod.seconds` and `PROACTIVE_HOME/config/scheduler/settings.ini:pa.scheduler.auth.policy.refreshperiod.seconds`.

A *group* is defined using the following syntax:

----
grant principal org.ow2.proactive.authentication.principals.GroupNamePrincipal "mygroup" {
  // roles definition
}
----

Each group must at least contain the following permissions (without them, the ProActive software will not work properly):

----
// AuthPermission is requires for those who would like to access any mbean
permission javax.security.auth.AuthPermission "getSubject";
permission java.lang.RuntimePermission "setContextClassLoader";
permission javax.management.MBeanPermission "-#-[-]", "queryNames";
permission javax.management.MBeanPermission "javax.management.MBeanServerDelegate#-[JMImplementation:type=MBeanServerDelegate]", "addNotificationListener";
permission javax.management.MBeanPermission "org.ow2.proactive.scheduler.core.jmx.mbean.MyAccountMBeanImpl#*[*:*]", "*";
permission javax.management.MBeanPermission "org.ow2.proactive.scheduler.core.jmx.mbean.RuntimeDataMBeanImpl#*[*:*]", "*";
permission javax.management.MBeanPermission "org.ow2.proactive.resourcemanager.core.jmx.mbean.MyAccountMBeanImpl#*[*:*]", "*";
permission javax.management.MBeanPermission "org.ow2.proactive.resourcemanager.core.jmx.mbean.RuntimeDataMBeanImpl#*[*:*]", "*";

// Granting file reading permission i.e. to read RRD database via JMX
permission java.io.FilePermission "<<ALL FILES>>", "read";

// API - access to database
permission java.sql.SQLPermission "setLog";
permission java.sql.SQLPermission "callAbort";
permission java.sql.SQLPermission "setSyncFactory";
permission java.sql.SQLPermission "setNetworkTimeout";
permission java.util.PropertyPermission "*", "read, write";
permission java.net.SocketPermission "*", "accept, connect, listen, resolve";
----

In addition to this common set of permissions, the group should contain permissions related to <<_resource_manager_permissions,Resource Manager access>>, <<_scheduler_permissions,Scheduler access>>, <<_dataspace_permissions,Dataspace access>> and <<_other_components_permissions,Other Components access>>.

=== Resource Manager Permissions

==== Core Resource Manager Permissions

Resource Manager permissions are defined using the following syntax:

----
permission org.ow2.proactive.permissions.ServiceRolePermission "org.ow2.proactive.resourcemanager.core.RMCore.<role>"
----

`<role>` can be one of:

* `basic`: login, logout.
* `read`: read access to the Resource Manager information such as node sources, nodes, etc.
* `write`: ability to use nodes in the Resource Manager (necessary to submit workflows).
* `provider`: ability to add/remove nodes to/from the Resource Manager.
* `nsadmin`: ability to create, edit, deploy, undeploy or remove node sources.
* `admin`: access to some specific administrative features such as server thread dump, loggers level change.
* `*`: shortcut for all above permissions.

When a group needs to have access to several of the above permissions, it must be specified in separate lines, for example:

----
permission org.ow2.proactive.permissions.ServiceRolePermission "org.ow2.proactive.resourcemanager.core.RMCore.basic"
permission org.ow2.proactive.permissions.ServiceRolePermission "org.ow2.proactive.resourcemanager.core.RMCore.read"
permission org.ow2.proactive.permissions.ServiceRolePermission "org.ow2.proactive.resourcemanager.core.RMCore.write"
----

==== Resource Manager Permissions Advanced Syntax

`ServiceRolePermission` is a coarse-grained permission definition, in order to fine tune permissions, it is possible to use either of the following syntax:

----
permission org.ow2.proactive.permissions.MethodCallPermission "org.ow2.proactive.resourcemanager.core.RMCore.<method_name>";
permission org.ow2.proactive.permissions.DeniedMethodCallPermission "org.ow2.proactive.resourcemanager.core.RMCore.<method_name>";
----

`MethodCallPermission` is an additive permission definition, which allows a member of the group to call the Java method `<method_name>` of class `org.ow2.proactive.resourcemanager.core.RMCore`.

`DeniedMethodCallPermission` is a subtractive permission definition, which prevents a member of the group from calling the Java method `<method_name>` of class `org.ow2.proactive.resourcemanager.core.RMCore`.

The description of `org.ow2.proactive.resourcemanager.core.RMCore` methods is beyond the scope of this documentation.

==== Additional Resource Manager Permissions

Additionally, administrative roles should include the following permissions:

----
permission javax.management.MBeanPermission "org.ow2.proactive.resourcemanager.core.jmx.mbean.AllAccountsMBeanImpl#*[*:*]", "*";
permission javax.management.MBeanPermission "org.ow2.proactive.resourcemanager.core.jmx.mbean.ManagementMBeanImpl#*[*:*]", "*";
----

=== Scheduler Permissions

==== Core Scheduler Permissions

Scheduler permissions are defined using the following syntax:

----
permission org.ow2.proactive.permissions.ServiceRolePermission "org.ow2.proactive.scheduler.core.SchedulerFrontend.<role>";
----

`<role>` can be one of:

* `basic`: login, logout.
* `read`: read access to scheduler information (list of jobs, etc).
* `write`: ability to submit, pause, resume, kill or remove a job.
* `admin`: access to some specific administrative features such as pause, freeze and stop the scheduler, accounting information, etc.
* `*`: shortcut for all above permissions.

==== Scheduler Permissions Advanced Syntax

`ServiceRolePermission` is a coarse-grained permission definition, in order to fine tune permissions, it is possible to use either of the following syntax:

----
permission org.ow2.proactive.permissions.MethodCallPermission "org.ow2.proactive.scheduler.core.SchedulerFrontend.<method_name>";
permission org.ow2.proactive.permissions.DeniedMethodCallPermission "org.ow2.proactive.scheduler.core.SchedulerFrontend.<method_name>";
----

`MethodCallPermission` is an additive permission definition, which allows a member of the group to call the Java method `<method_name>` of class `org.ow2.proactive.scheduler.core.SchedulerFrontend`.

`DeniedMethodCallPermission` is a subtractive permission definition, which prevents a member of the group from calling the Java method `<method_name>` of class `org.ow2.proactive.scheduler.core.SchedulerFrontend`.

==== Additional Scheduler Permissions

In addition to these standard set of permissions, a few special permissions are defined:

----
permission org.ow2.proactive.scheduler.permissions.HandleOnlyMyJobsPermission "true";
----
_Mandatory_ for any user that submits workflows, defines whether each member of the group can interact (kill, pause, resume, etc) with other users jobs (`false`) or only with the jobs that belong to the user (`true`).

----
permission org.ow2.proactive.scheduler.permissions.OtherUsersJobReadPermission;
----
_Optional_ permission that allows a group of users to have read-only access to other users jobs.
With this permission enabled, a user will be able to read the details of other users' jobs (such as logs, results, etc.) but will not be able to perform an operation that will modify the job execution (pause, kill, etc.).

----
permission org.ow2.proactive.scheduler.permissions.ChangePriorityPermission "1,2,3";
----
_Mandatory_ for any user that submits workflows, defines the set of link:../user/ProActiveUserGuide.html#_job_priority[Job Priorities] that the group is allowed to use in submitted workflows.

Possible values are:

 * `0`: Idle
 * `1`: Lowest
 * `2`: Low
 * `3`: Normal
 * `4`: High
 * `5`: Highest

When a member tries to submit a workflow with a priority not included in the defined set, an error will occur and the submission will be cancelled.

----
permission org.ow2.proactive.scheduler.permissions.HandleJobsWithGenericInformationPermission "<gi_definitions>";
----
_Optional_ permission that allows a group of users to interact with any job that contain the mentioned link:../user/ProActiveUserGuide.html#_generic_information[Generic Information].

`<gi_definitions>` can either be empty or contain a comma separated list of generic information pairs, such as `gi1=value1,gi2=value2`.

When more than one generic information is specified, then a member can interact with jobs which contain all generic information specified with their values. A job which contain a single generic information in the list will not be elected.

----
permission org.ow2.proactive.scheduler.permissions.ConnectToResourceManagerPermission;
----
_Administrative_ permission that allows linking the scheduler to an existing resource manager.

----
permission org.ow2.proactive.scheduler.permissions.ChangePolicyPermission;
----
_Administrative_ permission that allows changing the scheduler's <<_scheduling_policies,Scheduling Policy>>.

----
permission org.ow2.proactive.scheduler.permissions.TenantAllAccessPermission;
----
_Administrative_ permission that allows access to all tenants when tenant isolation is enabled. See <<_tenants>>.

Finally, the following permissions are needed for _administrative_ access to the scheduler:

----
permission javax.management.MBeanPermission "org.ow2.proactive.scheduler.core.jmx.mbean.AllAccountsMBeanImpl#*[*:*]", "*";
permission javax.management.MBeanPermission "org.ow2.proactive.scheduler.core.jmx.mbean.ManagementMBeanImpl#*[*:*]", "*";
----

=== Dataspace Permissions

Access to the link:../user/ProActiveUserGuide.html#_global_and_user_spaces[Global Space] through <<_glossary_workflow_studio,Workflow Studio portal>> or <<_glossary_workflow_automation_portal,Workflow Execution portal>> is controlled with the following permission:

----
permission org.ow2.proactive.permissions.ServiceRolePermission "org.ow2.proactive_grid_cloud_portal.dataspace.RestDataspaceImpl.global.<role>";
----

`<role>` can be one of:

* `read`: read access to the dataspace (list contents, download file).
* `write`: write access to the dataspace (create folder, delete folder, upload file).
* `*`: shortcut for all above permissions.

Similarly, access to the link:../user/ProActiveUserGuide.html#_global_and_user_spaces[User Space] through <<_glossary_workflow_studio,Workflow Studio portal>> or <<_glossary_workflow_automation_portal,Workflow Execution portal>> is controlled with the following permission:

----
permission org.ow2.proactive.permissions.ServiceRolePermission "org.ow2.proactive_grid_cloud_portal.dataspace.RestDataspaceImpl.user.*";
----


=== Other Components Permissions

In addition to Resource Manager and Scheduler, a few permissions can be defined for other components.

==== Portals access

----
permission org.ow2.proactive_grid_cloud_portal.common.PortalAccessPermission "<portal_list>";
----
_Mandatory_ permission that defines which portals are accessible for members of the group.

`<portal_list>` is either `*` to allow accessing all portals or a comma-separated list of portal names:

* `rm`: allow access to the <<_glossary_rm_web_interface,Resource Manager portal>>.
* `scheduler`: allow access to the <<_glossary_scheduler_web_interface,Scheduler portal>>.
* `studio`: allow access to the <<_glossary_workflow_studio,Workflow Studio portal>>.

* `automation-dashboard`: allow access to the <<_glossary_automation_dashboard,Automation Dashboard portal>>, which contain the following portals
(note that in order to access any of the portal below, `automation-dashboard` must be accessible):

** `workflow-execution`: allow access to the <<_glossary_workflow_automation_portal,Workflow Execution portal>>.
** `catalog`: allow access to the <<_glossary_catalog_portal,Catalog portal>>.
** `health-dashboard`: allow access to the <<_glossary_health_dashboard,Health Dashboard>>, part of the <<_glossary_analytics,Analytics portal>>.
** `job-analytics`: allow access to <<_glossary_analytics_portal,Job Analytics>>, part of the <<_glossary_analytics,Analytics portal>>.
** `job-gantt`: allow access to the <<_glossary_job_gantt,Job Gantt>>, part of the <<_glossary_analytics,Analytics portal>>.
** `node-gantt`: allow access to the <<_glossary_node_gantt,Node Gantt>>, part of the <<_glossary_analytics,Analytics portal>>.
** `job-planner-calendar-def`: allow access to <<_glossary_job_planner_calendar_definition_portal,Calendar Definition>>, part of the <<_glossary_job_planner_portal,Job Planner portal>>.
** `job-planner-calendar-def-workflows`: allow access to <<_glossary_job_planner_calendar_asssociation,Calendar Associations>>, part of the <<_glossary_job_planner_portal,Job Planner portal>>.
** `job-planner-execution-planning`: allow access to the <<_glossary_job_planner_execution_planning,Execution Planning>>, part of the <<_glossary_job_planner_portal,Job Planner portal>>.
** `job-planner-gantt-chart`: allow access to the <<_glossary_job_planner_gantt_chart,Gantt Chart>>, part of the <<_glossary_job_planner_portal,Job Planner portal>>.
** `event-orchestration`: allow access to the <<_glossary_event_orchestration_portal,Event Orchestration portal>>.
** `service-automation`: allow access to the <<_glossary_service_automation_portal,Service Automation portal>>.
** `notification-portal`: allow access to the <<_glossary_notification_portal,Notification portal>>.

==== Service Automation permissions

----
permission org.ow2.proactive.permissions.PcaAdminPermission;
----
This permission gives _administrative_ access to <<_glossary_cloud_automation_service,Service Automation>>. A member of a group with such permission can interact with any service (not only owned services).

==== Job Planner permissions

----
permission org.ow2.proactive.scheduler.permissions.JobPlannerAllAccessPermission;
----
This permission gives _administrative_ access to <<_glossary_job_planner,Job Planner>>. A member of a group with such permission can interact with any calendar association (not only owned associations).

==== Notification Service permissions

----
permission org.ow2.proactive.permissions.NotificationAdminPermission;
----
This permission gives _administrative_ access to <<_glossary_notification_service,Notification Service>>, allowing to delete any notification.

== Deploy ProActive Nodes

In the ProActive Scheduler, Computational Resources are managed by the ProActive Resource Manager component.

The ProActive Resource Manager supports ProActive Nodes aggregation from heterogeneous environments.
As a node is just a process running somewhere, the process of communication to such nodes is unified.
The only part which has to be defined is the procedure of nodes deployment which could be quite
different depending on infrastructures and their limitations. After installation of the server and node parts, it is
possible to configure an automatic nodes deployment. Basically, you can tell the Resource Manager how to
launch nodes and when.

In the ProActive Resource Manager, all the computational resources (<<_glossary_compute_host,Compute Hosts>>) are managed as a set of <<_glossary_node_source,*Node Sources*>>.
Each node source contains a set of <<_glossary_proactive_node,*ProActive Nodes*>> which shares the same deployment mechanism and access policy.
[NOTE]
====
Examples of a Node Source:

* a cluster with SSH access where nodes are available from 9 a.m. to 9 p.m.
* nodes from Amazon EC2 available permanently for users from group 'cloud'.
====

image::rm-portal.png[align=center]
As shown in the previous screenshot, the ProActive Resource Manager provides a <<_glossary_rm_web_interface,*Resource Manager Web Interface*>> to show and manage all the node sources with its nodes.

Adding a cluster of Compute Hosts to the ProActive Resource Manager requires to *create a node source*.
Its typical process involves copying the necessary ProActive binary files to all those Compute Hosts,
running ProActive Nodes on the hosts, and connecting the nodes to the ProActive Resource Manager.
There are two principal ways of doing that:

- Launch a process of ProActive Node *on the Compute Host* and connect it to the ProActive Resource Manager
- Initiate the deployment *from the ProActive Resource Manager*

If you are not familiar with ProActive Workflows & Scheduling product you may want to try the <<run_node_manually,*first method*>> as it's directly executed on the Compute Hosts.
Combined with <<_proactive_agents,*ProActive Agents*>> which control and limit the resources utilization on the Compute Hosts,
it gives you the same result as the *second method*.

The <<_deploy_nodes_from_proactive_rm,*second method*>> commands the ProActive Resource Manager to remotely launch the process of ProActive Nodes on Compute Hosts.
The method implies that you have a remote access to Compute Hosts (e.g., SSH access).

The ProActive Nodes can be deployed on various kinds of computational resources, such as:

- the local Server Host (i.e., the machine where the ProActive server is installed)
- remote compute servers
- Cloud Platforms (e.g., Amazon Web Services, OpenStack, GoogleComputeEngine)
- an existing cluster managed by another scheduler (e.g., SLURM, LSF, PBS)

A node source is defined by two parts of configurations: <<_glossary_node_source_infrastructure,*Node Source Infrastructure*>> and <<_glossary_node_source_policy,*Node Source Policy*>>.
Each *Node Source Infrastructure* defines the deployment mechanism for a specific kind of computational resources.
To support the deployment of these various computational resources, the ProActive Scheduler provides a set of supported <<_node_source_infrastructures, Node Source Infrastructures>>.
The *Node Source Policy* controls the resource usage. See the section <<_control_the_resource_usage>> for more details.
This section concentrates on the deployment mechanism of ProActive Nodes which is defined by the Node Source Infrastructure.

In the ProActive Resource Manager, there is always a *default node source* consisting of a <<_default_infrastructure>>
and <<_static_policy>>. It is not able to deploy nodes anywhere but makes it possible to add existing nodes to the RM.

When you connect the Compute Hosts to the ProActive Resource Manager through the *first method*, a node source of <<_default_infrastructure,*Default Infrastructure*>> is automatically created.
Through the *second method*, you just need to choose the proper node source infrastructure and fill out its required parameters.
//The *second method* supports all the various kinds of computational resources by using different <<_node_source_infrastructures,*node source infrastructures*>> to create Node Sources.

[[run_node_manually]]
=== Deploy ProActive Nodes manually
Let's take a closer look at the first method described above. To deploy a ProActive Node from the Compute Host, you have three ways to do it:

- using <<proactive_node,*proactive-node*>> command
- using <<node_jar,*node.jar*>>
- using <<proactive_node_autoupdate,*proactive-node-autoupdate*>> command

[[proactive_node]]
==== Using proactive-node command

To use `proactive-node` command, you need to unpack the ProActive node release archive inside the Compute Host and run the following command
----
$ PROACTIVE_HOME/bin/proactive-node -r pnp://SCHEDULER_ADDRESS:64738
----
where `-r` option is used to specify the URL of the <<_glossary_resource_manager,*Resource Manager*>>.
You can find this URL in the output of the <<_run_the_proactive_scheduler,ProActive Scheduler>> (the Resource Manager URL).
For example, in case of the following sample output of the ProActive server command:
....
The router created on localhost:33647
Starting the scheduler...
Starting the resource manager...
The resource manager with 4 local nodes created on pnp://localhost:41303/
The scheduler created on pnp://localhost:41303/
The web application /automation-dashboard created on http://localhost:8080/automation-dashboard
The web application /studio created on http://localhost:8080/studio
The web application /job-analytics created on http://localhost:8080/job-analytics
The web application /notification-service created on http://localhost:8080/notification-service
The web application /cloud-automation-service created on http://localhost:8080/cloud-automation-service
The web application /catalog created on http://localhost:8080/catalog
The web application /proactive-cloud-watch created on http://localhost:8080/proactive-cloud-watch
The web application /scheduler created on http://localhost:8080/scheduler
The web application /scheduling-api created on http://localhost:8080/scheduling-api
The web application /connector-iaas created on http://localhost:8080/connector-iaas
The web application /rm created on http://localhost:8080/rm
The web application /rest created on http://localhost:8080/rest
The web application /job-planner created on http://localhost:8080/job-planner
*** Get started at http://localhost:8080 **
....
You should use `pnp://localhost:41303/` as the Resource Manager URL.

TIP: You can also use *discovery* to let the ProActive Node find the URL to connect to on its own.
 Simply run `proactive-node` without any parameter to use discovery.
 It uses broadcast to retrieve the URL so this feature might not work depending on your network configuration.

If you want to run multiple tasks at the same time on the same machine, you can either start a few `proactive-node` processes
or start multiple nodes from the same process using the `-w` command line option.

[[node_jar]]
==== Using node.jar

It is also possible to launch a ProActive Node without copying the ProActive Server release to the Compute Host:

* Open a browser on the Compute Host.
* Navigate to the Resource Manager Web Interface. You can find the URL in the output of the <<_run_the_proactive_scheduler,ProActive Scheduler>> (the Resource Manager web application URL, e.g., `\http://localhost:8080/rm`).
* Use default demo/demo account to access the Resource Manager Web Interface.
* Click on 'Portal->Launch' to download *node.jar*.
* Click on 'Portal->Create Credentials' and download your credential file.
* Create a Node Source using the infrastructure DefaultInfrastructureManager.
* Run it:
----
$ java -Dproactive.communication.protocol=pnp -jar node.jar -f CREDENTIAL_FILE -s NAME
----
Where NAME is the name of the node source.
It should connect to the ProActive Resource Manager automatically using the discovery mechanism otherwise you might
have to set the URL to connect to with the `-r` parameter.

TIP: If you would like to execute several Tasks at the same time on one host, you can either launch several ProActive
Node process or use the _-w_ parameter to run multiple nodes in the same process. A node executes one Task at a time.

[[proactive_node_autoupdate]]
==== Using proactive-node-autoupdate command

This method is a combination of the two above, it starts a node from the command line, and makes sure the node library (node.jar) is synchronized with the latest server version.

The `proactive-node-autoupdate` command acts as a bootstrap and spawns a new java virtual machine with the up-to-date library classpath.

Launching the node is similar to the `proactive-node` command, additional options must be specified to allow the download of the node.jar from the server:

----
$ PROACTIVE_HOME/bin/proactive-node-autoupdate -r pnp://SCHEDULER_ADDRESS:64738 -nju http://SCHEDULER_ADDRESS:8080/rest/node.jar -njs /tmp/node.jar -nja
----

where `-nju` option is used to specify the http URL of the *node.jar*.
You can find this URL in the output of the <<_run_the_proactive_scheduler,ProActive Scheduler>>, the node.jar URL is built by appending */rest/node.jar* to the base HTTP URL of the server.

`-nju` option is used to specify where the node.jar will be stored locally on the machine.

Finally, `-nja` option, when enabled, specifies that the `proactive-node-autoupdate` will be always up, it means that it will automatically restart when the node terminates (for example in case of server upgrades).

[[_proactive_agents]]
=== Deploy ProActive Nodes via Agents

In your production environment you might want to control and limit the resources utilization on some or all Compute Hosts,
especially if those are desktop machines where people perform their daily activities. Using *ProActive Agent* you can:

* Control the number of ProActive Node processes on each Compute Host
* Launch ProActive Nodes automatically when a Compute Host starts
* Restart ProActive Nodes if they fail for some reason and reconnect them to ProActive Resource Manager

*ProActive Agents* exists for both <<_proactive_linux_agent,Linux>> and <<_proactive_windows_agent,Windows>>
 operating systems.

[[_proactive_linux_agent]]
==== ProActive Linux Agent

The ProActive Agent for Linux can be downloaded from ActiveEon's https://www.activeeon.com/community-downloads[website^].

To install the ProActive Agent on Debian based distributions (the archive name will vary depending on the version and architecture):

    sudo apt install ./proactive-agent.deb

TIP: Some extra dependencies might be required, to install them: `sudo apt-get -f install`

To install the ProActive Agent on Redhat based distributions (the archive name will vary depending on the version and architecture):

    sudo rpm -i proactive-agent.rpm

TIP: Some extra dependencies might be required, to install them you need to install each dependency individually using `sudo yum install`

By default the Linux Agent will launch locally as many nodes as the number of
CPU cores available on the host minus one. The URL to use for connecting nodes
to the Resource Manager can be configured in `/opt/proactive-agent/config.xml`,
along with several other parameters. If no URL is set, then broadcast is used
for auto discovery. However, this feature might not work depending on your
network configuration. Consequently, it is recommended to set the Resource
Manager URL manually.

Logs related to the Agent daemon are located in
`/opt/proactive-agent/proactive-agent.log`.

Binaries, configuration files and logs related to ProActive nodes started by the
agent are available at `/opt/proactive-node/`, respectively in subfolders `dist/lib`,
`config` and `logs`.

===== Configuring the Linux Agent

To configure the agent behaviour:

* Stop the Linux Agent
+
----
sudo /etc/init.d/proactive-agent stop
----

* Update the *config.xml* file in `/opt/proactive-agent` folder
+
In case there is no such directory, create the directory. Then, create the *config.xml* file or create a symbolic link
+
----
sudo mkdir -p /opt/proactive-agent
sudo ln -f -s <path-to-config-file> /opt/proactive-agent/config.xml
----

* Change the ownership of the *config.xml* file
+
----
sudo chown -Rv proactive:proactive config.xml
----
+
If the group named "proactive" does not exist, create the group and add "proactive" user to the group:
+
----
sudo groupadd proactive
sudo usermod -ag proactive proactive
----

* Start the Linux Agent

    sudo /etc/init.d/proactive-agent start


===== Uninstalling the Linux Agent

On Debian based distributions:

    sudo apt purge proactive-agent

On Redhat based distributions:

    sudo yum remove proactive-agent

[[_proactive_windows_agent]]
==== ProActive Windows Agent

The ProActive Windows Agent is a *Windows Service*: a long-running executable that performs specific
functions and which is designed to not require user intervention. The agent is able to create a ProActive Node
 on the local machine and to connect it to the *ProActive Resource Manager*.

After being installed, it:

* Loads the user's configuration
* Creates schedules according to the working plan specified in the configuration
* Spawns a ProActive Node that will run a specified java class depending on the selected connection type. 3 types of connections are available:
** *Local Registration* - The specified java class will create a ProActive Node and register it locally.
** *Resource Manager Registration* - The specified java class will create a ProActive Node and register
it in the specified Resource Manager, thus being able to execute java or native tasks received from the Scheduler. It is is important
to note that a ProActive Node running tasks can potentially spawn child processes.
** *Custom* - The user can specify his own java class.
* Watches the spawned ProActive Nodes in order to comply with the following limitations:
** *RAM limitation* - The user can specify a maximum amount of memory allowed for a ProActive Node and its children. If the limit is reached, then all processes are automatically killed.
** *CPU limitation* - The user can specify a maximum CPU usage allowed for a ProActive Node and its children. If the limit is exceeded by the sum of CPU usages of all processes, they are automatically throttled to reach the given limit.
* Restarts the spawned ProActive Node in case of failures with a timeout policy.

==== Install Agents on Windows

The ProActive Windows Agent installation pack is available on the https://www.activeeon.com/community-downloads[ProActive website^].
Run the `setup.exe` file and follow instructions. When the following dialog appears:

image::install_config.png[align=left, width=512, height=256, title=Windows agent installation]

. Specify the directory that will contain the *configuration* file named `PAAgent-config.xml`, note that if this file already exists in the specified directory it will be re-used.
. Specify the directory that will contain the *log files* of the ProActive Agent and the spawned runtimes.
. Specify an existing, local *account* under which the ProActive Nodes will be spawned. It is highly recommended to specify an account that is not part of the Administrators group to isolate the ProActive Node and reduce security risks.
. The *password* is encrypted using Microsoft AES Cryptographic Provider and only Administrators have access permissions to the keyfile (restrict.dat) this is done using the SubInACLtool.
. If the specified account does not exist the installation program will prompt the user to create a non-admin account with the required privileges.
+
Note that the ProActive Agent service is installed under LocalSystem account, this should not be changed,
however it can be using the `services.msc` utility. ('Control Panel->Administrative Tools->Services')

. If you want that any non-admin user (except guest accounts) be able to start/stop the ProActive Agent service check the "Allow everyone to start/stop" box. If this option is checked the installer will use the SubInACL tool. If the tool is not installed in the `Program Files\Windows Resource Kits\Tools` directory the installer will try to download its installer from the official Microsoft page.
. The installer will check whether the selected user account has the required privileges. If not follow the steps to add these privileges:
.. In the 'Administrative Tools' of the 'Control Panel', open the 'Local Security Policy'.
.. In 'Security Settings', select 'Local Policies' then select 'User Rights Assignments'.
.. Finally, in the list of policies, open the properties of 'Replace a process-level token' policy and add the needed user. Do the same for 'Adjust memory quotas for a process'. For more information about these privileges refer to the official Microsoft page.

At the end of the installation, the ProActive Agent Control utility should be started. This next section explains how to configure it.

To uninstall the ProActive Windows Agent, simply run 'Start->Programs->ProActiveAgent->Uninstall ProActive Agent'.

==== Configure Agents on Windows

To configure the Agent, launch 'Start->Programs->ProActiveAgent->AgentControl' program or click on
the notify icon if the "Automatic launch" is activated.
Double click on the tray icon to open the *ProActive Agent Control* window. The following window will appear:

image::agent_control.png[align=left, width=612, height=306, title=Windows Agent GUI]

From the ProActive Agent Control window, the user can load a configuration file, edit it, start/stop the service and view logs.
A GUI for editing is provided (explained below). Even if it is not recommended, you can edit the configuration file by yourself with your favorite text editor.

It is also possible to change the ProActive Nodes Account using the 'Change Account' button.

When you click on 'GUI Edit', the following window appears:

image::config_editor_general.png[align=left, width=612, height=306, title=General configuration]

In the general tab, the user can specify:

* The *ProActive Scheduler* location.
* The *JRE* location (the Java Runtime Environment is provided with the agent archive under this path `%programfiles(x86)%\ProActive Agent\schedworker\jre`).
* The *numbers of Runtimes and Nodes* (the number of spawned processes and the number of ProActive Nodes per process).
* The *numbers of workers* (the number of ProActive Nodes allocated in the Compute Host)
* The *JVM options*. Note that if the parameter contains `${rank}`, it will be dynamically replaced by the ProActive Node rank starting from 0.
* The *On Runtime Exit* script. A script executed after a ProActive Node exits. This can be useful to perform additional cleaning operation. Note that the script receives as parameter the PID of the ProActive Node.
* The user can set a *memory limit* that will prevent the spawned processes to exceed a specified amount of RAM. If a spawned process or its child process requires more memory, it will be killed as well as its child processes. Note that this limit is disabled by default (0 means no limit) and a ProActive Node will require at least 128 MBytes.
* It is possible to list all available *network interfaces* by clicking on the "Refresh" button and add the selected network interface name as a value of the `proactive.net.interface` property by clicking on "Use" button. See the ProActive documentation for further information.
* The user can specify the *protocol* (PNP or PAMR) to be used by the ProActive Node for incoming communications. If the selected protocol is PAMR, then the user has to add `-Dproactive.pamr.router.address` JVM property.
* To ensure that a unique port is used by a ProActive Node, the initial port value will be incremented for each node process and given as value of the `-Dproactive.SELECTED_PROTOCOL.port` JVM property. If the port chosen for a node is already used, it is incremented until an available port number is found.

Clicking on the 'Connection' tab, the window will look like this:

image::config_editor_connection.png[align=left, width=612, height=306, title=Connection configuration]

In the 'Connection' tab, the user can select between three types of connections:

* *Local Registration* - creates a local ProActive node and registers (advertises) it in a local RMI registry. The node name is optional.
* *Resource Manager Registration* - creates a local ProActive node and registers it in the specified Resource Manager. The mandatory Resource Manager's URL must comply with the following format: *protocol://host:port/*.
In case of the PAMR protocol, the URL is of the form pamr://resource_manager_id, which is by default pamr://0 (default value of the resource manager id). The PAMR router host and port are provided using JVM properties.
The node name and the node source name are optional. Since the Resource Manager requires authentication, the user specifies the file that contains the credential. If no file is specified the default one located in `%USERPROFILE%\.proactive\security` folder is used.
* *Custom* - the user specifies his own java starter class and the arguments to be given to the main method. The java starter class must be in the classpath when the ProActive Node is started.

Finally, clicking on the "Planning" tab, the window will look like this:

image::config_editor_planning.png[width=612, height=412, align=left, title=Planning configuration]

In the Planning Tab, depending on the selected connection type, the agent will initiate it according to a *weekly planning* where each plan specifies the connection start time as well as the working duration. The agent will end the connection as well as the ProActive Nodes and its child processes when the plan duration has expired.

Moreover, it is possible to specify the ProActive Node Priority and its *CPU usage* limit. The behavior of the CPU usage limit works as follows: if the ProActive Node spawns other processes, they will also be part of the limit so that if the sum of CPU% of all processes exceeds the user limit they will be throttled to reach the given limit. Note that if the Priority is set to RealTime the CPU % throttling will be disabled.

The "Always available" makes the agent to run permanently with a Normal Priority and Max CPU usage at 100%.

==== Launching Windows Agent

Once you have configured the agent, you can start it clicking on the "Start" button of the ProActive Agent Control window. However, before that, you have to ensure that *ProActive Scheduler* has been started on the address you specified in the agent configuration. You do not need to start a node since it is exactly the job of the agent.

Once started, you may face some problems. You can realise that an error occurred by first glancing at the color of the agent tray icon. If everything goes right, it should keep the blue color. If its color changes to yellow, it means that the agent has been stopped. To see exactly what happened, you can look at the runtime log file located into the agent installation directory and named `Executor<runtime number>Process-log.txt`.

The main troubles you may have to face are the following ones:

* You get an *access denied error*: this is probably due to your default java.security.policy file which cannot be found. If you want to specify another policy file, you have to add a JVM parameter in the agent configuration. A policy file is supplied in the scheduling directory. To use it, add the following line in the JVM parameter box of the agent configuration (Figure 5.3, “Configuration Editor window - General Tab ”):
----
-Djava.security.policy=PROACTIVE_HOME/config/security.java.policy-client
----
* You get an *authentication error*: this is probably due to your default credentials file which cannot be found. In the "Connection" tab of the Configuration Editor (Figure 5.4, “Configuration Editor window - Connection Tab (Resource Manager Registration)”), you can choose the credentials file you want. You can select, for instance, the credentials file located at PROACTIVE_HOME/config/authentication/scheduler.cred or your own credentials file.
* The node seems to be well started, but you cannot see it in the Resource Manager interface: in this case, make sure that the *port number* is the good one. Do not forget that the runtime port number is incremented from the initial ProActive Resource Manager port number. You can see exactly on which port your runtime has been started looking at the log file described above.
* The agent does not seem to be starting at all and the log file `Executor<runtime number>Process-log.txt` (see above) is empty. This issue has been observed on some Windows 10 environments. When facing this problem, open Windows https://en.wikipedia.org/wiki/Event_Viewer[Event Viewer] and look at errors associated with application *ProActiveAgent*. Follow the procedure <<_windows_10_patch>> if you see the following error:
----
JobManagement.JobException: The management process is running inside a Job, it will not be able to assign child processes to a new Job.
   à JobManagement.JobObject.ProbeForRunningInJob()
   à JobManagement.JobObject.CreateJob()
   à JobManagement.JobObject..ctor(String name)
   à ProActiveAgent.ProActiveRuntimeExecutor..ctor(CommonStartInfo commonStartInfo, Int32 rank)
   à ProActiveAgent.ExecutorsManager..ctor(AgentType configuration)
   à ProActiveAgent.WindowsService.OnStart(String[] args)
   à System.ServiceProcess.ServiceBase.ServiceQueuedMainCallback(Object state)
----

===== Windows 10 Patch

In order to solve the issue mentioned above, please do the following:

1. Download the patch zip from the following https://www.activeeon.com/public_content/patches/ProActiveAgent_Win10_patch.zip[link] and expand this archive inside an empty folder.
2. Close the agent GUI if it's open. In order to do so, in the https://support.microsoft.com/en-us/windows/customize-the-taskbar-notification-area-e159e8d2-9ac5-b2bd-61c5-bb63c1d437c3[Windows notification area] (aka system tray), find the ProActive Agent icon image:ProActiveAgentIcon.png[], right-click on it and select *Close*. A warning message will appear (simply confirm).
3. Replace existing files in the ProActive Agent installation folder `C:\Program Files (x86)\ProActiveAgent` with patch files copied from the expanded zip.
4. Start the ProActive Agent again. If the agent does not start and display the following error, start the service called *ProActiveAgent* manually from Windows services user interface (`services.exe`).

image::ProActiveAgentStartFailure.png[align=left, width=383, height=206, title=Start failure after Windows 10 Patch]


==== Automate Windows Agent Installation

The automated installation can be run through a command line (cmd.exe) with administrator priviledge.
This can be useful to trigger installation. In order to launch the silent installation, the /S option must be passed in the command line to the ProActive Windows Agent installer.
Several options are required, such as the user and password.

Here is an example of automated installation command:

```
ProactiveAgent-8.3.0-standalone-x64-setup.exe /S /USER=proactive /PASSWORD=proactive
```

Optionally you can add a domain name.

```
ProactiveAgent-8.3.0-standalone-x64-setup.exe /S /USER=proactive /PASSWORD=proactive /DOMAIN=mydomain
```

You can also activate compatibility mode from the command line if you have any problems:

```
set __COMPAT_LAYER=WINXPSP3
ProactiveAgent-8.3.0-standalone-x64-setup.exe /S /USER=proactive /PASSWORD=proactive
```


Here is the full list of command line options which can be passed to the installer:



[cols=2*]
|===

|`/S` | Run silently without graphical interface, uninstall any previous installation.

|`/UN` | Run the uninstall only.

|`/ALLOW` | Allow all users to control the service.

|`/USER=user` | Associate `user` account to manage proactive agent service, create user if it does not exist.

|`/PASSWORD=pass` | Define password for the proactive agent `user`.

|`/DOMAIN=dom` | Specify `dom` as the windows domain, by default domain will be set to the machine host name.

|`/LOG_DIR=path` | Define `path` where the agent log files, default is `$PROGRAMFILES\ProActiveAgent\logs`.

|`/CONFIG_DIR=path` | Define `path` where the configuration files are stored, default is `$PROGRAMFILES\ProActiveAgent\config`.

|`/USE_ACC` | Use current user's account home as `/CONFIG_DIR` and `/LOG_DIR`.

|===

==== Configuring Linux Or Windows Agents For Auto-Update

The *Linux* or *Windows Agents* can be configured to automatically synchronize the node libary (node.jar) with the ProActive server.

The behavior is similar to the <<proactive_node_autoupdate,proactive-node-autoupdate>> command, the main JVM will act as a bootstrap and spawn a child JVM with the up-to-date library in its classpath.

In order to enable auto-update, you need to edit the agent configuration file. This file location is:

 * Linux Agent: `/opt/proactive-agent/config.xml`
 * Windows Agent: `C:\Program Files (x86)\ProActiveAgent\config\PAAgent-config.xml`

The following changes must be performed:

 1. Change the *<javaStarterClass>* to `org.ow2.proactive.resourcemanager.updater.RMNodeUpdater`
 2. Add the following *<jvmParameters>*
   * `node.jar.url`: URL of the node jar (similar to the `nju` option of the `proactive-node-autoupdate` command)
   * `node.jar.saveas`: path used to store the node.jar file locally (similar to the `njs` option of the `proactive-node-autoupdate` command)
 3. Other *JVM properties* specified will be forwarded to the spawned JVM, but non-standard options such as *-Xmx* will not be forwarded. In order to do so, you must declare them using the following syntax:

----
   <param>-DXtraOption1=Xmx2048m</param>
   <param>-DXtraOption2=Xms256m</param>
   ...
----

[[_deploy_nodes_from_proactive_rm]]
=== Deploy ProActive Nodes from the ProActive Resource Manager Interface

The second way of deploying ProActive Nodes is to create <<_glossary_node_source,*Node Sources*>> from the ProActive Resource Manager Interface.

ProActive Resource Manager provides three ways to create a Node Source:

* Use the *Resource Manager Web Interface* ('Add Node Source' menu).
* Use the https://try.activeeon.com/doc/rest/[REST API]
* Use the <<_resource_manager_command_line>>

Whichever the way you use to create a node source, ultimately you just need to choose a <<_node_source_infrastructures,*Node Source Infrastructure*>>
and a <<_node_source_policies,*Node Source Policy*>> that defines rules and limitations of nodes' utilization.
Then according to the chosen node source infrastructure and policy, you need to fill out the required parameters.

The choice of *Infrastructure Manager* depends on the type of your Compute Hosts.
The following part of this section presents how to deploy ProActive Nodes on various types of Compute Hosts:

- <<_deploy_local_nodes,the local Server Host>>
- <<_deploy_nodes_via_ssh,remote Compute Hosts accessible via SSH>>
- <<_deploy_cloud_nodes,cloud platforms>>
- <<_deploy_via_other_schedulers,clusters managed by other schedulers>>

[[_deploy_local_nodes]]
==== Deploy local nodes on the ProActive server
In order to deploy the ProActive nodes locally (i.e., on the <<_glossary_server_host,server host>> where the ProActive server is running),
you can just create a node source with the *Local Infrastructure*.

image::rm-portal-add-localns.png[align=center]
The previous screenshot shows how to create a local node source in the *Resource Manager Web Interface*.

To create a local node source from the command line:
[source]
----
$ PROACTIVE_HOME/bin/proactive-client --createns localNodeSourceName -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.LocalInfrastructure credentialsPath numberOfNodes timeout javaProperties
----

For example, the complete command to create a local node source with 7 nodes:
[source]
----
$ $PROACTIVE_HOME/bin/proactive-client -createns local_node_source -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.LocalInfrastructure $PROACTIVE_HOME/config/authentication/rm.cred 7 30000 "" -policy org.ow2.proactive.resourcemanager.nodesource.policy.StaticPolicy 'ALL' 'ALL'
----

See <<_local_infrastructure, *Local Infrastructure*>> reference for details of each parameter.

[[_deploy_nodes_via_ssh]]
==== Deploy ProActive nodes via SSH
In order to create an SSH Node Source you should first configure an *SSH access* from the server to the Compute Hosts
that http://www.linuxproblem.org/art_9.html[does not require password]. Then create a text file (refered to as `HOSTS_FILE` below) containig the hostnames of all your Compute Hosts.
Each line shoud have the format:
[source]
----
HOSTNAME NODES_COUNT
----
where NODES_COUNT is the number of ProActive Nodes to start (corresponds to the number of Tasks that can be executed in parallel) on the corresponding host. Lines beginning with `#` are comments. Here is an example:

[source]
----
# you can use network names
host1.mydomain.com 2
host2.mydomain.com 4
# or ip addresses
192.168.0.10 8
192.168.0.11 16
----

Then using this file create a Node Source either from the *Resource Manager Web Interface* or from the command line:
[source]
----
$ PROACTIVE_HOME/bin/proactive-client -createns SSH_node_source --infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.SSHInfrastructure HOSTS_FILE 60000 3 5000 "" /home/user/jdk/bin/java PROACTIVE_HOME Linux "" config/authentication/rm.cred
----

Don't forget to replace `HOSTS_FILE` and `PROACTIVE_HOME` with the corresponding values.

See <<_ssh_infrastructure>> reference for details on each parameter.

[[_deploy_cloud_nodes]]
==== Deploy ProActive Nodes on Cloud Platforms

The ProActive Resource Manager can automate the deployment of ProActive Nodes on most popular Cloud Platforms.
Currently the following Cloud Platforms are supported:

- <<_ec2_infrastructure,Amazon Web Service EC2>>
- <<_aws_autoscaling_infrastructure, Amazon Web Service Auto Scaling>>
- <<_openstack_infrastructure,OpenStack>>
- <<_vmware_infrastructure,VMWare>>
- <<_gce_infrastructure,Google Compute Engine>>
- <<_azure_infrastructure,Microsoft Azure>>
- <<_azure_scale_set_infrastructure,Microsoft Azure Scale Set>>
- <<_kubernetes_infrastructure,Kubernetes>>

Unlike directly deploying on the compute servers, on the Cloud Platforms
one or more virtual machines usually need to be deployed prior to launching a ProActive Node.
Each virtual machine corresponds an instance in a cloud Node Source.

So the definition of most cloud Node Source Infrastructures have the following three parameters:

- `numberOfInstances`: specifies the total number of virtual machines to be created on this node source.
- `numberOfNodesPerInstance`: specifies the total number of ProActive Nodes to be deployed in each created virtual machine.
- `image`: specifies which image is used to deploy the virtual machine (e.g., Linux or Windows).

The general process of deploying ProActive Nodes on a Cloud Platform is:

- first, create the required number of virtual machines with the specified image
- then, on each virtual machine, download the ProActive `node.jar` and install the required middleware (e.g., java)
- finally, on each virtual machine, launch the specified number of ProActive Nodes (i.e., `numberOfNodesPerInstance`) using node.jar

See <<_node_source_infrastructures, *Node Source Infrastructure*>> reference for details of each Cloud Node Source Infrastructure parameters.

[[_deploy_via_other_schedulers]]
==== Deploy ProActive Nodes dynamically via other schedulers (PBS, SLURM, ...)

This functionality is also called *_Meta-Scheduling_*.

If an existing cluster is available in your organization and this cluster is managed by a native scheduler such as *SLURM*, *LSF* or *PBS*, you may want to execute transparently on this cluster ProActive <<_glossary_workflow,*Workflows*>> and <<_glossary_task,*Tasks*>>.

As ProActive Tasks can be executed only on <<_glossary_proactive_node,*ProActive Nodes*>>, it is necessary to deploy ProActive Nodes on the cluster.

Dynamic deployment of ProActive Nodes on a cluster to execute ProActive Tasks is handled by defining a _Native Scheduler <<_glossary_node_source,Node Source>>_ containing three components:

 * The `Native Scheduler Infrastructure`: a <<_glossary_node_source_infrastructure,*Node Source Infrastructure*>> which allows to interact with a native scheduler to deploy ProActive Nodes.

 * The `Native Scheduler Policy`:  a <<_glossary_node_source_policy,*Node Source Policy*>> which interacts with the Native Scheduler Infrastructure to request ProActive Nodes deployment dynamically based on the <<_glossary_proactive_scheduler,*ProActive Scheduler*>> pending queue.

 * The `Native Scheduler Scheduling Policy`: a <<_glossary_scheduling_policy,*Scheduling Policy*>> which allows the execution of ProActive Tasks on ProActive Nodes provisioned by the Native Scheduler Policy.

The provisioning of ProActive Nodes is controlled by a specific <<_glossary_generic_information,*Generic Information*>> `"NS"` which can be defined at _Task_ or _Job_ level.

When this generic information is configured for a _Task_, a ProActive Node will be dynamically deployed on the cluster to execute the Task. This ProActive Node will be associated with the Task and will only accept this Task execution. When the Task terminates, the ProActive Node will also be terminated.

When this generic information is configured for a _Job_, all Tasks contained in the Job will execute on the cluster. Nodes will be created dynamically to execute Tasks of this Job, with a similar Task/Node association.

The behavior of the Meta-Scheduling feature is summarized on the following diagram (example for PBS integration):

image::NativeScheduler.png[PBS Integration, , title="PBS Integration"]

The following paragraphs explain how to configure the Native Scheduler Infrastructure, Native Scheduler Policy, Native Scheduler Scheduling Policy and execute ProActive tasks using a Native Scheduler Node Source.

===== Glossary

[[_glossary_ns_cluster]]
Cluster::
  a group of tightly coupled <<_glossary_ns_node,nodes>> managed by a <<_glossary_ns_native_scheduler,native scheduler>>.

[[_glossary_ns_node]]
Cluster Node::
  a computer, part of a <<_glossary_ns_cluster,cluster>> used to execute <<_glossary_ns_cluster_job,cluster jobs>>.

[[_glossary_ns_native_scheduler]]
Native Scheduler::
  A software which dispatch <<_glossary_ns_cluster_job,cluster jobs>> on <<_glossary_ns_node,cluster nodes>>, also known as Cluster Manager.

[[_glossary_ns_cluster_job]]
Cluster Job::
  A running command executed on a <<_glossary_ns_cluster,cluster>> by a <<_glossary_ns_native_scheduler,native scheduler>>. Also known as Batch Job.

[[_glossary_ns_cluster_jobid]]
Cluster Job Id::
  Identifier representing a <<_glossary_ns_cluster_job,cluster job>> inside a <<_glossary_ns_native_scheduler,native scheduler>>.

[[_glossary_head_node]]
Head Node::
  A specific <<_glossary_ns_node,cluster node>> where the <<_glossary_ns_native_scheduler,native scheduler>> server runs.

[[_glossary_cluster_user]]
Cluster User::
  A linux operating system account registered on the cluster.

[[_glossary_proactive_scheduler_user]]
ProActive Scheduler User::
  An account registered in the <<_glossary_proactive_scheduler,*ProActive Scheduler*>>, the account is only registered inside the ProActive Scheduler and does not necessarily match an operating system account.

[[_glossary_proactive_scheduler_process_user]]
ProActive Scheduler Process User::
  The operating system account which started the ProActive Scheduler server process.


===== Native Scheduler Node Source Configuration

Using the <<_glossary_rm_web_interface,*Resource Manager Web Interface*>>, you can create a <<_glossary_node_source,*Node Source*>> used to acquire <<_glossary_proactive_node,*ProActive Nodes*>> from a <<_glossary_ns_native_scheduler,*Native Scheduler*>>.

From The drop down menu, select `NativeSchedulerInfrastructure` and `NativeSchedulerPolicy`.

See the reference <<_native_scheduler_infrastructure>> and <<_native_scheduler_policy>> for detailed explanation of the node source parameters.

When the node source is created, it will be activated as other node sources (LocalInfrastructure, SSHInfrastructure, etc), but no ProActive Node will appear.

This is expected as the Native Scheduler node source is *dynamic*, it will only create ProActive Nodes when specific conditions are met.

===== Configure NativeSchedulerSchedulingPolicy

After creating the Native Scheduler Node Source, it is necessary as well to change the <<_scheduling_policies,*ProActive Scheduling Policy*>> to use the `NativeSchedulerSchedulingPolicy`.

This policy ensures that tasks are executed on appropriate ProActive Nodes when using a Native Scheduler Node Source.

In order to do that, edit the file `PROACTIVE_HOME/config/scheduler/settings.ini` and change the following line:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.ExtendedSchedulerPolicy
----

to:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.NativeSchedulerSchedulingPolicy
----

===== Match User Accounts With the Cluster

Submission of jobs to the native scheduler is done by default using a SSH connection.

When a ProActive Task belonging to a given user _Alice_ needs to execute on a NativeScheduler node source, a SSH connection will be performed with the login and password of the _Alice_ user registered in the ProActive Scheduler.

Accordingly, this login/password combination must correspond to a real user on the cluster <<_glossary_head_node,head node>>.

Please refer to the <<_user_authentication,*User Authentication*>> section in order to manage ProActive Users.

===== Execute Tasks on a Native Scheduler Node Source

The link:../user/ProActiveUserGuide.html#_generic_information[Generic Information] `"NS"` allows a ProActive Task to be executed on a Native Scheduler node source.

It must contain the name of the target node source. For example, to submit a Task on the "PBS" Node Source:

```xml
<task name="PBS_Task">
    <description>
        <![CDATA[ Execute this Task in the PBS node source. ]]>
    </description>
    <genericInformation>
        <info name="NS" value="PBS"/>
    </genericInformation>
    <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println "Hello World"
]]>
          </code>
        </script>
    </scriptExecutable>
</task>
```

The NS value can also be defined at the job level, in that case, every task of this job will be executed on the Native Scheduler node source:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="PBS_Job"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2"
>
  <genericInformation>
    <info name="NS" value="PBS"/>
  </genericInformation>
  <taskFlow>
    <task name="Groovy_Task">
      <description>
        <![CDATA[ The simplest task, ran by a groovy engine. ]]>
      </description>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println "Hello World"
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
</job>
```

It is also possible to define in a ProActive Task or Job the `NS_BATCH` generic information. This parameter allows to provide custom arguments to `nsSubmitCommand`.

For example, to submit a task on the PBS Node source, using a specific PBS queue and reserve for this task 2 cluster nodes with 2 cpu each:

```xml
<task name="PBS_Task">
  <description>
    <![CDATA[ Runs on the PBS node source on queue1, using 2 nodes * 2 cpus ]]>
  </description>
  <genericInformation>
    <info name="NS" value="PBS"/>
    <info name="NS_BATCH" value="-q queue1 -lnodes=2:ppn=2"/>
  </genericInformation>
  <scriptExecutable>
    <script>
      <code language="groovy">
         <![CDATA[
println "Hello World"
]]>
      </code>
    </script>
  </scriptExecutable>
</task>
```

Finally, the `NS_PRE` generic information can be used to enter a single line command which will be executed in the cluster just before
the ProActive node starts. It can be used for example to load linux modules required by the task execution.

===== Native Scheduler Node Life Cycle

As soon as tasks containing the `NS` generic information are pending, the target Native Scheduler node source will try to deploy ProActive Nodes to execute them.

The node will first appear in `Deploying` <<_node_states,*state*>>. If some error occurs prior to the `nsSubmitCommand` execution (SSH connection, command syntax), the node state will change to `Lost`, with some explanation about the failure displayed.

If the node remains in `Deploying` state, it is possible to monitor the job execution on the native scheduler itself, by logging into the head node, and use the native scheduler command tools.

Example using the PBS `qstat` command:

----
root@osboxes:/tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPSHOT/logs# qstat

Job ID                    Name             User            Time Use S Queue
------------------------- ---------------- --------------- -------- - -----
241.osboxes                STDIN            osboxes                0 R batch

root@osboxes:/tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPSHOT/logs# qstat -f 241

Job Id: 241.osboxes
    Job_Name = STDIN
    Job_Owner = osboxes@osboxes
    job_state = R
    queue = batch
    server = osboxes
    Checkpoint = u
    ctime = Fri May  4 10:13:06 2018
    Error_Path = osboxes:/tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPSH
	OT/logs/Node-osboxes_852t0.out
    exec_host = osboxes/0
    Hold_Types = n
    Join_Path = oe
    Keep_Files = n
    Mail_Points = a
    mtime = Fri May  4 10:13:06 2018
    Output_Path = osboxes:/tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPS
	HOT/logs/Node-osboxes_852t0.out
    Priority = 0
    qtime = Fri May  4 10:13:06 2018
    Rerunable = True
    Resource_List.walltime = 01:00:00
    Resource_List.nodes = 1
    Resource_List.nodect = 1
    Resource_List.neednodes = 1
    session_id = 6486
    substate = 42
    Variable_List = PBS_O_QUEUE=batch,PBS_O_HOME=/home/osboxes,
	PBS_O_LOGNAME=osboxes,
	PBS_O_PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/b
	in:/usr/games:/usr/local/games,PBS_O_MAIL=/var/mail/osboxes,
	PBS_O_SHELL=/bin/bash,PBS_O_LANG=fr_FR.UTF-8,
	PBS_O_WORKDIR=/home/osboxes,PBS_O_HOST=osboxes,PBS_O_SERVER=osboxes
    euser = osboxes
    egroup = osboxes
    hashname = 241.osboxes
    queue_rank = 237
    queue_type = E
    comment = Job started on Fri May 04 at 10:13
    etime = Fri May  4 10:13:06 2018
    submit_args = -o /tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPSHOT/l
	ogs/Node-osboxes_852t0.out -e /tmp/activeeon_enterprise-node-linux-x64
	-8.1.0-SNAPSHOT/logs/Node-osboxes_852t0.out -j oe
    start_time = Fri May  4 10:13:06 2018
    Walltime.Remaining = 3587
    start_count = 1
    fault_tolerant = False
    job_radix = 0
    submit_host = osboxes
    init_work_dir = /home/osboxes
    request_version = 1
----

When the deployment is successful, the ProActive Node state will change to `Free` and shortly after to `Busy` as soon as the associated task will be deployed on the node.
After the task completes, the node will be removed and the <<_glossary_ns_cluster_job,cluster job>> will be cancelled using the `nsKillCommand`.

===== Troubleshooting

If the <<_glossary_ns_cluster_job,cluster job>> is in _running_ state, but the ProActive Node associated remains in `Deploying` state, it probably means there is a connection issue between the ProActive Node and the Resource Manager.
If the default ProActive Network Protocol is used (`PNP`), it is necessary to have a two way connection between the cluster and ProActive server. You can refer to the <<_available_network_protocols,*network protocols*>> documentation for more info.

To troubleshoot node deployment, you can:

 * inspect the output of the <<_glossary_ns_cluster_job,cluster job>> if provided in the `nsSubmitCommand`
 * add the following log4j loggers to the ProActive Scheduler Server `config/log/server.properties` +
```properties
log4j.logger.org.ow2.proactive.scheduler.policy=DEBUG
log4j.logger.org.ow2.proactive.scheduler.util=DEBUG
log4j.logger.org.ow2.proactive.nativescheduler=DEBUG
```
 * inspect the ProActive Scheduler server logs.

[[_node_source_infrastructures]]
=== Node Source Infrastructures

include::./references/NodeSourceInfrastructureReference.adoc[]

== Control the resource usage

[[_node_source_policies]]
=== Node Source Policies

You can limit the utilization of resources connected to the ProActive Scheduler in different ways. When you create node
sources you can use a *Node Source Policy*. A node source policy is a set of rules and conditions which describes, inside a <<_deploy_nodes_from_proactive_rm,*Node Source*>>, when and how many nodes have to be acquired or released.

When creating a node source, a <<_node_source_infrastructures,*Node Source Infrastructure*>> and a Node Source Policy must be chosen.
Node Sources can be created using the Resource Manager portal or the command line (see <<_command_line_examples>> for more information).

The Node Source Policy interacts internally with the associated <<_node_source_infrastructures,*Node Source Infrastructure*>> to request new nodes deployments or nodes removal.

Node Sources were designed in a way that:

* All logic related to node acquisition is encapsulated in the *Infrastructure Manager*.
* Conditions and rules of node acquisition is described in the *Node Source Policy*.

In the ProActive Resource Manager, there is always a default node source configured with a *DefaultInfrastructureManager* and a *Static policy*.
It is not able to deploy nodes anywhere but makes it possible to add existing nodes to the Scheduler (see <<run_node_manually>>)

Out of the box the Scheduler supports time slot policies, cron policies, load based policies and many others.
Please see detailed information about policies in the following sections.


[[_policy_common_parameters]]
==== Common Parameters

Each node source policy regardless its specifics has a common part where you describe users, groups or tenants permissions.
When you create a policy you must specify them:

* *nodeUsers* - utilization permission that defines who can get nodes for computations from this node source. It has to take one of the following values:
** `ME` - only the node source creator
** `users=user1,user2;groups=group1,group2;tenants=tenant1,tenant2;tokens=t1,t2` - only specific users, groups, tenants or tokens. For example:
*** users=user1 - node access is limited to user1
*** tenants=tenant1 - node access is limited to users belonging to tenant1
*** users=user1;groups=group1 - node access is limited to user1 and all users from group group1
*** users=user1;tokens=t1 - node access is limited to user1 or anyone who specified token t1. If node access is protected by a token, node will not be found by the ProActive Resource Manager when trying to execute workflows, unless the corresponding token is specified inside the workflow(s).
To exclude specific users or groups use for example:
*** users=!user1,!user2;groups=!group1,!group2. Tokens can not be excluded, and mixing inclusion with exclusion is not permitted (i.e. users=user1,!user2;groups=!group1,group2).
** `ALL` - everybody can use nodes from this node source

NOTE: To specify a token inside a chosen workflow, add the key/value pair *NODE_ACCESS_TOKEN:<token>* to its generic information.

* *nodeProviders* - provider permission defines who can add nodes to this node source. It should take one of the following values:
** `ME` - pnly the node source creator
** `users=user1,user2;groups=group1,group2;tenants=tenant1,tenant2` - only specific users, groups or tenants (for our example user1, user2, group1, group2, tenant1 or tenant2). It is also possible to specify only groups, only users or only tenants.
** `ALL` - everybody can add nodes to this node source

The user who created the node source is the administrator of this node source. He can add and removed nodes to it, remove the node source itself,
but cannot use nodes if usage policy is set to *PROVIDER* or *PROVIDER_GROUPS*.

NOTE: Node source policy user restrictions are not enforced for members of the [.underline]#deprecated# `admin` group, they will have no effect on such users (see <<_user_permissions>>).


include::./references/NodeSourcePolicyReference.adoc[]

=== Agents schedule

*Node source policies* limit ProActive Nodes utilization on the level of the ProActive Scheduler.
If you need fine-grained limits on the node level *ProActive Agents* will help you achieve that.

TIP: The typical scenario is when you use desktop workstation for computations during non working hours.

Both linux and windows agents have an ability to:

* Run ProActive Nodes according to the schedule
* Limit resources utilization for these daemons (e.g CPU, memory)

Agents configuration is detailed in the section <<_proactive_agents>>.

[[_nodes_locking]]
=== Locking ProActive Nodes

The Resource Manager allows to lock and unlock ProActive nodes. Locking a Node prevents new Tasks to be launched on
that node. This operation is possible whatever the <<_node_states,state of a Node>> is. Once locked, the Resource Manager keeps track of
who has locked the Node and when.

A common use case for locking Nodes is about maintenances. You may have a long running Task executing on a ProActive
Node where a maintenance is planned. Let's say the current Task must not be interrupted and new ones not started before
achieving the maintenance. A solution is to lock the Node. This way, the current Task will complete but no new Tasks are
scheduled on that Node. Then, it is possible to perform the maintenance and on termination to unlock the Node. Upon
unlocking, the Node becomes again eligible for Tasks execution.

Locking and unlocking a Node, or more generally a set of Nodes, is possible from the REST API, the command line client
but also the Resource Manager portal.

Please note that locks are restored, by default, on Resource Manager restart and thus Scheduler restart.
It is possible to disable this feature by editing the value associated to the property named
`pa.rm.nodes.lock.restoration` in `PROACTIVE_HOME/config/rm/settings.ini`.

When the nodes lock restoration feature is enabled, the Resource Manager will try to lock, per Node Source,
as many Nodes as there were on the previous run.
The approach is best effort and Node hostname is not considered.
As a consequence, Nodes are not necessarily locked on the same host after a Scheduler restart.

=== Undeploying Node Sources

The <<_glossary_resource_manager,Resource Manager>> allows for <<_glossary_node_source,Node Sources>> to be
*undeployed*. In this case, the Node Source is shut down and its <<_glossary_proactive_node,Nodes>> are removed, but
the Node Source definition is kept. An undeployed Node Source can be *redeployed* later, using its initial
configuration.

Node Source undeploying can be *preemptive* or not. If a Node Source undeploying is initiated with a non-preemptive
requirement, then the Nodes of this Node Source which are currently running <<_glossary_task,Tasks>> will not be
removed until their Task is finished. In this case, these Nodes are displayed in the
_to-be-removed_ state. In other terms, if a Node Source is undeployed *non-preemptively*, it will be completely
undeployed as soon as all of its Nodes have finished executing their Task. On the other hand, if a Node Source is
undeployed *with preemption*, its Nodes are immediately removed regardless of whether they execute Tasks, and this may
cause Tasks to fail.

Deploying and undeploying Node Sources is possible from the
<<_glossary_rm_web_interface,Resource Manager Web Interface>>, from the Resource Manager
<<_glossary_rest_api,REST API>>, and from the <<_resource_manager_command_line,Command Line>> client.

TIP: The deployment status of a Node Source is persistent. If the Resource Manager is restarted, then all Node Sources
will be *restored* to their previous state.

== Available Network Protocols

ProActive Workflows and Scheduling offers several protocols for
the Scheduler and nodes to communicate. These protocols provide different
features: _speed_, _security_, _fast error detection_, _firewall_ or
_NAT friendliness_ but none of the protocols can offer all these features at
the same time.
Consequently, the selection should be made carefully. Below are introduced
the available network protocols.
Configuration and properties are discussed in <<_network_properties>>.

=== ProActive Network Protocol

ProActive Network Protocol (PNP) is the general purpose communication protocol
(`pnp://` scheme). Its performances are quite similar to the well known Java RMI
protocol, but it is much more robust and network friendly. It requires only one
TCP port per JVM and no shared registry. Besides, it enables fast network
failure discovery and better scalability.
PNP binds to a given TCP port at startup. All incoming communications use this
TCP port. Deploying the Scheduler or a node with PNP requires to open one and
only one incoming TCP port per machine.

=== ProActive Network Protocol over SSL

ProActive Network Protocol over SSL (PNPS) is the PNP protocol wrapped inside
an SSL tunnel. The URI scheme used for the protocol is `pnps://`. It includes
the same features as PNP plus ciphering and optionally authentication.
Using SSL creates some CPU overhead which implies that PNPS is slower than PNP.

[[_proactive_message_routing]]
=== ProActive Message Routing

ProActive Message Routing (PAMR) allows the deployment of the Scheduler and nodes
behind a firewall. Its associated URI
scheme is `pamr://`. PAMR has the weakest expectations on how the network is
configured. Unlike all the other communication protocols introduced previously,
it has been designed to work when only outgoing TCP connections are available.

=== Installation on a Cluster with Firewall

When incoming connections are not allowed (ports closed, firewalls, etc.), the
ProActive Scheduler allows you to connect nodes without significant changes
in your network firewall configuration. It relies on the PAMR protocol.

This last does *not* expect *bidirectional* TCP connections.
It has been designed to work when only outgoing TCP connections are available.
Such environments can be encountered due to:

  - Network address translation devices
  - Firewalls allowing only outgoing connections (this is the default setup of many firewalls)
  - Virtual Machines with a virtualized network stack

image::firewall.png[]

When PAMR is activated, the ProActive Scheduler and nodes connect to a PAMR router. This connection is kept open, and used
as a tunnel to receive incoming messages. If the tunnel goes down, it is automatically reopened by nodes.

The biggest drawback of PAMR is that a centralized PAMR router is in charge of routing message between all the PAMR clients.
To soften this limitation PAMR can be used with other communication protocols. This way, PAMR is used only when needed.

By default, PNP is enabled. PNP is the default protocol for better performance and nodes can also use PAMR if needed.
The PAMR Router is started by default along with the ProActive Scheduler.

For a ProActive Node to connect to the ProActive Scheduler using PAMR, the following ProActive configuration file can be used
(`PROACTIVE_HOME/config/network/node.ini`). The properties tell the ProActive Node where to find the PAMR router.
The ProActive Node will then connect to `pamr://0` where `0` is the PAMR id of the Scheduler (0 by default).

[source]
----
proactive.communication.protocol=pamr
proactive.pamr.router.address=ROUTER_HOSTNAME
proactive.pamr.router.port=33647
----

This sample configuration requires to open only one port `33647` for *incoming connections* on the router host
and all the ProActive Nodes will be able to connect to the Scheduler.

PAMR communication can be tunneled using SSH for better security. In that case, the ProActive node will establish
a SSH tunnel between it and the ProActive Scheduler and use that tunnel for PAMR traffic.

See <<_pamr_protocol_properties>> reference for a detailed explanation of each property.


== Task Termination Behavior

=== ProActive Node Graceful Task Termination (SIGTERM) at Killing

The task termination timeout is a cleanup timeout for each task (executed on this Node)
after it was killed (through the Scheduler REST API or the Scheduler web portal).
By default the task termination timeout is set to 10 seconds
but it can be setup on each ProActive Node individually.
It can be set at node startup by setting the "proactive.node.task.cleanup.time" property.
Example: add  "-Dproactive.node.task.cleanup.time=30" (set to 30 seconds as example) to
the node startup command.

Find an overview of termination behavior by task and language in section <<Termination behavior by language>>.


==== Step by Step Killing Example

With a 30 seconds termination timeout following steps happen.

===== First

The kill request is received by the Scheduler Server.

===== Second

The ProActive Node which executes the task receives the kill request.
The behavior depends if the Scheduler Server is configured to launch tasks in Forked or Non-Forked Mode.

*Forked Mode*:

On Linux, a SIGTERM signal is send to the JVM which executes the task.
On Windows, the TerminateProcess method of the JVM process which executes the task is executed.
That will finally lead to the Task Process being interrupted by:

* A SIGTERM signal on Linux
* TerminateProcess method called, on Windows

The task can deal with the killing event and start a cleanup procedure. If
the task has started sub-processes it is responsible to initiate graceful termination
of the sub-processes. Handling a graceful termination event looks different in each task type
and is dependent on the capabilities of the language used.
As an example Bash and Jython (Python task) can catch a SIGTERM signal.
Java and Groovy task need to register shutdownHooks to achieve similar behavior.

*Non-Forked Mode*:

In Non-Forked Mode, the task runs in a separate thread in the same JVM as the ProActive Node.
Therefore, the thread running the task will be interrupted.

Bash tasks, in Non-Forked Mode, do receive SIGTERM.
Java and Groovy tasks will receive an InterruptedException, which can be used to handle
the graceful termination of that task and sub-processes.
But, Jython (Python tasks) will not receive any notification.
If the task has started sub-processes it is responsible to initiate graceful termination of the sub-processes,
if possible.

===== Third

The ProActive Node waits as many seconds as setup in the proactive.node.task.cleanup.time property or until
the task is stopped.
Example: cleanup timeout is 30 seconds and the task takes 3 seconds to terminate, then the ProActive Node will
wait 3 seconds only.

===== Fourth

The ProActive Node initiates the task process and sub-process removal.
This will forcefully kill the whole tree of processes. Another SIGTERM
might be send shortly before the forcefully killing (SIGKILL).

===== Additional information

*On Windows* the termination procedure is similar. But the Windows SIGTERM equivalent is
executing the TerminateProcess method of the process. Whereas the SIGKILL is equivalent to
the forceful removal of the running process.


===== Termination behavior by language

.Termination Behavior for a Few Languages
|===
|Language/Execution type|Bash |Python |Java |Groovy

|Forked Mode
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Add a shutdown hook. Task waits cleanup timeout, before being killed.
| add a shutdown hook. Task waits cleanup timeout, before being killed.

|Forked Mode Run As Me
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Add a shutdown hook. Task waits cleanup timeout, before being killed.
| Add a shutdown hook. Task waits cleanup timeout, before being killed.


|Non-Forked Mode
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Terminates immediately.
| Catch an InterruptedException. Task waits cleanup timeout, before being killed.
| Catch an InterruptedException. Task waits cleanup timeout, before being killed.


|===

== Monitor the cluster state

Cluster monitoring typically means checking that all ProActive Nodes that were added to *ProActive Scheduler*
are up and running. We don't track for example the free disk space or software upgrade which can be
better achieved with tools like https://www.nagios.org[Nagios^].

In the *Resource Manager Web Interface* you can see how many ProActive Nodes were added to the
Resource Manager and their usage.

image::admin_web.png[]

The same information is accessible using the command line:

[source]
----
$ PROACTIVE_HOME/bin/proactive-client --listnodes
----

[[_node_states]]
=== ProActive Node States

When you look at your cluster, ProActive Nodes can be in one of the following states:

* `Deploying` - The deployment of the node has been triggered by the ProActive Resource Manager but it has not yet been added.
* `Lost` - The deployment of the node has failed for some reason.
The node has never been added to the ProActive Resource Manager and won't be usable.
* `Configuring` - Node has been added to the ProActive Resource Manager and is being configured.
* `Free` - Node is available for computations.
* `Busy` - Node has been given to user to execute computations.
* `To be removed` - Node is busy but requested to be removed. So it will be removed once the client will release it.
* `Down` - Node is unreachable or down and cannot be used anymore.
* `Needed` - Metric shows how many additional nodes the Scheduler needs to schedule all pending tasks.

The state of a ProActive Node is managed by the Resource Manager. However, each Node has a
<<_nodes_locking,user managed lock status>>.

=== JMX

The JMX interface for remote management and monitoring provides information about the running ProActive Resource Manager and allows the user to modify its configuration.
For more details about JMX concepts, please refer to official documentation about
the https://www.oracle.com/technetwork/java/javase/tech/javamanagement-140525.html[JMX architecture^].

image::jmx_archi.png[align="center"]

The following aspects (or services) of the *ProActive Scheduler* are instrumented using MBeans that are managed through a JMX agent.

* *Server status* is exposed using the RuntimeDataMBean
** The Resource Manager status
** Available/Free/Busy/Down nodes count
** Average activity/inactivity percentage
* The *Accounts Manager* exposes accounting information using the MyAccountMBean and AllAccountsMBean
** The used node time
** The provided node time
** The provided node count
* Various *management operations* are exposed using the ManagementMBean
** Setting the accounts refresh rate
** Refresh all accounts
** Reload the permission policy file

MBean server can be accessed by remote applications using one of the two available connectors

* The standard solution based on Remote Method Invocation (RMI) protocol is the RMI Connector accessible at the following URL:
`service:jmx:rmi:///jndi/rmi://HOSTNAME:PORT/JMXRMAgent` where
** *HOSTNAME* is the hostname on which the Resource Manager is started
** *PORT* (5822 by default) is the port number on which the JMX RMI connector server has been started. It is defined by the property pa.rm.jmx.port .
* The ProActive Remote Objects Connector provides ProActive protocol aware connector accessible at the following URL:
`service:jmx:ro:///jndi/PA_PROTOCOL://HOSTNAME:PORT/JMXRMAgent` where
** *PA_PROTOCOL* is the protocol defined by the proactive.communication.protocol property
** *HOSTNAME* is the hostname on which the Resource Manager is started
** *PORT* is the protocol dependent port number usually defined by the property proactive.PA_PROTOCOL.port

The name of the connector (JMXRMAgent by default) is defined by the property `rm.jmx.connectorname`.

The JMX URL to connect to can be obtained from the Authentication API of the Resource Manager or by reading the log file located in `PROACTIVE_HOME/logs/RM.log`.
In that log file, the address you have to retrieve is the one where the JMX RMI connector server has been started

[source]
----
[INFO 2010-06-17 10:23:27,813] [RM.AbstractJMXHelper.boot] Started JMX RMI connector server at service:jmx:rmi:///jndi/rmi://kisscool.inria.fr:5822/JMXRMAgent
----

Once connected, you'll get an access to Resource Manager statistics and accounting.

For example, to connect to the ProActive Scheduler JMX Agent with JConsole tool, just enter the URL of the standard RMI
Connector, as well as the username and the password.

image::jmx_jconsole_connect.png[align="center"]

Then depending on the allowed permissions browse the attributes of the MBeans.

image::jmx_jconsole.png[align="center"]

=== Accounting

The users of ProActive Scheduler request and offer nodes for computation. To keep track of how much node time was
consumed or contributed by a particular user, ProActive Scheduler associates a user to an account.

More precisely, the nodes can be manipulated by the following basic operations available to the users

* The `ADD` operation is a registration of a node in the Resource Manager initiated by a user considered as a node provider.
A node can be added, through the API, as a result of a deployment process, through an agent or manually from the command line interface.
* The `REMOVE` operation is the unregistration of a node from the Resource Manager.
A node can be removed, through the API, by a user or automatically if it is unreachable by the Resource Manager.
* The `GET` operation is a node reservation, for an unknown amount of time, by a user considered as a node owner.
For example, the ProActive Scheduler can be considered as a user that reserves a node for a task computation.
* The `RELEASE` operation on a reserved node by any user.

The following accounting data is gathered by the Resource Manager

* *The used node time*: The amount of time other users have spent using the resources of a particular user.
More precisely, for a specific node owner, it is the sum of all time intervals from `GET` to `RELEASE`.
* *The provided node time*: The amount of time a user has offered resources to the Resource Manager.
More precisely, for a specific node provider, it is the sum of all time intervals from `ADD` to `REMOVE`.
* *The provided node count*: The number of provided nodes.

The accounting information can be accessed only through a *JMX client* or the ProActive Resource Manager *command line*.

=== Metric to monitor Scheduler load

*Needed Nodes* metric shows how many additional nodes the Scheduler needs to schedule all pending tasks,
e.g. there are 4 nodes and all of them are busy, but *Needed Nodes* shows 10, which means if you would have
14 nodes instead of 4, all of these nodes would be busy.
Please notice, that *Needed Nodes* shows how many more nodes could be used right now.
However, it does not give any indication about the kind of nodes needed,
e.g. Selection Script of the task could require some specific nodes,
thus *Needed Nodes* could be positive despite still having free nodes
(they can be inappropriate for the current task).


You can find this metric in multiple places. In RM portal you can see it in Nodes History:

image::neededNodes-rm-nodeshistory.png[align="center"]

In Nodes State:

image::neededNodes-rm-nodesstate.png[align="center"]

And you can find it in RM portal detailed statistics:

image::neededNodes-rm-stats.png[align="center"]

Also, you can find *Needed Nodes* in the Scheduler portal where the Scheduler status is displayed:

image::neededNodes-scheduler.png[align="center"]

[[_run_as_me_admin]]
== Run Computation with a user's system account

ProActive workflow Tasks can be configured to run under a user's system account by ticking the
link:../user/ProActiveUserGuide.html#_run_as_me[Run as me] checkbox in the task configuration.

image::../images/StudioRunAsMe.png["RunAsMe in Studio",width=300]

The RunAsMe mode can also be configured globally on the ProActive server. In that case, all workflow tasks from all users will be executed in RunAsMe mode.
Global RunAsMe can be configured in file `PROACTIVE_HOME/config/scheduler/settings.ini`, by enabling the following property:

[source, properties]
----
# If true tasks are always ran in RunAsMe mode (impersonation). This automatically implies pa.scheduler.task.fork=true (other setting is ignored)
pa.scheduler.task.runasme=true
----

By default, authentication is done through a <<_using_password,password>>, but can also be configured to use a <<_using_ssh_keys,SSH key>> or <<_using_password_less_sudo,password-less sudo>>.

The impersonation mode is configurable for each ProActive Node, using the java property `pas.launcher.forkas.method`.

For example:

[source, bash]
----
./bin/proactive-node -Dpas.launcher.forkas.method=pwd # start a node using password impersonation
./bin/proactive-node -Dpas.launcher.forkas.method=key # start a node using SSH key impersonation
./bin/proactive-node -Dpas.launcher.forkas.method=none # start a node using password-less sudo impersonation
----

This property can also be specified in ResourceManager <<_node_source_infrastructures>> through parameters that allow adding extra arguments to the _java virtual machine_.

Find a step by step tutorial link:AdminTutorials.html[here].

=== System Configuration

For proper execution, *the user's system account must have*:

* Execution rights to the PROACTIVE_HOME directory and all it's parent directories.
* Write access to the PROACTIVE_HOME directory.

Logs are written during task execution, as a matter of fact, the executing user needs write access to the
PROACTIVE_HOME directory. ProActive does some tests before executing as a different user, those need full execution
rights, including the PROACTIVE_HOME parent directories.

TIP: Create a ProActive group which owns the PROACTIVE_HOME directory and has write access. Add every system user, which
executes tasks under its own system account, to the ProActive group.

NOTE: On Mac OS X, the default temporary folder is not shared between all users. It is required by the RunAsMe feature.
To use a shared temporary folder, you need to set the _$TMPDIR_ environment variable and the Java property _java.io.tmpdir_
to /tmp before starting the ProActive Node.

NOTE: the same applies on Windows. To use a shared temporary folder, create a folder shared by all ProActive users e.g. C:\TEMP and define
the Java property _java.io.tmpdir_ to C:\TEMP before starting the ProActive Node.

Example:
----
proactive-node -Djava.io.tmpdir=C:\TEMP
----

Extra configurations are also needed depending on the authentication method below.

=== Using password

The ProActive Node will try to impersonate the user that submitted the task when running it. It means the
username and password must be the same between the ProActive Scheduler and the operating system.

=== Using SSH keys

To enable this authentication method, set the java system property `-Dpas.launcher.forkas.method=key` when starting a ProActive Node.

Example:
----
proactive-node -Dpas.launcher.forkas.method=key
----

A SSH key can be tied to the user's account and used to impersonate the user when running a task on a given machine (using SSH).
Additionally:

 - The `.ssh/authorized_keys` files of all machines must be configured to accept this SSH key.
 - The SSH key must not contain a *passphrase*.
 - Using this method on Windows machines is not recommended, as it requires the installation and configuration of a SSH Server.

When login into the scheduler portal, the private key of the user must be provided, this can be done by selecting on the login dialog: `More options > Use SSH private key`.

Alternatively, a user can also add its SSH private key to <<../user/ProActiveUserGuide.adoc#_third_party_credentials,Third Party Credentials>> under the name `SSH_PRIVATE_KEY`.
This method allows the user to login normally to the ProActive portals, without the need to enter its private key each time.

Finally, users can also authenticate to the portals with credential files.
The user must first create a credential file containing the user login, password and SSH key.

Run the following command on the ProActive server to create a credential file:

----
$ <PROACTIVE_HOME>/tools/proactive-create-cred -F <PROACTIVE_HOME>/config/authentication/keys/pub.key -l username -p userpwd -k path/to/private/sshkey -o myCredentials.cred
----

This command will create an encrypted file with *username* as login, *userpwd* as password, using Scheduler public key at
`config/authentication/keys/pub.key`
for credentials encryption and using the user private SSH key at
`path/to/private/sshkey`. The new credential will be stored in *myCredentials.cred*.

Once created, the user must connect to the ProActive portals using this credential file. For example, in the studio portal below:

image::../images/StudioCredentialsLogin.png["Studio login with Credentials", ,width=300]

=== Using password-less sudo

This configuration, only available on linux or unix Nodes, allows the impersonation to be performed using password-less sudo.

To enable it at the system level, edit the `/etc/sudoers` file to allow password-less sudo from the account running the ProActive node
to any users which require impersonation. Password-less sudo should be enabled for any command.

For example, the following line will allow the proactive account to impersonate to any user:
----
proactive ALL=(ALL) NOPASSWD: ALL
----

To enable this configuration on the ProActive node, start it with the system property `-Dpas.launcher.forkas.method=none`

Example:
----
proactive-node -Dpas.launcher.forkas.method=none
----

[[_web_applications]]
== Configure Web applications

The ProActive Scheduler deploys automatically several web applications in an
embedded Jetty web server. The binaries and sources associated to the web
applications can be found in `PROACTIVE_HOME/dist/war`.

TIP: To configure Web applications, for instance the HTTP port to use, edit the `PROACTIVE_HOME/config/web/settings.ini` file.

Web applications are deployed on the host using the HTTP port 8080 by default. The local part of the URL is based on the WAR files found in `PROACTIVE_HOME/dist/war`.
For instance, the Scheduler web portal is available at `\http://localhost:8080/scheduler`.

=== Enable HTTPS

ProActive Workflows and Scheduling provides support for HTTPS. Similarly to
other web settings, HTTPS related properties are defined in
`PROACTIVE_HOME/config/web/settings.ini` and any update would require a restart
of the Scheduler instance.

Enabling HTTPS requires to set `web.https` property to `true` but also to define a path
to a valid link:https://en.wikipedia.org/wiki/Keystore[keystore] through
`web.https.keystore` along with its associated password
by using `web.https.keystore.password`. A default keystore is provided for testing purposes, however it must not be used in production.

When you are using the default keystore or a custom keystore with a self-signed
certificate you need to enable `web.https.allow_any_certificate` and optionally
`web.https.allow_any_hostname` if your certificate Common Name (CN) does not
match the fully qualified host name used for the machine hosting the Web
applications.

Port 8443 is used for listening to HTTPS connections.
This port number prevents to have root access for deploying web applications.
However, you can change it by editing `web.https.port` value.

When HTTPS is set up, you can automatically redirect any HTTP request from
users to the secured version of the protocol to make sure exchanged information
is protected. It requires to enable `web.redirect_http_to_https`.

Additionally, you will need to update the port number used in the property `jp.url`.

Likewise, it is necessary to manually update the port number and enable HTTPS in the configuration and properties files of the web applications, as follows (example using default port 8443):

* `PROACTIVE_HOME/dist/war/job-planner/WEB-INF/classes/application.properties`
....
pa.scheduler.rest.url=https://localhost:8443/rest
pa.catalog.url=https://localhost:8443/catalog
pa.pca.service.base.url=https://localhost:8443/cloud-automation-service
pa.scheduling.api.url=https://localhost:8443/scheduling-api/v1/graphql
....

* `PROACTIVE_HOME/dist/war/catalog/WEB-INF/classes/application.properties`
....
pa.scheduler.url=https://localhost:8443
....

* `PROACTIVE_HOME/dist/war/cloud-automation-service/WEB-INF/classes/application.properties`
....
pa.catalog.url=https://localhost:8443/catalog
pa.scheduler.rest.url=https://localhost:8443/rest/scheduler
pa.resource_manager.rest.url=https://localhost:8443/rest/rm
pa.cloud_automation_service.url=https://localhost:8443/cloud-automation-service
server.port=8443
pa.web.https.allow_any_certificate=true
pa.web.https.allow_any_hostname=true
....

* `PROACTIVE_HOME/dist/war/notification-service/WEB-INF/classes/application.properties`
....
scheduling.url=https://localhost:8443
....

* `PROACTIVE_HOME/dist/war/proactive-cloud-watch/WEB-INF/classes/application.properties`
....
pca.rm.url=https://localhost:8443/rest/rm
pca.service.url=https://localhost:8443/cloud-automation-service
pa.catalog.url=https://localhost:8443/catalog
pa.scheduler.rest.url=https://localhost:8443/rest
....

* `PROACTIVE_HOME/dist/war/scheduling-api/WEB-INF/classes/application.properties`
....
pa.scheduler.rest.url=https://localhost:8443/rest
....

* `PROACTIVE_HOME/dist/war/rm/rm.conf`
....
rm.rest.url=https://localhost:8443/rest
web.https.allow_any_hostname=true
web.https.allow_any_certificate=true
....

* `PROACTIVE_HOME/dist/war/scheduler/scheduler.conf`
....
jobplanner.rest.url=https://localhost:8443/job-planner/planned_jobs
pa.scheduling.api=https://localhost:8443/scheduling-api
sched.catalog.url=https://localhost:8443/catalog
sched.rest.url=https://localhost:8443/rest
web.https.allow_any_hostname=true
web.https.allow_any_certificate=true
....

NOTE: If you are familiar with Nginx or a similar web server, you can also use
it to enable HTTPS connection to Web applications. It has the advantage to make
it possible to have ProActive web applications and additional ones to coexist.

==== Creating a valid keystore

Java is bundled with some utility binaries such as
link:https://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html[keytool].
This last allows to create and manage a keystore but also to generate keys and
certificates.

===== With a self-signed certificate

Generating a new self-signed certificate imported in a new keystore with a
custom password is as simple as executing the following command:

[source]
----
$JAVA_HOME/bin/keytool -keystore keystore -alias jetty -genkey -keyalg RSA -validity 365
----

This command prompts for information about the certificate and for a password to
protect both the keystore and the keys within it. The only mandatory responses
are to provide a password and the Fully Qualified Domain Name of the server. Once
done, the value for properties `web.https.keystore` and
`web.https.keystore.password` must be adapted based on the information you have
entered.

For more information, please look at the
link:https://wiki.eclipse.org/Jetty/Howto/Configure_SSL[Jetty documentation].

===== With a trusted certificate

The first step is to obtain a trusted certificate that is valid for your Fully
Qualified Domain Name (FQDN). This process differs from a Certification
Authority (CA) to another. In the following, it is assumed that
link:https://letsencrypt.org[Let's Encrypt] is used. By way of illustration,
Digital Ocean provides examples for link:https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-centos-7[CentOS] and
link:https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-14-04[Ubuntu].

Assumming your FQDN is _example.org_, then after obtaining a certificate you will
get the following PEM-encoded files in `/etc/letsencrypt/live/example.org`:

  - cert.pem: Your domain's certificate
  - chain.pem: The Let's Encrypt chain certificate
  - fullchain.pem: cert.pem and chain.pem combined
  - privkey.pem: Your certificate's private key

Before loading the key and the certificate into a new keystore that is
recognized by ProActive Workflows and Scheduling, you need to combine them into
a PKCS12 format file. You can achieve this action by means of the following
OpenSSL command:

[source]
----
openssl pkcs12 \
    -inkey /etc/letsencrypt/live/example.org/privkey.pem \
    -in /etc/letsencrypt/live/example.org/cert.pem \
    -export -out jetty.pkcs12
----

Then, you can load the resulting PKCS12 file into a JSSE keystore with keytool:

[source]
----
keytool -importkeystore -srckeystore jetty.pkcs12 \
    -srcstoretype PKCS12 -destkeystore keystore
----

Both commands prompt for a password. You need to use the same and set it as a
value to property `web.https.keystore.password`. The resulting _keystore_ file
corresponds to the file whose path must be used for property
`web.https.keystore`.

IMPORTANT: link:https://bugs.openjdk.java.net/browse/JDK-8154757[Starting from update 101],
Java 8 trusts Lets Encrypt certificates. However, if you are using a less recent
version of Java for running JVMs related to ProActive Workflows and Scheduling,
you will need to update the truststore of your Java installation.

==== Updating Java truststore for accepting Lets Encrypt certificates

[source]
----
sudo keytool -trustcacerts \
    -keystore $JAVA_HOME/jre/lib/security/cacerts \
    -storepass changeit \
    -noprompt \
    -importcert \
    -file /etc/letsencrypt/live/example.org/chain.pem
----

=== Enable VNC remote visualization in the browser

The REST API web application embeds a proxy to allow the remote display visualization of a ProActive Node running a given task
via a VNC server from the browser. To enable this feature, the scheduler has to be configured as follows:

* configure the proxy: edit the `PROACTIVE_HOME/config/web/settings.ini` file and set `novnc.enabled` to true in order
to start the proxy.
* configure the scheduler portal to use the proxy when opening a new tab browser that shows the remote visualization:
edit the `PROACTIVE_HOME/dist/war/scheduler/scheduler.conf`.
** Set `sched.novnc.url` to the public address of the proxy. This public address is the public address of the host where the sheduler is started and the port specified by the option `novnc.port` in the file `PROACTIVE_HOME/config/web/settings.ini`
** Set `sched.novnc.page.url` to the public address of the VNC client to be executed in the browser. The client is located
in the REST API with the file `novnc.html`.

For more information, see https://github.com/ow2-proactive/scheduling/blob/master/rest/README-novnc.txt

[[_catalog]]
=== Catalog

include::./Catalog.adoc[]

=== Notification service
include::./references/NotificationServiceReference.adoc[]

[[_script_engines]]
== Configure script engines

Most script engines do not need any configuration. This section talks about the script engines which can be configured
individually.

=== Docker Compose (Docker task)

The Docker Compose script engine is a wrapper around the Docker Compose command. It needs to have Docker Compose
 and Docker installed, in order to work. The Docker Compose script engine has a configuration file which configures each ProActive Node individually.
The configuration file is in `PROACTIVE_HOME/config/scriptengines/docker-compose.properties`. +
`docker-compose.properties` has the following configuration options:

---
docker.compose.command=[docker-compose executable example:/usr/local/bin/docker-compose]
---
Defines which Docker Compose executable is used by the script engine.

---
docker.compose.sudo.command=[sudo executable example:/usr/bin/sudo]
---
Defines the sudo executable which is used. The sudo property is used to give the Docker Compose command root rights.

---
docker.compose.use.sudo=false
---
Defines whether to execute Docker Compose with sudo [true] or without sudo [false]

---
docker.host=
---
Defines the DOCKER_HOST environment variable. The DOCKER_HOST variable defines the Docker socket which the Docker client connects to.
That property is useful when accessing a Docker daemon which runs inside a container or on a remote machine.
Further information about the DOCKER_HOST property can be found https://docs.docker.com/engine/reference/commandline/cli/[in the official Docker documentation^].

=== Perl scripts execution (Perl task)
Perl should be installed on your machine. By default the perl engine is installed in Linux and Mac Os. Please install the perl engine on Windows. +
The execution mechanism of perl tasks is based on running perl files on your machine.

If you encountered the issues, please check if you have installed Perl properly on you machine.
In order to verify this you can follow the next steps:

* For example you should to be able to launch the command 'perl -V'.
* Then from command line you should be able to launch a simple perl file (perl yourPerlFile.pl).

=== Python Script Engine (Python task)
Python should be installed on your machine. By default Python2 is installed on Linux and Mac OS. If another version of Python is required,
it should be installed on your machine in advance.

More over, the native Python Script Engine depends on the py4j library in order to communicate with our Java classes. Please follow the following step to install it:

* With Python2: `pip install py4j`

* With Python3: `pip3 install py4j`

Further information can be found https://www.py4j.org/install.html[in the official py4j documentation^].

[[_scheduling_policies]]
== Extending ProActive Scheduling Policy

In order to decide which pending Tasks are executed, the ProActive Scheduler uses a <<_glossary_scheduling_policy,*Scheduling Policy*>>.

This policy allows, for example, to execute task in a First-In-First-Out order or according to priorities.

The default Scheduling Policy can be extended to perfectly fit with an organization scheduling constraints.

Moreover, policy can be changed dynamically throught Java API `org.ow2.proactive.scheduler.common.Scheduler::changePolicy(String policyClassName)`.

In the following example, we show how a different policy can be used to control Task Scheduling.


=== Earliest deadline first (EDF) policy

To use the earliest deadline first (EDF) policy,  edit the file `PROACTIVE_HOME/config/scheduler/settings.ini` and change the following line:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.ExtendedSchedulerPolicy
----

to:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.edf.EDFPolicyExtended
----


=== License Policy Example

To use the LicenseSchedulingPolicy, edit the file `PROACTIVE_HOME/config/scheduler/settings.ini` and change the following line:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.ExtendedSchedulerPolicy
----

to:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.license.LicenseSchedulingPolicy
----

The LicenseSchedulingPolicy requires an extra configuration: the maximum number of licenses per software must be specified into
`PROACTIVE_HOME/config/scheduler/license.properties` like this

----
software_A = 3
software_B = 2
----

From now, at a job or a task level, user can specify which software licenses are required, by adding the REQUIRED_LICENSES generic information,
which value can be

----
softwareA,softwareB
----

As expected, if 10 tasks requiring software_A,software_B licenses are submitted, they will be executed 2 by 2, according to the software with the fewest licenses.
The same logic is applied at the job level, whatever the number of tasks composing each job, since all tasks of the same job share the same license.
Moreover, user can combine both levels.

== Addons

[[_get_notifications]]
=== Get Notification on Job Events Configuration

In order to enable this functionality the following properties should be set in `config/scheduler/setting.ini`:

[source, groovy]
----
pa.scheduler.notifications.email.enabled = true
pa.scheduler.notifications.email.from = username@example.com
----

To configure the email sender on Scheduler, the configuration file `config/scheduler/emailnotification.properties` should be filled in, please refer to
link:../admin/ProActiveAdminGuide.html#_smtp_configuration_examples[SMTP configuration examples] for examples.

=== Email notification

The email notification addons includes an utility Java class but also a Groovy
task (accessible from the Studio Web app) that allow to send an email
from a ProActive Workflow.

The task assumes that configuration for connecting to an SMTP server is done using
third-party credentials. It requires to define some values as key/value pairs in the third-party
credentials associated to the user that runs the task.

This configuration can be achieved from the Scheduler Web portal, the
ProActive client or even through the REST API. Please see
link:../user/ProActiveUserGuide.html#_third_party_credentials[Third-party credentials] for more information.

==== SMTP configuration examples

===== Free

[width="60%",frame="topbot",options="header,footer"]
|======================
| Key | Value
| mail.smtp.host | smtp.free.fr
| mail.smtp.username | user@free.fr
| mail.smtp.password | user_password
|======================

===== Gmail

Password authentication to Google servers requires extra configuration.

See https://www.google.com/settings/security/lesssecureapps.

[width="60%",frame="topbot",options="header,footer"]
|======================
| Key | Value
| mail.smtp.host            | smtp.gmail.com
| mail.smtp.starttls.enable | true
| mail.smtp.ssl.trust       | smtp.gmail.com
| mail.smtp.username        | user@gmail.com
| mail.smtp.password        | user_password
|======================

===== Outlook

Password authentication to Microsoft servers requires extra configuration.

[width="60%",frame="topbot",options="header,footer"]
|======================
| Key | Value
| mail.smtp.host            | smtp-mail.outlook.com
| mail.smtp.starttls.enable | true
| mail.smtp.ssl.trust       | smtp-mail.outlook.com
| mail.smtp.username        | user@gmail.com
| mail.smtp.password        | user_password
|======================

If the above settings do not succeed, check in the scheduler logs any errors related to email sending.

=== Statistics on ProActive Jobs and Resource Usage

include::./references/StatisticsOnProActiveJobsAndResourceUsageReference.adoc[]

[#_servicenow_integration]
=== ServiceNow integration

include::./references/ServiceNowReference.adoc[]

== Scheduler start tuning

Scheduler start can be customized by automatically triggering user's scripts.
A list of scripts can be specified in `PROACTIVE_HOME/config/scheduler/settings.ini`
following this

`pa.scheduler.startscripts.paths=/your/script1/path;/your/script2/path;/your/script3/path`

For example, load-examples.groovy which is executed by default at scheduler start, has in charge
the full deployment of proactive-examples by: populating the running Catalog, exposing as templates
specific workflows and copying workflows dependencies into dataspaces.

== Add custom third-party libraries to ProActive server and Nodes classpath

Some workflows may need to use third-party libraries, so they can be executed properly. In order to do that, the first option is to add those libraries to the global classpath of ProActive server or Nodes.
Depending on how Nodes are started, libraries can be added in the ProActive server or the Nodes classpath in three ways:

- If Nodes are running on the same host as ProActive server or on a different host with a shared Proactive installation, the libraries can be placed under the addons directory of the server (i.e., $PROACTIVE_HOME/addons);
- If Nodes are running on different hosts than ProActive Server and do not share a common proactive installation (such as NFS), the libraries can be placed under the addons directory of the Node (i.e., $PROACTIVE_NODE/addons);
- If Nodes are running on different hosts than ProActive Server, and such Nodes are started using the executable jar 'node.jar' (usually used for cloud infrastructure), the libraries can be placed inside the lib folder of the node.jar archive itself, which is located on the server installation under $PROACTIVE_HOME/dist/war/rest (the node.jar is downloaded dynamically by the node).

The second option is to add those libraries dynamically for a given task execution. Please refer to the User guide documentation to learn how to add link:../user/ProActiveUserGuide.html#_fork_environment[additional classpath].


== Performance tuning

[[_perf_tuning_db]]
=== Database

By default, ProActive Scheduler is configured to use http://hsqldb.org[HSQLDB]
embedded database. This last is lightweight and does not require complex
configuration. However, it cannot compete with more conventional database
management systems, especially when the load increases. Consequently, it is
recommended to setup an RDBMS such as https://mariadb.com[MariaDB],
https://www.postgresql.org/[Postgres] or
https://www.microsoft.com/en-us/sql-server/[SQL Server] if
you care about performance or notice a slowdown.

ProActive Workflows and Scheduling distribution is provided with several
samples for configuring the Scheduler and Resource Manager to use most
standard RDBMS.
You can find these examples in `PROACTIVE_HOME/config/rm/templates/` and
`PROACTIVE_HOME/config/scheduler/templates/` folders.

==== Indexes

The Scheduler and RM databases rely on Hibernate to create and update the
database schema automatically on startup. Hibernate is configured for adding
indexes on frequently used columns, including columns associated to foreign
keys. The goal is to provide a default configuration that behaves in a
satisfactory manner with the different features available in the Scheduler.
For instance, the Jobs housekeeping feature requires indexes on most foreign
keys since a Job deletion may imply several cascade delete.

Unfortunately, each database vendor implements its own strategy regarding
indexes on foreign keys.  Thus, some databases such as _MariaDB_ and _MySQL_
automatically add indexes for foreign keys whereas some others like _Postgres_
or _Oracle_ do not add such indexes by default. Besides, Hibernate does not
offer control over this strategy.
As a consequence, this difference of behaviour, once linked to our default
configuration for Hibernate, may lead to duplicate indexes (two indexes with
different names for a same column) that are created for columns in RM and
Scheduler tables.

In case you are using in production _MySQL_, _MariaDB_ or a similar database
that automatically adds indexes on foreign keys, it is strongly recommended to
ask your DBA to identify and delete duplicate indexes created by the database
system since they may hurt performances.

==== Database impact on job submission performance

The choice of the database provider can influence the Scheduler and Resource Manager performance overall.
In the following figure, we see the job submission time between different database providers:

image::job-submission-time-comparison.png[align="center"]

On a 1000-jobs submission basis across ProActive's embedded database, the in-memory database and a standalone PostgreSQL server,
we can see that the in-memory database is in fact faster and more consistent in speed than the default embedded database.
However the in-memory database has no persistence of the data on disk, as opposed to the embedded database.
A standalone database is a good tradeoff between speed and data resilience.

The embedded database is used by default in ProActive. To switch to an in-memory database, you need to modify the following files:

`PROACTIVE_HOME/config/scheduler/database.properties`:

Replace the default value of `hibernate.connection.url` to:
```
hibernate.connection.url=jdbc:hsqldb:mem:scheduler;hsqldb.tx=mvcc;hsqldb.lob_file_scale=1
```

`PROACTIVE_HOME/config/rm/database.properties`:

Replace the default value of `hibernate.connection.url` to:
```
hibernate.connection.url=jdbc:hsqldb:mem:rm;hsqldb.tx=mvcc;hsqldb.lob_file_scale=1
```

To switch to a standalone database, see <<_perf_tuning_db>>.

==== Database impact on task creation performance

The choice of the database provider influences task creation time.
In the following figure, we see the task creation time for different database providers:

image::grouped-bar-direct-labels.png[align="center"]

Each experiment, consists of submitting one replicated job with some number of replicated tasks, and measuring
time to create all these replicated tasks.

//MariaDB
//10K - 95s
//20K - 296s
//Default
//10K - 750s
//20K - 1750s
//In-memory
//10K - 85s
//20K - 238s

=== Tuning Linux for 15K nodes

It is necessary to tune underlying Linux system, in case there will be plenty of nodes in the Resource Manager.

It is mandatory to allow having a large number of running processes and open files.

For example, on 15.000 nodes, the recommended ulimit settings for running processes and open files are:

```
nproc soft/hard 100000
nofile soft/hard 65536
```

In addition, we recommend to have at least 16GB RAM for 15.000 nodes.

=== Housekeeping

The Scheduler provides a housekeeping mechanism that periodically removes finished jobs
from the Scheduler and Workflow Execution portals. It also removes associated logs and all job data from the database to save space.
This mechanism has two phases:

. Once a job is finished, it sets its scheduled time for removal (the job expiration date)
. Actual cleaning of expired jobs from the scheduler and/or the database will be periodically triggered and performed in a bulk operation

You can configure housekeeping with the following parameters located in <<Scheduler Properties,the scheduler config file>>:

* `pa.scheduler.core.automaticremovejobdelay` : delay in seconds to remove a terminated job.
* `pa.scheduler.core.automaticremove.errorjob.delay` : delay in seconds to remove a job terminated with errors.
* `pa.scheduler.core.removejobdelay` : instead of using job termination time to initiate the delay, use the result retrieval time (for example, when a user opens the job results in the scheduler portal).

TIP: Additionally, you can configure a remove delay for individual job execution using the <<../user/ProActiveUserGuide.adoc#_remove_delay,`REMOVE_DELAY`>> or <<../user/ProActiveUserGuide.adoc#_remove_delay_on_error,`REMOVE_DELAY_ON_ERROR`>> generic information.

If the `pa.scheduler.core.automaticremovejobdelay` setting is set to `0` (the default), the Housekeeping mechanism is *disabled* (no occurrence of housekeeping).

If the `pa.scheduler.core.automaticremove.errorjob.delay` setting is set to `0` (the default), removal of finished jobs containing errors uses the `pa.scheduler.core.automaticremovejobdelay` configuration.

The `errorjob.delay` setting is generally used in production environment for audit purpose. It allows keeping jobs containing errors longer in the database.

IMPORTANT: Please note that the housekeeping mechanism is not applied to the jobs
which were already in the database before housekeeping was switched on.
Similarly, a modification of the `automaticremovejobdelay`, `automaticremove.errorjob.delay` or `removejobdelay` settings will not be retroactively applied to already finished jobs.

The housekeeping mechanism is periodically triggered through the cron
expression `pa.scheduler.core.automaticremovejobcronexpression`.
As the boolean `pa.scheduler.job.removeFromDataBase` property is set to `true` by default,
a query is executed in the scheduler database to remove all jobs that qualify for the bulk removal.

IMPORTANT: The support of `pa.scheduler.job.removeFromDataBase=false` has been discontinued in ProActive Scheduler version `10.1.0`.


== Nodes and Task Recovery

The Scheduler can be started in *recovery mode*. If so, the Scheduler will try to restore Resource Manager nodes and
ProActive tasks that were running in the previous execution of the Scheduler. The nodes
and task recovery feature makes sure you do not lose on-going computations if the Scheduler experiences a failure.

=== Use Case and Scope of the Feature

Suppose that at an instant T the Scheduler is running tasks. Without the nodes and task
recovery feature, if the Scheduler crashed at instant T, then the tasks that were on-going would be
*re-scheduled* on any node and *re-executed* from the beginning when the Scheduler restarts. With the nodes and task
recovery feature enabled, when the Scheduler restarts it will retrieve the *previous* RM nodes and the *on-going* tasks,
and the execution of the tasks will continue and finish as if there was no failure of the Scheduler.

WARNING: The nodes and task recovery feature is *not* applicable when the Scheduler is cleanly stopped or exits normally.
This is a mechanism that is *only* applicable upon *failure* of the Scheduler (machine crash, abrupt exit).

=== How Does Nodes and Task Recovery Work?

At startup, the Scheduler checks the nodes and the tasks that were previously persisted in database to be able to restore
their state. Here are the different cases that can occur at recovery time:

* If a node that is found in database is still *alive*, the node's information is restored in the Resource Manager. The
same restoration mechanism is applied for running tasks in the Scheduler.

* If however, no node is found in database for a given *node source*, then the node recovery will
not take place for this node source. Instead, the nodes will be re-deployed like for the first time.

* If a node that is found in database is *not alive* at recovery time, then the node will also be recreated automatically.

*A recovery is not a restart*, and as such a clean restart of the Scheduler will not preserve on-going tasks and
running nodes. Indeed, in the case of a *regular shutdown* of the Scheduler, the nodes are *cleaned and removed* from the
Resource Manager before the Scheduler exits. Thus, there will be no nodes and no running tasks to recover when the
Scheduler is started again.

To handle the particular situation in which the Scheduler is down when a running task terminates, a proper
*configuration* of the Scheduler allows the task to hold the result until the Scheduler is up and running again.

=== Nodes and Task Recovery Configuration

==== Restrictions

The nodes recovery is not available for Scheduler settings that use the PAMR protocol. This is because this
communication protocol relies on router endpoint identifiers that are regenerated when connections are reestablished.

The nodes recovery is also not available on the Nodes that are launched at the Scheduler startup by default. However
you get the choice to activate or deactivate the feature when you create your own Node Source. By default, the nodes
recovery is activated for any new Node Source.

==== How to make sure that nodes and task recovery is enabled

WARNING: The nodes and task recovery feature requires a proper configuration to ensure that nodes and tasks are *kept
alive* during the down time of the Scheduler.

* *Resource Manager* configuration (`PROACTIVE_HOME/config/rm/settings.ini`)
+
In order to enable the nodes and task recovery feature, make sure that the `pa.rm.nodes.recovery` property of the
Resource Manager is set to `true`. This property is set to `true` by default.
+

* *Node* configuration
+
The ProActive nodes should be started with a configuration that keeps them alive if the Scheduler fails. Node properties
can be set at node startup:

** Use `-Dproactive.node.ping.delay` to specify how often the node will try to communicate with the Resource Manager.
The default for this property is 30 seconds.

** Use `-Dproactive.node.reconnection.attempts` to specify how many times the node will try to reconnect with the
Resource Manager. The default for this property is to attempt 10 times.

+
These two properties define the total delay for the node to reconnect to the Resource Manager. By default, the total
delay of reconnection is 5 minutes. When this total delay is consumed, _the node shuts itself down_, and the node recovery
will not be applicable any more for this node.
+

* *Scheduler* configuration (`PROACTIVE_HOME/config/scheduler/settings.ini`)
+
In order to make sure that a task will attempt several times to send back its result to the Scheduler, you must set the
properties:

** `pa.scheduler.core.node.ping.attempts` to specify the number of times a finished task will try to contact the
Scheduler. The default for this property is to attempt only once.

** `pa.scheduler.core.nodepingfrequency` property to define a delay (in seconds) in between two attempts. The default
for this property is 20 seconds.

+
If the Scheduler is still not up and running after the total delay defined by these properties (which is 20 seconds by
default), then _the task’s result will be lost_ forever.
Note that these two properties are also used by the scheduler to decide when to *re-schedule* tasks.

==== How to disable nodes and task recovery

You can completely disable the nodes and task recovery feature by setting the `pa.rm.nodes.recovery` property of the
*Resource Manager* to `false`.

In addition, the nodes and task recovery can be configured *per Node Source* at creation time. This configuration will
be applied only if the global `pa.rm.nodes.recovery` property is set to `true`. The specification of nodes recovery per
Node Source can be done through the <<_glossary_rm_web_interface,Resource Manager Web Interface>>, through the Resource
Manager <<_glossary_rest_api,REST API>>, and through the <<_resource_manager_command_line,Command Line>> client (CLI).

** Through the *Web Interface*, the form to create a new Node Source contains a checkbox that controls nodes
recoverability.

** Through the *REST API*, the `nodesRecoverable` parameter of the `rm/nodesource/create` and `rm/nodesource` *REST
endpoints* specify whether the ProActive Nodes of a new Node Source will be recoverable. _Nodes recovery will be
disabled_ if the given value does not match any case of the "true" string.

** Through the *CLI*, the `createns` and `definens` *commands* take an optional parameter to specify whether the
ProActive Nodes of a new Node Source will be recoverable. _Nodes recovery will be disabled_ if
the optional parameter is provided and does not match any case of the "true" string.

=== Nodes and Task Recovery on Cloud Platforms

The nodes and task recovery feature of ProActive is also available for the deployments on _Microsoft Azure_ and _Amazon
EC2_ clouds.

For these platforms, when the Scheduler is restarted after a failure, it will try to contact the nodes that are
deployed on the cloud *instances*:

* If the nodes are alive then the recovery proceeds like with non-cloud infrastructures.

* If the nodes cannot be contacted, the recovery mechanism will try to re-deploy them by asking the instances to run a
node deployment script.

* If the script fails, it means that the cloud instances do not exist anymore, so the recovery mechanism will trigger a
re-deployment including the re-creation of the instances.

=== Nodes and Task Recovery Performance Tuning

By default, the persistence of the node states is slightly delayed in order to batch several database operations in the
same transaction. This is done in order to minimize the execution time overhead of the nodes and task recovery feature.
However, this default *batching mechanism* can be overriden to adapt the delay, or to disable batching.

The properties that allow performance tuning are the following:

** `pa.rm.node.db.operations.delay` defines the delay in milliseconds that is applied when the RM requests the
persistence of a node. By default the delay is set to 100 milliseconds. If the value of this property is set to `0`,
then all database operations related to the persistence of nodes are executed *immediately and synchronously*.

** `pa.rm.nodes.db.operations.update.synchronous` defines whether the node updates are persisted synchronously
(with no delay) whenever it is possible. By default this property is set to `true`.


== Troubleshooting
=== Logs

If something goes wrong the first place to look for the problem are the Scheduler logs. By default all logs are in
`PROACTIVE_HOME/logs` directory.

Users submitting jobs have access to server logs of their jobs through the *Scheduler Web interface*

image::server_logs.png[align="center"]

=== Common Problems

==== 'Path too Long' Errors When Unzipping Windows Downloads

When you unzip a Windows package using the default Windows compression utility,
you might get errors stating that the path is too long. Path length is
determined by the Windows OS. The maximum path, which includes drive letter,
colon, backslash, name components separated by backslashes, and a terminating
null character, is defined as 260 characters.

Workarounds:

  - Move the Zip file on the root level of the system drive and unzip from there.
  - Use a third-party compression utility. Unlike the default Windows
  compression utility, some third-party utilities allow for longer maximum
  path lengths.

== Reference

=== Scheduler Properties

include::./references/SchedulerPropertiesReference.adoc[]

=== Resources Manager Properties

include::./references/RMPropertiesReference.adoc[]

[[_network_properties]]
=== Network Properties

include::./references/NetworkPropertiesReference.adoc[]

[[_rest_api_properties]]
=== REST API & Web Properties

include::./references/WebPropertiesReference.adoc[]

[[_catalog_properties]]
=== Catalog Properties
include::./references/CatalogReference.adoc[]

[[_scheduler_portal_properties]]
=== Scheduler Portal Properties
include::./references/SchedulerPortalReference.adoc[]

[[_resource_manager_command_line]]
=== Command Line
include::../CLI.adoc[]

[[_cloud_infrastructure_startup_script]]
=== Cloud Node Source Infrastructure Startup Script
include::./references/NodeSourceStartupScriptReference.adoc[]
include::../common-footer.adoc[]


++++
<script>

    var composeInstaller = document.getElementById('composeInstaller');
    var helmInstaller = document.getElementById('helmInstaller');
    var shellInstaller = document.getElementById('shellInstaller');

    var confVersion = localStorage.getItem('conf_version');

    if (confVersion && confVersion.includes('SNAPSHOT')) {
        composeInstaller.innerHTML = "$ wget <a href='https://www.activeeon.com/public_content/proactive-installers/dev/proactive-compose-installer-package-dev.zip'>https://www.activeeon.com/public_content/proactive-installers/dev/proactive-compose-installer-package-dev.zip</a>";
        helmInstaller.innerHTML = "$ wget <a href='https://www.activeeon.com/public_content/proactive-installers/dev/proactive-helm-installer-package-dev.zip'>https://www.activeeon.com/public_content/proactive-installers/dev/proactive-helm-installer-package-dev.zip</a>";
        shellInstaller.innerHTML = "$ wget <a href='https://www.activeeon.com/public_content/proactive-installers/dev/proactive-shell-installer-package-dev.zip'>https://www.activeeon.com/public_content/proactive-installers/dev/proactive-shell-installer-package-dev.zip</a>";
    } else {
        composeInstaller.innerHTML = "$ wget <a href='https://www.activeeon.com/public_content/proactive-installers/" + confVersion + "/proactive-compose-installer-package-" + confVersion + ".zip'>https://www.activeeon.com/public_content/proactive-installers/" + confVersion + "/proactive-compose-installer-package-" + confVersion + ".zip</a>";
        helmInstaller.innerHTML = "$ wget <a href='https://www.activeeon.com/public_content/proactive-installers/" + confVersion + "/proactive-helm-installer-package-" + confVersion + ".zip'>https://www.activeeon.com/public_content/proactive-installers/" + confVersion + "/proactive-helm-installer-package-" + confVersion + ".zip</a>";
        shellInstaller.innerHTML = "$ wget <a href='https://www.activeeon.com/public_content/proactive-installers/" + confVersion + "/proactive-shell-installer-package-" + confVersion + ".zip'>https://www.activeeon.com/public_content/proactive-installers/" + confVersion + "/proactive-shell-installer-package-" + confVersion + ".zip</a>";
    }
</script>
++++