:docinfo:
:toc: 
:toc-title: PWS Admin Guide
= ProActive Workflows & Scheduling -- Admin Guide
include::../common-settings.adoc[]
:sectnums!:
== All documentation links
=== link:../user/ProActiveUserGuide.html[User Guide]
ProActive Workflow & Scheduler (PWS) User Guide (Workflows, Tasks, Jobs Submission, Resource Management)

=== link:../MLOS/MLOSUserGuide.html[ML Open Studio] 
Machine Learning Open Studio (ML-OS) User Guide (ready to use palettes with ML Tasks & Workflows)

=== link:../PCA/PCAUserGuide.html[Cloud Automation]
ProActive Cloud Automation (PCA) User Guide (automate deployment and management of Services)

=== link:../admin/ProActiveAdminGuide.html[Admin Guide]
Administration Guide (Installation, networks, nodes, clusters, users, permissions)

*API documentation: &emsp; https://try.activeeon.com/rest/doc/[Scheduler REST] &emsp; link:../user/ProActiveUserGuide.html#_scheduler_command_line[Scheduler CLI] &emsp; https://doc.activeeon.com/javadoc/latest/index.html?org/ow2/proactive/scheduler/rest/SchedulerClient.html[Scheduler Java] &emsp; https://doc.activeeon.com/javadoc/latest/index.html?org/ow2/proactive/scheduler/common/job/TaskFlowJob.html[Workflow Creation Java] &emsp; https://github.com/ow2-proactive/proactive-python-client#proactive-scheduler-client[Python Client]*

:sectnums:
== Overview

include::../Overview.adoc[]

The administration guide covers cluster setup and cluster administration. Cluster setup includes two main steps:

* The installation and configuration of the <<_glossary_proactive_scheduler,*ProActive Scheduler*>>.
* The set up of <<_glossary_proactive_node,*ProActive Nodes*>>.

=== Glossary

include::../Glossary.adoc[]

[[_run_the_proactive_scheduler]]
== Get Started

http://www.activeeon.com/community-downloads[Download^] ProActive Scheduler and *unzip* the archive.

The extracted folder will be referenced as `PROACTIVE_HOME` in the rest of the documentation.
The archive contains all required dependencies.

ProActive Scheduler is ready to be started with no extra configuration.

[source]
----
$ PROACTIVE_HOME/bin/proactive-server
----

[source]
----
The router created on localhost:33647
Starting the scheduler...
Starting the resource manager...
The resource manager with 4 local nodes created on pnp://localhost:41303/
The scheduler created on pnp://localhost:41303/
Starting the web applications...
The web application /scheduler created on http://localhost:8080/scheduler
The web application /rm created on http://localhost:8080/rm
The web application /rest created on http://localhost:8080/rest
The web application /studio created on http://localhost:8080/studio
The web application /catalog created on http://localhost:8080/catalog
*** Get started at http://localhost:8080 ***
----

The following ProActive Scheduler components are started:

* <<_glossary_resource_manager,*Resource Manager*>>
* 4 local <<_glossary_proactive_node,*ProActive Nodes*>>
* <<_glossary_scheduler,*Scheduler*>>
* <<_glossary_rest_api,*REST API*>>
* <<_glossary_rm_web_interface,*Resource Manager Web Interface*>>
* <<_glossary_scheduler_web_interface,*Scheduler Web Interface*>>
* <<_glossary_workflow_studio,*Workflow Studio*>>
* <<_glossary_catalog,*Catalog*>>

The URLs of the Scheduler, Resource Manager, REST API and Web Interfaces are displayed in the output.

*Default credentials: admin/admin*

Your ProActive Scheduler is ready to execute <<_glossary_job,*Jobs*>>!

=== Minimum requirements

The minimum requirements can be found in the Data Sheet. Please find an up to date Data Sheet in the in the link:https://www.activeeon.com/about/resources[resources].

=== Scheduler Life-Cycle Management

Under the Admin menu from the scheduler portal, the following administrator actions are proposed :

* Start the scheduler: Jobs can be submitted. Getting the jobs results is possible. The scheduler can be stopped, frozen, paused or killed.
* Stop the scheduler: Jobs cannot be submitted anymore. Already running jobs run to completion, but not pending jobs. Getting the jobs results is possible. The scheduler can be started or killed.
* Freeze the scheduler: Every running tasks run to completion, but the running jobs wait for the scheduler to resume. The scheduler can be stopped, resumed or killed.
* Resume the scheduler: If the scheduler is Paused or Frozen.
* Pause the scheduler: Every running jobs run to completion. The scheduler can be stopped, resumed or killed.
* Kill the scheduler: Similar to Ctrl-C. No interation can be done anymore.
* Shutdown the scheduler: Every running tasks run to completion before terminating the scheduler.

== ProActive Scheduler configuration

All configuration files of ProActive Scheduler can be found under `PROACTIVE_HOME`.

=== Java Virtual Machine configuration

Various command-line tools shipped with ProActive (bin/proactive-server, bin/proactive-node) start a Java Virtual Machine.
The parameters of the JVM can be modified by editing DEFAULT_JVM_OPTS= in the corresponding script both in Linux and Windows.

For example, to set the maximum heap capacity on the JVM to 6GB in Linux:

Change the line
`DEFAULT_JVM_OPTS='"-server" "-Dfile.encoding=UTF-8" "-Xms4g"'`

into

`DEFAULT_JVM_OPTS='"-server" "-Dfile.encoding=UTF-8" "-Xms4g" "-Xmx6g"'`


=== General configuration

.ProActive Scheduler configuration files

|===
|Component | Description|File |Reference

.1+^.^|Scheduler
|Scheduling Properties|config/scheduler/settings.ini|<<_scheduler_properties>>

.1+^.^|Resource Manager
|Node management configuration|config/rm/settings.ini|<<_resources_manager_properties>>

.1+^.^|Web Applications
|REST API and Web Applications configuration|config/web/settings.ini|<<_rest_api_properties>>

.1+^.^|Networking
|Network, firewall, protocols configuration|config/network/node.ini, config/network/server.ini|<<_network_properties>>

.4+^.^|Security
|User logins and passwords|config/authentication/login.cfg|<<_file>>
|User group assignments|config/authentication/group.cfg|<<_file>>
|User permissions|config/security.java.policy-server|<<_user_permissions>>
|LDAP configuration|config/authentication/ldap.cfg|<<_ldap>>

.5+^.^|Database
|Scheduler configuration|config/scheduler/database.properties|<<Database configuration>>
|Resource Manager configuration|config/rm/database.properties|<<Database configuration>>
|Scheduling-api microservice|dist/war/scheduling-api/WEB-INF/classes/application.properties|<<Database configuration>>
|Job-planner microservice|dist/war/job-planner/WEB-INF/classes/application.properties|<<Database configuration>>
|Catalog microservice|dist/war/catalog/WEB-INF/classes/application.properties|<<Database configuration>>

|===

=== Database configuration

Scheduler, Resource Mananger, and some microservices require to have direct access to the database.
Thus, they must each have a correct database configuration.

To configure Scheduler or Resource Mananger, one have to modify *PROACTIVE_HOME/config/scheduler/database.properties*
and *PROACTIVE_HOME/config/rm/database.properties* respectively.

The following five properties must be configured:

* `hibernate.connection.driver_class`
* `hibernate.connection.url`
* `hibernate.dialect`
* `hibernate.connection.username`
* `hibernate.connection.password`

TIP: There are usefull templates for different databases in *PROACTVE_HOME/config/scheduler/templates/* and *PROACTVE_HOME/config/rm/templates/*.

The *job-planner*, *catalog*, *scheduling-api*, and *proactive-cloud-watch* microservices must contain a database configuration.
For each microservice, a microservice configuration file can be found in: *PROACTIVE_HOME/dist/war/MICROSERVICE-NAME/WEB-INF/classes/application.properties*.
In their *application.properties* file, you need to set the following five properties:

* `spring.datasource.url`
* `spring.datasource.username`
* `spring.datasource.password`
* `spring.datasource.driverClassName`
* `spring.jpa.database-platform`

Moreover, it is mandatory to put the proper database driver in *PROACTIVE_HOME/addons* folder, e.g. if you want to use MySQL 5.7 database,
then you have to download the database driver and put it at: *PROACTIVE_HOME/addons/mysql-connector-java-8.0.11.jar*.

Scheduler, and 3 microservices (*scheduling-api*, *job-planner*, and *proactive-cloud-watch*) have to point to the same database.

== Installation on a Cluster

Adding Compute Hosts of a cluster to the ProActive Scheduler typically involves unpacking the release archive on all those hosts.
Once it's done you need to run a ProActive Node on the Compute Host and connect it to the ProActive Scheduler. There are two principal ways of
doing that:

* Launch a process on the Compute Host and connect it to the ProActive Scheduler
* Initiate the deployment from the ProActive Scheduler: <<_deploy_nodes_via_ssh,Node Source creation>>

If you are not familiar with ProActive Scheduler you may want to try the *first method* as it's easier to understand.
Combined with <<_proactive_agents,*ProActive Agents*>>, it gives you the same result as the *second method*.

The *second method* implies that you have a remote access to Compute Hosts (e.g. SSH access) and you want to start and stop ProActive Nodes
by launching commands remotely on Compute Hosts. For instance, it can be useful when a virtual machine needs to be deployed prior to launching a ProActive Node.

[[run_node_manually]]
=== Deploy ProActive Nodes manually

[[proactive_node]]
==== Using proactive-node command

Let's take a closer look at the first method described above. To deploy a ProActive Node from the Compute Host you need
to run the following command
----
$ PROACTIVE_HOME/bin/proactive-node -r pnp://SCHEDULER_ADDRESS:64738
----
where `-r` option is used to specify the URL of the *Resource Manager*.
You can find this URL in the output of the <<_run_the_proactive_scheduler,ProActive Scheduler>> (the Resource Manager URL).
If you want to run multiple tasks at the same time on the same machine, you can either start a few `proactive-node` processes
or start multiple nodes from the same process using the `-w` command line option.

TIP: You can also use *discovery* to let the ProActive Node find the URL to connect to on its own.
 Simply run `proactive-node` without any parameter to use discovery.
 It uses broadcast to retrieve the URL so this feature might not work depending on your network configuration.

[[node_jar]]
==== Using node.jar

It is also possible to launch a ProActive Node without even copying the ProActive Scheduler to a Compute Host:

* Open a browser on the Compute Host.
* Navigate to the Resource Manager Web Interface. You can find the URL in the output of the <<_run_the_proactive_scheduler,ProActive Scheduler>> (the Resource Manager web application URL).
* Use default demo/demo account to access the Resource Manager Web Interface.
* Click on 'Portal->Launch' to download *node.jar*.
* Click on 'Portal->Create Credentials' and download your credential file.
* Create a Node Source using the infrastructure DefaultInfrastructureManager.
* Run it:
----
$ java -Dproactive.communication.protocol=pnp -jar node.jar -f CREDENTIAL_FILE -s NAME
----
Where NAME is the name of the node source.
It should connect to the ProActive Scheduler automatically using the discovery mechanism otherwise you might
have to set the URL to connect to with the `-r` parameter.

TIP: If you would like to execute several Tasks at the same time on one host, you can either launch several ProActive
Node process or use the _-w_ parameter to run multiple nodes in the same process. A node executes one Task at a time.

[[proactive_node_autoupdate]]
==== Using proactive-node-autoupdate command

This method is a combination of the two above, it starts a node from the command line, and makes sure the node libary (node.jar) is synchronized with the latest server version.

The `proactive-node-autoupdate` command acts as a bootstrap and spawns a new java virtual machine with the up-to-date library classpath.

Launching the node is similar to the `proactive-node` command, additional options must be specified to allow the download of the node.jar from the server:

----
$ PROACTIVE_HOME/bin/proactive-node-autoupdate -r pnp://SCHEDULER_ADDRESS:64738 -nju http://SCHEDULER_ADDRESS:8080/rest/node.jar -njs /tmp/node.jar -nja
----

where `-nju` option is used to specify the http URL of the *node.jar*.
You can find this URL in the output of the <<_run_the_proactive_scheduler,ProActive Scheduler>>, the node.jar url is built by appending */rest/node.jar* to the base http url of the server.

`-nju` option is used to specify where the node.jar will be stored locally on the machine.

Finally, `-nja` option, when enabled, specifies that the `proactive-node-autoupdate` will be always up, it means that it will automatically restart when the node terminates (for example in case of server upgrades).

[[_deploy_nodes_via_ssh]]
=== Deploy ProActive Nodes via SSH

The second way of deploying ProActive Nodes is to create <<_glossary_node_source,*Node Sources*>> from the ProActive Scheduler.

[NOTE]
====
Examples of a Node Source:

* a cluster with SSH access where nodes are available from 9 a.m. to 9 p.m.
* nodes from Amazon EC2 available permanently for users from group 'cloud'.
====

When creating a Node Source, you can choose an *Infrastructure Manager* from the list of supported <<_node_source_infrastructure,Node Source Infrastructures>>
 and a <<_node_source_policy>> that defines rules and limitations of nodes' utilization.

To create a Node Source you can do any of the following:

* Use the *Resource Manager Web Interface* ('Add Nodes' menu)
* Use the *REST API*
// TODO add link dev
* Use the <<_resource_manager_command_line>>

In order to create an SSH Node Source you should first configure an *SSH access* from the server to the Compute Hosts
that http://www.linuxproblem.org/art_9.html[does not require password]. Then create a text file (refered to as `HOSTS_FILE` below) containig the hostnames of all your Compute Hosts.
Each line shoud have the format:
[source]
----
HOSTNAME NODES_COUNT
----
where NODES_COUNT is the number of ProActive Nodes to start (corresponds to the number of Tasks that can be executed in parallel) on the corresponding host. Lines beginning with `#` are comments. Here is an example:

[source]
----
# you can use network names
host1.mydomain.com 2
host2.mydomain.com 4
# or ip addresses
192.168.0.10 8
192.168.0.11 16
----

Then using this file create a Node Source either from the *Resource Manager Web Interface* or from the command line:
[source]
----
$ PROACTIVE_HOME/bin/proactive-client -createns SSH_node_source --infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.SSHInfrastructure HOSTS_FILE 60000 3 5000 "" /home/user/jdk/bin/java PROACTIVE_HOME Linux "" config/authentication/rm.cred
----

Don't forget to replace `SCHEDULER_ADDRESS`, `HOSTS_FILE` and `PROACTIVE_HOME` with the corresponding values.

See <<_ssh_infrastructure>> reference for details on each parameter.

[[_proactive_agents]]
=== Deploy ProActive Nodes via Agents

In your production environment you might want to control and limit the resources utilization on some or all Compute Hosts,
especially if those are desktop machines where people perform their daily activities. Using *ProActive Agent* you can:

* Control the number of ProActive Node processes on each Compute Host
* Launch ProActive Nodes automatically when a Compute Host starts
* Restart ProActive Nodes if they fail for some reason and reconnect them to ProActive Scheduler

*ProActive Agents* exists for both <<_proactive_linux_agent,Linux>> and <<_proactive_windows_agent,Windows>>
 operating systems.

[[_proactive_linux_agent]]
==== ProActive Linux Agent

The ProActive Agent for Linux can be downloaded from ActiveEon's http://www.activeeon.com/community-downloads[website^].

To install the ProActive Agent on Debian based distributions (the archive name will vary depending on the version and architecture):

    sudo dpkg -i proactive-agent.deb

TIP: Some extra dependencies might be required, to install them: `sudo apt-get -f install`

To install the ProActive Agent on Redhat based distributions (the archive name will vary depending on the version and architecture):

    sudo rpm -i proactive-agent.rpm

TIP: Some extra dependencies might be required, to install them you need to install each dependency individually using `sudo yum install`

By default the Linux Agent will launch locally as many nodes as the number of
CPU cores available on the host minus one. The URL to use for connecting nodes
to the Resource Manager can be configured in `/opt/proactive-agent/config.xml`,
along with several other parameters. If no URL is set, then broadcast is used
for auto discovery. However, this feature might not work depending on your
network configuration. Consequently, it is recommended to set the Resource
Manager URL manually.

Logs related to the Agent daemon are located in
`/opt/proactive-agent/proactive-agent.log`.

Binaries, configuration files and logs related to ProActive nodes started by the
agent are available at `/opt/proactive-node/`, respectively in subfolders `dist/lib`,
`config` and `logs`.

===== Configuring the Linux Agent

To configure the agent behaviour:

* Stop the Linux Agent
+
----
sudo /etc/init.d/proactive-agent stop
----

* Update the *config.xml* file in `/opt/proactive-agent` folder
+
In case there is no such directory, create the directory. Then, create the *config.xml* file or create a symbolic link
+
----
sudo mkdir -p /opt/proactive-agent
sudo ln -f -s <path-to-config-file> /opt/proactive-agent/config.xml
----

* Change the ownership of the *config.xml* file
+
----
sudo chown -Rv proactive:proactive config.xml
----
+
If the group named "proactive" does not exist, create the group and add "proactive" user to the group:
+
----
sudo groupadd proactive
sudo usermod -ag proactive proactive
----

* Start the Linux Agent

    sudo /etc/init.d/proactive-agent start


===== Uninstalling the Linux Agent

On Debian based distributions:

    sudo dpkg -r proactive-agent

On Redhat based distributions:

    sudo yum remove proactive-agent

[[_proactive_windows_agent]]
==== ProActive Windows Agent

The ProActive Windows Agent is a *Windows Service*: a long-running executable that performs specific
functions and which is designed to not require user intervention. The agent is able to create a ProActive Node
 on the local machine and to connect it to the *ProActive Resource Manager*.

After being installed, it:

* Loads the user's configuration
* Creates schedules according to the working plan specified in the configuration
* Spawns a ProActive Node that will run a specified java class depending on the selected connection type. 3 types of connections are available:
** *Local Registration* - The specified java class will create a ProActive Node and register it locally.
** *Resource Manager Registration* - The specified java class will create a ProActive Node and register
it in the specified Resource Manager, thus being able to execute java or native tasks received from the Scheduler. It is is important
to note that a ProActive Node running tasks can potentially spawn child processes.
** *Custom* - The user can specify his own java class.
* Watches the spawned ProActive Nodes in order to comply with the following limitations:
** *RAM limitation* - The user can specify a maximum amount of memory allowed for a ProActive Node and its children. If the limit is reached, then all processes are automatically killed.
** *CPU limitation* - The user can specify a maximum CPU usage allowed for a ProActive Node and its children. If the limit is exceeded by the sum of CPU usages of all processes, they are automatically throttled to reach the given limit.
* Restarts the spawned ProActive Node in case of failures with a timeout policy.

==== Install Agents on Windows

The ProActive Windows Agent installation pack is available on the official http://proactive.inria.fr/[ProActive website^].
Run the `setup.exe` file and follow instructions. When the following dialog appears:

image::install_config.png[align="center"]

. Specify the directory that will contain the *configuration* file named `PAAgent-config.xml`, note that if this file already exists in the specified directory it will be re-used.
. Specify the directory that will contain the *log files* of the ProActive Agent and the spawned runtimes.
. Specify an existing, local *account* under which the ProActive Nodes will be spawned. It is highly recommended to specify an account that is not part of the Administrators group to isolate the ProActive Node and reduce security risks.
. The *password* is encrypted using Microsoft AES Cryptographic Provider and only Administrators have access permissions to the keyfile (restrict.dat) this is done using the SubInACLtool.
. If the specified account does not exist the installation program will prompt the user to create a non-admin account with the required privileges.
+
Note that the ProActive Agent service is installed under LocalSystem account, this should not be changed,
however it can be using the `services.msc` utility. ('Control Panel->Administrative Tools->Services')

. If you want that any non-admin user (except guest accounts) be able to start/stop the ProActive Agent service check the "Allow everyone to start/stop" box. If this option is checked the installer will use the SubInACL tool. If the tool is not installed in the `Program Files\Windows Resource Kits\Tools` directory the installer will try to download its installer from the official Microsoft page.
. The installer will check whether the selected user account has the required privileges. If not follow the steps to add these privileges:
.. In the 'Administrative Tools' of the 'Control Panel', open the 'Local Security Policy'.
.. In 'Security Settings', select 'Local Policies' then select 'User Rights Assignments'.
.. Finally, in the list of policies, open the properties of 'Replace a process-level token' policy and add the needed user. Do the same for 'Adjust memory quotas for a process'. For more information about these privileges refer to the official Microsoft page.

At the end of the installation, the ProActive Agent Control utility should be started. This next section explains how to configure it.

To uninstall the ProActive Windows Agent, simply run 'Start->Programs->ProActiveAgent->Uninstall ProActive Agent'.

==== Configure Agents on Windows

To configure the Agent, launch 'Start->Programs->ProActiveAgent->AgentControl' program or click on
the notify icon if the "Automatic launch" is activated.
Double click on the tray icon to open the *ProActive Agent Control* window. The following window will appear:

image::agent_control.png[align="center"]

From the ProActive Agent Control window, the user can load a configuration file, edit it, start/stop the service and view logs.
A GUI for editing is provided (explained below). Even if it is not recommended, you can edit the configuration file by yourself with your favorite text editor.

It is also possible to change the ProActive Nodes Account using the 'Change Account' button.

When you click on 'GUI Edit', the following window appears:

image::config_editor_general.png[align="center"]

In the general tab, the user can specify:

* The *ProActive Scheduler* location.
* The *JRE* location (usually something like `C:\Program Files\Java\jdk1.6.0_12`).
* The *numbers of Runtimes and Nodes* (the number of spawned processes and the number of ProActive Nodes per process).
* The *JVM options*. Note that if the parameter contains `${rank}`, it will be dynamically replaced by the ProActive Node rank starting from 0.
* The *On Runtime Exit* script. A script executed after a ProActive Node exits. This can be useful to perform additional cleaning operation. Note that the script receives as parameter the PID of the ProActive Node.
* The user can set a *memory limit* that will prevent the spawned processes to exceed a specified amount of RAM. If a spawned process or its child process requires more memory, it will be killed as well as its child processes. Note that this limit is disabled by default (0 means no limit) and a ProActive Node will require at least 128 MBytes.
* It is possible to list all available *network interfaces* by clicking on the "Refresh" button and add the selected network interface name as a value of the `proactive.net.interface` property by clicking on "Use" button. See the ProActive documentation for further information.
* The user can specify the *protocol* (PNP or PAMR) to be used by the ProActive Node for incoming communications.
* To ensure that a unique port is used by a ProActive Node, the initial port value will be incremented for each node process and given as value of the `-Dproactive.SELECTED_PROTOCOL.port` JVM property. If the port chosen for a node is already used, it is incremented until an available port number is found.

Clicking on the 'Connection' tab, the window will look like this:

image::config_editor_connection.png[]

In the 'Connection' tab, the user can select between three types of connections:

* *Local Registration* - creates a local ProActive node and registers (advertises) it in a local RMI registry. The node name is optional.
* *Resource Manager Registration* - creates a local ProActive node and registers it in the specified Resource Manager. The mandatory Resource Manager's url must be like *protocol://host:port/*. The node name and the node source name are optional. Since the Resource Manager requires authentication, the user specifies the file that contains the credential. If no file is specified the default one located in `%USERPROFILE%\.proactive\security` folder is used.
* *Custom* - the user specifies his own java starter class and the arguments to be given to the main method. The java starter class must be in the classpath when the ProActive Node is started.

Finally, clicking on the "Planning" tab, the window will look like this:

image::config_editor_planning.png[]

In the Planning Tab, depending on the selected connection type, the agent will initiate it according to a *weekly planning* where each plan specifies the connection start time as well as the working duration. The agent will end the connection as well as the ProActive Nodes and its child processes when the plan duration has expired.

Moreover, it is possible to specify the ProActive Node Priority and its *CPU usage* limit. The behavior of the CPU usage limit works as follows: if the ProActive Node spawns other processes, they will also be part of the limit so that if the sum of CPU% of all processes exceeds the user limit they will be throttled to reach the given limit. Note that if the Priority is set to RealTime the CPU % throttling will be disabled.

The "Always available" makes the agent to run permanently with a Normal Priority and Max CPU usage at 100%.

==== Launching Windows Agent

Once you have configured the agent, you can start it clicking on the "Start" button of the ProActive Agent Control window. However, before that, you have to ensure that *ProActive Scheduler* has been started on the address you specified in the agent configuration. You do not need to start a node since it is exactly the job of the agent.

Once started, you may face some problems. You can realise that an error occurred by first glancing at the color of the agent tray icon. If everything goes right, it should keep the blue color. If its color changes to yellow, it means that the agent has been stopped. To see exactly what happened, you can look at the runtime log file located into the agent installation directory and named `Executor<runtime number>Process-log.txt`.

The main troubles you may have to face are the following ones:

* You get an *access denied error*: this is probably due to your default java.security.policy file which cannot be found. If you want to specify another policy file, you have to add a JVM parameter in the agent configuration. A policy file is supplied in the scheduling directory. To use it, add the following line in the JVM parameter box of the agent configuration (Figure 5.3, “Configuration Editor window - General Tab ”):
----
-Djava.security.policy=PROACTIVE_HOME/config/security.java.policy-client
----
* You get an *authentication error*: this is probably due to your default credentials file which cannot be found. In the "Connection" tab of the Configuration Editor (Figure 5.4, “Configuration Editor window - Connection Tab (Resource Manager Registration)”), you can choose the credentials file you want. You can select, for instance, the credentials file located at PROACTIVE_HOME/config/authentication/scheduler.cred or your own credentials file.
* The node seems to be well started but you cannot see it in the Resource Manager interface : in this case, make sure that the *port number* is the good one. Do not forget that the runtime port number is incremented from the initial ProActive Resource Manager port number. You can see exactly on which port your runtime has been started looking at the log file described above.

==== Automate Windows Agent Installation

The automated installation can be run through a command line (cmd.exe) with administrator priviledge.
This can be useful to trigger installation. In order to launch the silent installation, the /S option must be passed in the command line to the ProActive Windows Agent installer.
Several options are required, such as the user and password.

Here is an example of automated installation command:

```
ProactiveAgent-8.3.0-standalone-x64-setup.exe /S /USER=proactive /PASSWORD=proactive
```

Optionally you can add a domain name.

```
ProactiveAgent-8.3.0-standalone-x64-setup.exe /S /USER=proactive /PASSWORD=proactive /DOMAIN=mydomain
```

You can also activate compatibility mode from the command line if you have any problems:

```
set __COMPAT_LAYER=WINXPSP3
ProactiveAgent-8.3.0-standalone-x64-setup.exe /S /USER=proactive /PASSWORD=proactive
```


Here is the full list of command line options which can be passed to the installer:



[cols=2*]
|===

|`/S` | Run silently without graphical interface, uninstall any previous installation.

|`/UN` | Run the uninstall only.

|`/ALLOW` | Allow all users to control the service.

|`/USER=user` | Associate `user` account to manage proactive agent service, create user if it does not exist.

|`/PASSWORD=pass` | Define password for the proactive agent `user`.

|`/DOMAIN=dom` | Specify `dom` as the windows domain, by default domain will be set to the machine host name.

|`/LOG_DIR=path` | Define `path` where the agent log files, default is `$PROGRAMFILES\ProActiveAgent\logs`.

|`/CONFIG_DIR=path` | Define `path` where the configuration files are stored, default is `$PROGRAMFILES\ProActiveAgent\config`.

|`/USE_ACC` | Use current user's account home as `/CONFIG_DIR` and `/LOG_DIR`.

|===



==== Configuring Linux Or Windows Agents For Auto-Update

The *Linux* or *Windows Agents* can be configured to automatically synchronize the node libary (node.jar) with the ProActive server.

The behavior is similar to the <<proactive_node_autoupdate,proactive-node-autoupdate>> command, the main JVM will act as a bootstrap and spawn a child JVM with the up-to-date library in its classpath.

In order to enable auto-update, you need to edit the agent configuration file. This file location is:

 * Linux Agent: `/opt/proactive-agent/config.xml`
 * Windows Agent: `C:\Program Files (x86)\ProActiveAgent\config\PAAgent-config.xml`

The following changes must be performed:

 1. Change the *<javaStarterClass>* to `org.ow2.proactive.resourcemanager.updater.RMNodeUpdater`
 2. Add the following *<jvmParameters>*
   * `node.jar.url` : url of the node jar (similar to the `nju` option of the `proactive-node-autoupdate` command)
   * `node.jar.saveas` : path used to store the node.jar file locally (similar to the `njs` option of the `proactive-node-autoupdate` command)
 3. Other *JVM properties* specified will be forwarded to the spawned JVM, but non-standard options such as *-Xmx* will not be forwarded. In order to do so, you must declare them using the following syntax:

----
   <param>-DXtraOption1=Xmx2048m</param>
   <param>-DXtraOption2=Xms256m</param>
   ...
----

=== Deploy ProActive Nodes dynamically via other schedulers (PBS, SLURM, ...)

This functionality is also called _*Meta-Scheduling*_.

If an existing cluster is available in your organization and this cluster is managed by a native scheduler such as *SLURM*, *LSF* or *PBS*, you may want to execute transparently on this cluster ProActive <<__glossary_workflow,*Workflows*>> and <<_glossary_task,*Tasks*>>.

As ProActive Tasks can be executed only on <<_glossary_proactive_node,*ProActive Nodes*>>, it is necessary to deploy ProActive Nodes on the cluster.

Dynamic deployment of ProActive Nodes on a cluster to execute ProActive Tasks is handled by defining a _Native Scheduler <<_glossary_node_source,Node Source>>_ containing three components:

 * The `Native Scheduler Infrastructure`: a <<_glossary_node_source_infrastructure,*Node Source Infrastructure*>> which allows to interact with a native scheduler to deploy ProActive Nodes.

 * The `Native Scheduler Policy`:  a <<_glossary_node_source_policy,*Node Source Policy*>> which interacts with the Native Scheduler Infrastructure to request ProActive Nodes deployment dynamically based on the <<_glossary_proactive_scheduler,*ProActive Scheduler*>> pending queue.

 * The `Native Scheduler Scheduling Policy`: a <<_glossary_scheduling_policy,*Scheduling Policy*>> which allows the execution of ProActive Tasks on ProActive Nodes provisioned by the Native Scheduler Policy.

The provisioning of ProActive Nodes is controlled by a specific <<../user/ProActiveUserGuide.adoc#_generic_information,*Generic Information*>> `"NS"` which can be defined at _Task_ or _Job_ level.

When this generic information is configured for a _Task_, a ProActive Node will be dynamically deployed on the cluster to execute the Task. This ProActive Node will be associated with the Task and will only accept this Task execution. When the Task terminates, the ProActive Node will also be terminated.

When this generic information is configured for a _Job_, all Tasks contained in the Job will execute on the cluster. Nodes will be created dynamically to execute Tasks of this Job, with a similar Task/Node association.

The behavior of the Meta-Scheduling feature is summarized on the following diagram (example for PBS integration):

image::NativeScheduler.png[PBS Integration, , title="PBS Integration"]

The following paragraphs explain how to configure the Native Scheduler Infrastructure, Native Scheduler Policy, Native Scheduler Scheduling Policy and execute ProActive tasks using a Native Scheduler Node Source.

==== Glossary

[[_glossary_ns_cluster]]
Cluster::
  a group of tightly coupled <<_glossary_ns_node,nodes>> managed by a <<_glossary_ns_native_scheduler,native scheduler>>.

[[_glossary_ns_node]]
Cluster Node::
  a computer, part of a <<_glossary_ns_cluster,cluster>> used to execute <<_glossary_ns_cluster_job,cluster jobs>>.

[[_glossary_ns_native_scheduler]]
Native Scheduler::
  A software which dispatch <<_glossary_ns_cluster_job,cluster jobs>> on <<_glossary_ns_node,cluster nodes>>, also known as Cluster Manager.

[[_glossary_ns_cluster_job]]
Cluster Job::
  A running command executed on a <<_glossary_ns_cluster,cluster>> by a <<_glossary_ns_native_scheduler,native scheduler>>. Also known as Batch Job.

[[_glossary_ns_cluster_jobid]]
Cluster Job Id::
  Identifier representing a <<_glossary_ns_cluster_job,cluster job>> inside a <<_glossary_ns_native_scheduler,native scheduler>>.

[[_glossary_head_node]]
Head Node::
  A specific <<_glossary_ns_node,cluster node>> where the <<_glossary_ns_native_scheduler,native scheduler>> server runs.

[[_glossary_cluster_user]]
Cluster User::
  A linux operating system account registered on the cluster.

[[_glossary_proactive_scheduler_user]]
ProActive Scheduler User::
  An account registered in the <<_glossary_proactive_scheduler,*ProActive Scheduler*>>, the account is only registered inside the ProActive Scheduler and does not necessarily match an operating system account.

[[_glossary_proactive_scheduler_process_user]]
ProActive Scheduler Process User::
  The operating system account which started the ProActive Scheduler server process.


==== Native Scheduler Node Source Configuration

Using the <<_glossary_rm_web_interface,*Resource Manager Web Interface*>>, you can create a <<_glossary_node_source,*Node Source*>> used to acquire <<_glossary_proactive_node,*ProActive Nodes*>> from a <<_glossary_ns_native_scheduler,*Native Scheduler*>>.

From The drop down menu, the `NativeSchedulerInfrastructure` and `NativeSchedulerPolicy` must be selected.

Here is an explanation of the node source parameters:

 * `Name` : you should name the node source accordingly to your infrastructure, it can be the <<_glossary_ns_cluster,cluster>> name, <<_glossary_ns_head_node,head node>> name, or <<_glossary_ns_native_scheduler,native scheduler>> name (PBS, SLURM), etc.

===== NativeSchedulerInfrastructure

 * `RMCredentialsPath` : path to a file which contains the credentials of an administrator user which will own the node source. The ProActive Scheduler Server release contains two admin users credentials files : `config/authentication/rm.cred` and `config/authentication/admin_user.cred`
 * `NSFrontalHostAddress` : the host name or IP address of the cluster <<_glossary_ns_head_node,head node>>.
 * `NSSchedulerHome` : the location of the shared ProActive installation on <<_glossary_ns_node,cluster nodes>> (cluster nodes must be able to access ProActive libraries in order to start ProActive Node). Example `/opt/proactive/activeeon_enterprise-node-linux-x64-8.1.0`.
 * `javaHome` : similarly, cluster nodes must be able to access the java command in order to start ProActive Nodes. ProActive installation includes a Java Runtime Environment under the `jre` subfolder. Example: `/opt/proactive/activeeon_enterprise-node-linux-x64-8.1.0/jre`.
 * `jvmParameters` : additional options which can be passed to the java command.
 * `sshOptions` : additional options which can be passed to the ssh command used to connect to connect to the host name or IP address specified in the NSFrontalHostAddress parameter.
 * `NSNodeTimeoutInSeconds` : timeout to wait for the deployment of ProActive Nodes on the cluster. As the time needed to deploy ProActive Nodes depends on the cluster load, this timeout should be a large value. If the timeout is reached, the ProActive Nodes will be in `"Lost"` <<_node_states,state>>.
 * `ìmpersonationMethod`: when a job is submitted to the native scheduler, the submission is performed under the current <<_glossary_proactive_scheduler_user,ProActive Scheduler user>>. An impersonation is thus performed between the <<_glossary_proactive_scheduler_process_user,scheduler server process>> and the target <<_glossary_cluster_user,cluster user>>.
 This impersonation can be performed using 3 different strategies:
 ** `ssh`: in that case the <<_glossary_ns_head_node,head node>> is contacted using a ssh command with the current <<_glossary_proactive_scheduler_user,ProActive Scheduler user>> and password. User/password combination between the ProActive Scheduler and the head node operating system must match.
 ** `none`: in that case the head node is contacted using a ssh command with the <<_glossary_proactive_scheduler_process_user,ProActive Scheduler process user>> (passwordless ssh). Submission to the native scheduler will be performed with the same account.
 ** `sudo`: similar to `none` regarding the connection to the head node, but a `sudo` command will be initiated to impersonate as the current <<_glossary_proactive_scheduler_user,ProActive Scheduler user>>, before doing a job submission.
 * `alternateRMUrl` : the url used by the ProActive Nodes to contact <<_glossary_resource_manager,ProActive Resource Manager>>. This url is displayed on ProActive server startup. Example: `pnp://myserver:64738`.
 * `sshPort` : port used for ssh connections.
 * `nsPreCommand` : a linux command which can be run before launching ProActive Nodes on the cluster. Can be used as a workaround when some system environment variables are not properly set when starting ProActive Nodes.
 * `nsSubmitCommand` : this is the main command used to start ProActive Nodes on the cluster. Depending on the actual native scheduler implementation, `nsSubmitCommand` will vary, here are examples definitions: +
+
[cols=2*]
|===

|PBS |`qsub -N %NS_JOBNAME% -o %LOG_FILE% -j oe`

|SLURM |`sbatch -J %NS_JOBNAME% -o %LOG_FILE%`

|LSF |`bsub -J %NS_JOBNAME% -o %LOG_FILE% -e %LOG_FILE%`

|===
+
The command can use patterns which will be replaced dynamically by the ProActive Resource Manager. +
+
[cols=2*]
|===

|`%NS_JOBNAME%` |contains a configurable job name dynamically created by the resource manager.

|`%LOG_FILE%` |contains a log file path dynamically created by the resource manager and located in side the NSSchedulerHome installation. This log file is useful to debug errors during <<_glossary_ns_cluster_job,cluster job>> submission.

|`%PA_USERNAME%` |contains the current ProActive Scheduler user.
|===

 * `nsKillCommand` : this is the command used to kill ProActive Nodes started previously by the nsSubmitCommand. Similarly to nsSubmitCommand, `nsKillCommand` will vary for each native scheduler syntax: +
+
[cols=2*]
|===

|PBS |`qdel %NS_JOBID%`

|SLURM |`scancel -n %NS_JOBNAME%`

|LSF |`bkill -J %NS_JOBNAME%`

|===
+
It can use the following patterns: +
+
[cols=2*]
|===

|`%NS_JOBNAME%` |contains a configurable job name dynamically created by the resource manager.

|`%NS_JOBID%` |contains the job id returned by the native scheduler when submitting the job. Currently, job id can only be used with PBS, when the setting `submitReturnsJobId` is set to `true`.

|===

 * `submitReturnsJobId`: is the <<_glossary_ns_cluster_jobid,cluster job id>> returned plainly when calling the nsSubmitCommand. This is the behavior of PBS, and this is why this setting should be set to `true` when using PBS.
 * `nsJobName`: a way to configure the `%NS_JOBNAME%` pattern. The following patterns can be used: +
+
[cols=2*]
|===

|`%PA_TASKID%` |contains the ProActive Task and Job ID associated with the node request.

|`%PA_USERNAME%` |contains the current ProActive Scheduler user.

|===

 * `maxDeploymentFailure`: number of attempts when starting a ProActive Node on the cluster using the nsSubmitCommand, after all attempts failed, the ProActive Node will be declared as `Lost`.

===== NativeSchedulerPolicy

 * `userAccessType` : which users are allowed to use ProActive Nodes created by the NativeSchedulerInfrastructure. Refer to the <<_policies,Policies documentation>>.
 * `providerAccessType` : defines who can add nodes to this node source. Refer to the <<_policies,Policies documentation>>.
 * `schedulerUrl` : the url used by the ProActive Nodes to contact the <<_glossary_resource_manager,ProActive Resource Manager>>. This url is displayed on ProActive server startup. Example: `pnp://myserver:64738`.
 * `schedulerCredentialsPath` : path to a file which contains the credentials of an administrator user which will connect to the scheduler. The ProActive Scheduler Server release contains two admin users credentials files : `config/authentication/rm.cred` and `config/authentication/admin_user.cred`
 * `rearrangeTasks` : currently not implemented.
 * `autoScaling` : if set to `true`, the NativeSchedulerPolicy will scan the Resource Manager activity and Scheduling queue. If the scheduling queue is not empty and all resource manager nodes are busy, `autoscaling` will automatically start ProActive Nodes from the NativeSchedulerInfrastructure. This setting cannot be used when multiple NativeScheduler node sources are deployed.
 * `refreshTime` : the NativeSchedulerPolicy will refresh its status and observe the ProActive Scheduler queue every `refreshTime` milliseconds.

===== Creating the Node Source

When the node source is created, it will be activated as other node sources (LocalInfrastructure, SSHInfrastructure, etc), but no ProActive Node will appear.

This is expected as the Native Scheduler node source is *dynamic*, it will only create ProActive Nodes when specific conditions are met.

==== Configure NativeSchedulerSchedulingPolicy

After creating the Native Scheduler Node Source, it is necessary as well to change the <<_extending_proactive_scheduling_policy,*ProActive Scheduling Policy*>> to use the `NativeSchedulerSchedulingPolicy`.

This policy ensures that tasks are executed on appropriate ProActive Nodes when using a Native Scheduler Node Source.

In order to do that, edit the file `PROACTIVE_HOME/config/scheduler/settings.ini` and change the following line:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.ExtendedSchedulerPolicy
----

to:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.NativeSchedulerSchedulingPolicy
----

==== Match User Accounts With the Cluster

Submission of jobs to the native scheduler is done by default using a SSH connection.

When a ProActive Task belonging to a given user _Alice_ needs to execute on a NativeScheduler node source, a SSH connection will be performed with the login and password of the _Alice_ user registered in the ProActive Scheduler.

Accordingly, this login/password combination must correspond to a real user on the cluster <<_glossary_ns_head_node,head node>>.

Please refer to the <<_user_authentication,*User Authentication*>> section in order to manage ProActive Users.

==== Execute Tasks on a Native Scheduler Node Source

The <<../user/ProActiveUserGuide.adoc#_generic_information,*Generic Information*>> `"NS"` allows a ProActive Task to be executed on a Native Scheduler node source.

It must contain the name of the target node source. For example, to submit a Task on the "PBS" Node Source:

```xml
<task name="PBS_Task">
    <description>
        <![CDATA[ Execute this Task in the PBS node source. ]]>
    </description>
    <genericInformation>
        <info name="NS" value="PBS"/>
    </genericInformation>
    <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println "Hello World"
]]>
          </code>
        </script>
    </scriptExecutable>
</task>
```

The NS value can also be defined at the job level, in that case, every task of this job will be executed on the Native Scheduler node source:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="PBS_Job"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2"
>
  <genericInformation>
    <info name="NS" value="PBS"/>
  </genericInformation>
  <taskFlow>
    <task name="Groovy_Task">
      <description>
        <![CDATA[ The simplest task, ran by a groovy engine. ]]>
      </description>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println "Hello World"
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
</job>
```

It is also possible to define in a ProActive Task or Job the `NS_BATCH` generic information. This parameter allows to provide custom arguments to `nsSubmitCommand`.

For example, to submit a task on the PBS Node source, using a specific PBS queue and reserve for this task 2 cluster nodes with 2 cpu each:

```xml
<task name="PBS_Task">
  <description>
    <![CDATA[ Runs on the PBS node source on queue1, using 2 nodes * 2 cpus ]]>
  </description>
  <genericInformation>
    <info name="NS" value="PBS"/>
    <info name="NS_BATCH" value="-q queue1 -lnodes=2:ppn=2"/>
  </genericInformation>
  <scriptExecutable>
    <script>
      <code language="groovy">
         <![CDATA[
println "Hello World"
]]>
      </code>
    </script>
  </scriptExecutable>
</task>
```

==== Native Scheduler Node Life Cycle

As soon as tasks containing the `NS` generic information are pending, the target Native Scheduler node source will try to deploy ProActive Nodes to execute them.

The node will first appear in `Deploying` <<_node_states,*state*>>. If some error occurs prior to the `nsSubmitCommand` execution (SSH connection, command syntax), the node state will change to `Lost`, with some explanation about the failure displayed.

If the node remains in `Deploying` state, it is possible to monitor the job execution on the native scheduler itself, by logging into the head node, and use the native scheduler command tools.

Example using the PBS `qstat` command:

----
root@osboxes:/tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPSHOT/logs# qstat

Job ID                    Name             User            Time Use S Queue
------------------------- ---------------- --------------- -------- - -----
241.osboxes                STDIN            osboxes                0 R batch

root@osboxes:/tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPSHOT/logs# qstat -f 241

Job Id: 241.osboxes
    Job_Name = STDIN
    Job_Owner = osboxes@osboxes
    job_state = R
    queue = batch
    server = osboxes
    Checkpoint = u
    ctime = Fri May  4 10:13:06 2018
    Error_Path = osboxes:/tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPSH
	OT/logs/Node-osboxes_852t0.out
    exec_host = osboxes/0
    Hold_Types = n
    Join_Path = oe
    Keep_Files = n
    Mail_Points = a
    mtime = Fri May  4 10:13:06 2018
    Output_Path = osboxes:/tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPS
	HOT/logs/Node-osboxes_852t0.out
    Priority = 0
    qtime = Fri May  4 10:13:06 2018
    Rerunable = True
    Resource_List.walltime = 01:00:00
    Resource_List.nodes = 1
    Resource_List.nodect = 1
    Resource_List.neednodes = 1
    session_id = 6486
    substate = 42
    Variable_List = PBS_O_QUEUE=batch,PBS_O_HOME=/home/osboxes,
	PBS_O_LOGNAME=osboxes,
	PBS_O_PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/b
	in:/usr/games:/usr/local/games,PBS_O_MAIL=/var/mail/osboxes,
	PBS_O_SHELL=/bin/bash,PBS_O_LANG=fr_FR.UTF-8,
	PBS_O_WORKDIR=/home/osboxes,PBS_O_HOST=osboxes,PBS_O_SERVER=osboxes
    euser = osboxes
    egroup = osboxes
    hashname = 241.osboxes
    queue_rank = 237
    queue_type = E
    comment = Job started on Fri May 04 at 10:13
    etime = Fri May  4 10:13:06 2018
    submit_args = -o /tmp/activeeon_enterprise-node-linux-x64-8.1.0-SNAPSHOT/l
	ogs/Node-osboxes_852t0.out -e /tmp/activeeon_enterprise-node-linux-x64
	-8.1.0-SNAPSHOT/logs/Node-osboxes_852t0.out -j oe
    start_time = Fri May  4 10:13:06 2018
    Walltime.Remaining = 3587
    start_count = 1
    fault_tolerant = False
    job_radix = 0
    submit_host = osboxes
    init_work_dir = /home/osboxes
    request_version = 1
----

When the deployment is successful, the ProActive Node state will change to `Free` and shortly after to `Busy` as soon as the associated task will be deployed on the node.
After the task completes, the node will be removed and the <<_glossary_ns_cluster_job,cluster job>> will be cancelled using the `nsKillCommand`.

==== Troubleshooting

If the <<_glossary_ns_cluster_job,cluster job>> is in _running_ state, but the ProActive Node associated remains in `Deploying` state, it probably means there is a connection issue between the ProActive Node and the Resource Manager.
If the default ProActive Network Protocol is used (`PNP`), it is necessary to have a two way connection between the cluster and ProActive server. You can refer to the <<_available_network_protocols,*network protocols*>> documentation for more info.

To troubleshoot node deployment, you can:

 * inspect the output of the <<_glossary_ns_cluster_job,cluster job>> if provided in the `nsSubmitCommand`
 * add the following log4j loggers to the ProActive Scheduler Server `config/log/server.properties` +
```properties
log4j.logger.org.ow2.proactive.scheduler.policy=DEBUG
log4j.logger.org.ow2.proactive.scheduler.util=DEBUG
log4j.logger.org.ow2.proactive.nativescheduler=DEBUG
```
 * inspect the ProActive Scheduler server logs.

== Available Network Protocols

ProActive Workflows and Scheduling offers several protocols for
the Scheduler and nodes to communicate. These protocols provide different
features: _speed_, _security_, _fast error detection_, _firewall_ or
_NAT friendliness_ but none of the protocols can offer all these features at
the same time.
Consequently, the selection should be made carefully. Below are introduced
available network protocols.
Configuration and properties are discussed in <<_network_properties>>.

=== ProActive Network Protocol

ProActive Network Protocol (PNP) is the general purpose communication protocol
(`pnp://` scheme). Its performances are quite similar to the well known Java RMI
protocol, but it is much more robust and network friendly. It requires only one
TCP port per JVM and no shared registry. Besides, it enables fast network
failure discovery and better scalability.
PNP binds to a given TCP port at startup. All incoming communications use this
TCP port. Deploying the Scheduler or a node with PNP requires to open one and
only one incoming TCP port per machine.

=== ProActive Network Protocol over SSL

ProActive Network Protocol over SSL (PNPS) is the PNP protocol wrapped inside
an SSL tunnel. The URI scheme used for the protocol is `pnps://`. It includes
the same features as PNP plus ciphering and optionally authentication.
Using SSL creates some CPU overhead which implies that PNPS is slower than PNP.

=== ProActive Message Routing

ProActive Message Routing (PAMR) allows the deployment of the Scheduler and nodes
behind a firewall. Its associated URI
scheme is `pamr://`. PAMR has the weakest expectations on how the network is
configured. Unlike all the other communication protocols introduced previously,
it has been designed to work when only outgoing TCP connections are available.

== Installation on a Cluster with Firewall

When incoming connections are not allowed (ports closed, firewalls, etc.), the
ProActive Scheduler allows you to connect nodes without significant changes
in your network firewall configuration. It relies on the PAMR protocol.

This last does *not* expect *bidirectional* TCP connections.
It has been designed to work when only outgoing TCP connections are available.
Such environments can be encountered due to:

  - Network address translation devices
  - Firewalls allowing only outgoing connections (this is the default setup of many firewalls)
  - Virtual Machines with a virtualized network stack

image::firewall.png[]

When PAMR is activated, the ProActive Scheduler and nodes connect to a PAMR router. This connection is kept open, and used
as a tunnel to receive incoming messages. If the tunnel goes down, it is automatically reopened by nodes.

The biggest drawback of PAMR is that a centralized PAMR router is in charge of routing message between all the PAMR clients.
To soften this limitation PAMR can be used with other communication protocols. This way, PAMR is used only when needed.

By default, PNP is enabled. PNP is the default protocol for better performance and nodes can also use PAMR if needed.
The PAMR Router is started by default along with the ProActive Scheduler.

For a ProActive Node to connect to the ProActive Scheduler using PAMR, the following ProActive configuration file can be used
(`PROACTIVE_HOME/config/network/node.ini`). The properties tell the ProActive Node where to find the PAMR router.
The ProActive Node will then connect to `pamr://0` where `0` is the PAMR id of the Scheduler (0 by default).

[source]
----
proactive.communication.protocol=pamr
proactive.pamr.router.address=ROUTER_HOSTNAME
proactive.pamr.router.port=33647
----

This sample configuration requires to open only one port `33647` for *incoming connections* on the router host
and all the ProActive Nodes will be able to connect to the Scheduler.

PAMR communication can be tunneled using SSH for better security. In that case, the ProActive node will establish
a SSH tunnel between him and the ProActive Scheduler and use that tunnel for PAMR traffic.

See <<_pamr_protocol_properties>> reference for a detailed explanation of each property.

== Control the resource usage

=== Policies

You can limit the utilization of resources connected to the ProActive Scheduler in different ways. When you create node
sources you can use a node source *policy*. A node source policy is a set of rules and conditions which describes when and how many nodes have to be selected for computations.

NOTE: Node source policy are enforced for *non-admin* users *only*, and will have no effect for a user granted with the administrator privileges.

Each node source policy regardless it specifics has a common part where you describe users' and groups' permissions.
When you create a policy you must specify them:

* *nodeUsers* - utilization permission that defines who can get nodes for computations from this node source. It has to take one of the following values:
** `ME` - only the node source creator
** `users=user1,user2;groups=group1,group2;tokens=t1,t2` - only specific users, groups or tokens. I.e. users=user1 - node access is limited to user1; users=user1;groups=group1 - node access is limited to user1 and all users from group group1; users=user1;tokens=t1 - node access is limited to user1 or anyone who specified token t1. If node access is protected by a token, node will not be found by the ProActive Resource Manager when trying to execute workflows, unless the corresponding token is specified inside the workflow(s).
** `ALL` - everybody can use nodes from this node source

NOTE: To specify a token inside a chosen workflow, add the key/value pair *NODE_ACCESS_TOKEN:<token>* to its generic information.

* *nodeProviders* - provider permission defines who can add nodes to this node source. It should take one of the following values:
** `ME` - pnly the node source creator
** `users=user1,user2;groups=group1,group2` - only specific users or groups (for our example user1, user2, group1 and group2). It is possible to specify only groups or only users.
** `ALL` - everybody can add nodes to this node source

The user who created the node source is the administrator of this node source. He can add and removed nodes to it, remove the node source itself,
but cannot use nodes if usage policy is set to *PROVIDER* or *PROVIDER_GROUPS*.

In the ProActive Resource Manager, there is always a default node source configured with a *DefaultInfrastructureManager* and a *Static policy*.
It is not able to deploy nodes anywhere but makes it possible to add existing nodes to the Scheduler (see <<run_node_manually>>)

Out of the box the Scheduler supports time slot policies, cron policies, load based policies and many others.
Please see detailed information about policies in <<_node_source_policy>>.

=== Agents schedule

*Node source policies* limit ProActive Nodes utilization on the level of the ProActive Scheduler.
If you need fine-grained limits on the node level *ProActive Agents* will help you achieve that.

TIP: The typical scenario is when you use desktop workstation for computations during non working hours.

Both linux and windows agents have an ability to:

* Run ProActive Nodes according to the schedule
* Limit resources utilization for these daemons (e.g CPU, memory)

Agents configuration is detailed in the section <<_proactive_agents>>.

[[_nodes_locking]]
=== Locking ProActive Nodes

The Resource Manager allows to lock and unlock ProActive nodes. Locking a Node prevents new Tasks to be launched on
that node. This operation is possible whatever the <<_node_states,state of a Node>> is. Once locked, the Resource Manager keeps track of
who has locked the Node and when.

A common use case for locking Nodes is about maintenances. You may have a long running Task executing on a ProActive
Node where a maintenance is planned. Let's say the current Task must not be interrupted and new ones not started before
achieving the maintenance. A solution is to lock the Node. This way, the current Task will complete but no new Tasks are
scheduled on that Node. Then, it is possible to perform the maintenance and on termination to unlock the Node. Upon
unlocking, the Node becomes again eligible for Tasks execution.

Locking and unlocking a Node, or more generally a set of Nodes, is possible from the REST API, the command line client
but also the Resource Manager portal.

Please note that locks are restored, by default, on Resource Manager restart and thus Scheduler restart.
It is possible to disable this feature by editing the value associated to the property named
`pa.rm.nodes.lock.restoration` in `PROACTIVE_HOME/config/rm/settings.ini`.

When the nodes lock restoration feature is enabled, the Resource Manager will try to lock, per Node Source,
as many Nodes as there were on the previous run.
The approach is best effort and Node hostname is not considered.
As a consequence, Nodes are not necessarily locked on the same host after a Scheduler restart.

=== Undeploying Node Sources

The <<_glossary_resource_manager,Resource Manager>> allows for <<_glossary_node_source,Node Sources>> to be
*undeployed*. In this case, the Node Source is shut down and its <<_glossary_proactive_node,Nodes>> are removed, but
the Node Source definition is kept. An undeployed Node Source can be *redeployed* later, using its initial
configuration.

Node Source undeploying can be *preemptive* or not. If a Node Source undeploying is initiated with a non-preemptive
requirement, then the Nodes of this Node Source which are currently running <<_glossary_task,Tasks>> will not be
removed until their Task is finished. In this case, these Nodes are displayed in the
_to-be-removed_ state. In other terms, if a Node Source is undeployed *non-preemptively*, it will be completely
undeployed as soon as all of its Nodes have finished executing their Task. On the opposite, if a Node Source is
undeployed *with preemption*, its Nodes are immediately removed regardless of whether they execute Tasks, and this may
cause Tasks to fail.

Deploying and undeploying Node Sources is possible from the
<<_glossary_rm_web_interface,Resource Manager Web Interface>>, from the Resource Manager
<<_glossary_rest_api,REST API>>, and from the <<_resource_manager_command_line,Command Line>> client.

TIP: The deployment status of a Node Source is persistent. If the Resource Manager is restarted, then all Node Sources
will be *restored* to their previous state.

== User Authentication

In order to use ProActive Scheduler every user must have an account. It supports three methods for authentication:

* *File based*
* *LDAP*
* *PAM*

=== Select authentication method

By default the ProActive Scheduler is configured to use *file based* authentication and has some default accounts ('demo/demo', 'admin/admin') that
work out of the box.

If you would like to change the method authentication type to use your *LDAP* server or use Linux PAM, you need to modify two configs:

* *Resource Manager* configuration (`PROACTIVE_HOME/config/rm/settings.ini`)
+
[source]
----
#Property that defines the method that has to be used for logging users to the Resource Manager
#It can be one of the following values:
#    - "RMFileLoginMethod" to use file login and group management
#    - "RMLDAPLoginMethod" to use LDAP login management
#    - "RMPAMLoginMethod" to use PAM login management
pa.rm.authentication.loginMethod=RMLDAPLoginMethod
----

* *Scheduler* configuration (`PROACTIVE_HOME/config/scheduler/settings.ini`)
+
[source]
----
#Property that define the method that have to be used for logging users to the Scheduler
#It can be one of the following values :
#	- "SchedulerFileLoginMethod" to use file login and group management
#	- "SchedulerLDAPLoginMethod" to use LDAP login management
#   - "SchedulerPAMLoginMethod" to use PAM login management
pa.scheduler.core.authentication.loginMethod=SchedulerLDAPLoginMethod
----


=== File

By default, the ProActive Resource Manager stores users accounts, passwords, and group memberships (user or admin), in two files:

* users and passwords accounts are stored in `PROACTIVE_HOME/config/authentication/login.cfg`.
Each line has to follow the format *user:encypted password*.
The default accounts in `login.cfg` file correspond to the following passwords, which are encrypted in the actual login.cfg file:
+
.unencrypted_login.cfg
----
admin:admin
user:pwd
demo:demo
guest:pwd
test:pwd
radmin:pwd
nsadmin:pwd
provider:pwd
scheduler:scheduler_pwd
rm:rm_pwd
watcher:w_pwd
test_executor:pwd
----

* users membership is stored in `PROACTIVE_HOME/config/authentication/group.cfg`. For each user registered in login.cfg,
a group membership has to be defined in this file. Each line has to look like *user:group*. Group has to be
user to have user rights, or admin to have administrator rights. Below is an example `group.cfg` file:
+
.group.cfg
----
admin:admin
demo:admin
guest:guests
rm:admin
scheduler:user
user:user
watcher:watchers
----

ProActive contains a set of predefined groups such as "user" and "admin". Groups are defined in `PROACTIVE_HOME/config/security.java.policy-server` as described in chapter <<User Permissions>>.

In order to create new users, delete an existing user, or modify the groups or password for an existing user, a command-line tool is available.

This command is available inside the *tools* folder: `PROACTIVE_HOME/tools/proactive-users`.
You can check the command syntax using the -h option.

----
$ proactive-users -h
usage: proactive-users [-C | -D | -U]  [-g <GROUPS>] [-gf <GROUPFILE>] [-h] [-kf <KEYFILE>] [-l <LOGIN>] [-lf <LOGINFILE>] [-p
       <PASSWORD>] [-sgf <SOURCEGROUPFILE>] [-slf <SOURCELOGINFILE>]
----

Here are examples of use:

* Creating users
+
----
$ proactive-users -C -l user1 -p pwd1 -g user
Created user user1 in H:\Install\scheduling\tools\..\config/authentication/login.cfg
Added group user to user user1
----
The user with login "user1", password "pwd1" and group "user" was created

* Updating users
+
----
$ proactive-users -U -l user1 -p pwd2 -g nsadmins,admin
Changed password for user user1 in H:\Install\scheduling\tools\..\config/authentication/login.cfg
Added group nsadmins to user user1
Added group admin to user user1
----
User "user1" now has password pwd2 and groups nsadmins & admin (group "user" was removed).

* Deleting users
+
----
$ proactive-users -D -l user1
Deleted user user1 in H:\Install\scheduling\tools\..\config/authentication/login.cfg
----
User "user1" does not exist any more.

* Creating multiple users
+
It is also possible to create multiple users at once using a source login file and a source group file.
In that case, the source login file contains, for each user, a line with the format "login:unencrypted_password".
The source group file has the same structure as the `PROACTIVE_HOME/config/authentication/group.cfg` file.
This can be used, for example, to convert login files used by ProActive Scheduler versions prior to 7.19.0.
+
----
$ proactive-users -C -slf source_login.cfg -sgf source_group.cfg
Adding group admin to user admin1
Created user admin1
Adding group user to user user2
Created user user2
Adding group user to user user1
Created user user1
Stored login file in H:\Install\scheduling\tools\..\config/authentication/login.cfg
Stored group file in H:\Install\scheduling\tools\..\config/authentication/group.cfg
----

* Updating multiple users
+
Similarly, it is possible to update existing users with source login or group files. It is possible to update only group membership for existing users, or only passwords, or both.
+
The example below shows how to update only groups for existing users:
+
----
proactive-users -U -sgf source_group_2.cfg
Adding group admin to user user1
Updated user user1
Adding group admin to user user2
Updated user user2
Stored login file in H:\Install\scheduling\tools\..\config/authentication/login.cfg
Stored group file in H:\Install\scheduling\tools\..\config/authentication/group.cfg
----

=== LDAP

The ProActive Resource Manager is able to connect to an existing *LDAP* server, to check users login/password and verify users group
membership. This authentication method can be used with existing LDAP server that is already configured.


In order to use it, few parameters have to be configured, such as *path in LDAP tree users*, LDAP *groups* that define
user and admin group membership, *URL* of the LDAP server, LDAP *binding method* used by connection and configuration
of SSL/TLS if you want a secured connection between the ProActive Resource Manager and LDAP.

We assume that LDAP server is configured in the way that:

* all existing users and groups are located under single domain
* users have object class specified in parameter *pa.ldap.user.objectclass*
* groups have object class specified in parameter *pa.ldap.group.objectclass*
* user and group name is defined in cn (Common Name) attribute

[source]
----
# EXAMPLE of user entry
#
# dn: cn=jdoe,dc=example,dc=com
# cn: jdoe
# firstName: John
# lastName: Doe
# objectClass: inetOrgPerson

# EXAMPLE of group entry
#
# dn: cn=mygroup,dc=example,dc=com
# cn: mygroup
# firstName: John
# lastName: Doe
# uniqueMember: cn=djoe,dc=example,dc=com
# objectClass: groupOfUniqueNames
----

The LDAP configuration is defined in `PROACTIVE_HOME/config/authentication/ldap.cfg`. You need to:

. *Set the LDAP server URL*
+
First, you have to define the LDAP's URL of your organisation. This address corresponds to the property: `pa.ldap.url`. You have to put a standard LDAP-like URL, for example *ldap://myLdap*. You can also set a URL with secure access: *ldaps://myLdap:636*.
+
. *Define object class of user and group entities*
+
Then you need to define how to differ user and group entities in LDAP tree. The users object class is defined by
property `pa.ldap.user.objectclass` and by default is _inetOrgPerson_. For groups, the property `pa.ldap.group.objectclass`
has a default value _groupOfUniqueNames_ which could be changed.

. *Configure LDAP authentication parameters*
+
By default, the ProActive Scheduler binds to LDAP in anonymous mode. You can change this authentication
method by modifying the property `pa.ldap.authentication.method`. This property can have several values:
+
* none (default value) - the ProActive Resource Manager performs connection to LDAP in anonymous mode.
* simple - the ProActive Resource Manager performs connection to LDAP with a specified login/password (see below for user password setting).
+
You can also specify a SASL mechanism for LDAPv3. There are many SASL available mechanisms: cram-md5, digest-md5, kerberos4. Just set this property to *sasl* to let the ProActive Resource Manager JVM choose SASL authentication mechanism.
If you specify an authentication method different from 'none' (anonymous connection to LDAP), you must specify a login/password for authentication.
+
There are two properties to set in LDAP configuration file:
+
** `pa.ldap.bind.login` - sets user name for authentication.
** `pa.ldap.bind.pwd` - sets password for authentication.
+
. *Set SSL/TLS parameters*
+
A secured SSL/TLS layer can be useful if your network is not trusted, and critical information is transmitted between the rm server and LDAP, such as user passwords.
First, set the LDAP URL property `pa.ldap.url` to a URL of type *ldaps://myLdap*. Then set `pa.ldap.authentication.method` to *none* so as to delegate authentication to SSL.
+
For using SSL properly, you have to specify your certificate and public keys for SSL handshake. Java stores certificates in a keyStore and public keys in a trustStore. In most of the cases, you just have to define a trustStore with public key part of LDAP's certificate. Put certificate in a keyStore, and public keys in a trustStore with the keytool command (keytool command is distributed with standard java platforms):
+

    keytool -import -alias myAlias -file myCertificate -keystore myKeyStore
+
myAlias is the alias name of your certificate, myCertificate is your private certificate file and myKeyStore is the new keyStore file produced in output. This command asks you to enter a password for your keyStore.
+
Put LDAP certificate's public key in a trustStore, with the keytool command:
+
    keytool -import -alias myAlias -file myPublicKey -keystore myTrustStore
+
myAlias is the alias name of your certificate's public key, myPublicKey is your certificate's public key file and myTrustore is the new trustStore file produced in output. This command asks you to enter a password for your trustStore.
+
Finally, in `config/authentication/ldap.cfg`, set keyStore and trustStore created before to their respective passwords:
+
* Set `pa.ldap.keystore.path` to the path of your keyStore.
* Set `pa.ldap.keystore.passwd` to the password defined previously for keyStore.
* Set `pa.ldap.truststore.path` to the path of your trustStore.
* Set `pa.ldap.truststore.passwd` to the password defined previously for trustStore.

. *Use fall back to file authentication*
+
You can use simultaneously file-based authentication and LDAP-based authentication. Then, ProActive Scheduler can check at first user password and group membership in
login and group files, as performed in FileLogin method. If user or group is not found in login file, login or group will be searched in LDAP.
It uses `pa.rm.defaultloginfilename` and `pa.rm.defaultgroupfilename` files to authenticate user and check group membership. There are two rules:

* If file group membership checking fails, fall back to group membership checking with LDAP.
To activate this behavior set `pa.ldap.group.membership.fallback` to true, in LDAP configuration file.
* If a user is not found in the login file, fall back to authentication and group membership checking with LDAP. To activate this behavior, set `pa.ldap.authentication.fallback` to true, in LDAP configuration file.

=== PAM

The ProActive Scheduler & Resource Manager are able to interact with Linux *PAM (Pluggable Authentication Modules)* to check users login/password.
It is not currently possible to retrieve linux system group memberships for users. Thus groups must be managed using the `PROACTIVE_HOME/config/authentication/group.cfg` file.

In order to enable PAM authentication, follow the following steps:

1. Configure the PAM login methods in the Scheduler and RM settings:
+
.PROACTIVE_HOME/config/rm/settings.ini
----
pa.rm.authentication.loginMethod=RMPAMLoginMethod
----
+
.PROACTIVE_HOME/config/scheduler/settings.ini
----
pa.scheduler.core.authentication.loginMethod=SchedulerPAMLoginMethod
----
+

2. Copy the file `PROACTIVE_HOME/config/authentication/proactive-jpam` to `/etc/pam.d` (you must be root to perform this operation).
+
You can modify this default PAM configuration file if you need specific PAM policies.


3. Add the user which will start the scheduler process `PROACTIVE_HOME/bin/proactive-server` to the *shadow* group
+
[source]
----
sudo usermod -aG shadow your_user
----
+
After this command, the user, called "your_user" here in this example, will be added to group *shadow*. You may need to logoff/logon before this modification can be effective.

4. Associate, for each Linux system users which will connect to the Scheduler, a ProActive group.
+
Following the procedure described in chapter <<File>>, groups must be associated to existing PAM users in the ProActive group file `PROACTIVE_HOME/config/authentication/group.cfg`.
+
In case of PAM users, the login file should not be modified as password authentication will be performed at the Linux system level. Accordingly, you should not use the *proactive-users* command to associate groups to PAM users.

NOTE: Similarly to the LDAP authentication, there is a fallback mechanism for login authentication, with the difference that it is always activated.
A user defined in the ProActive login file always has precedence over the same user defined on the Linux system.

== Task Termination Behavior

=== ProActive Node Graceful Task Termination (SIGTERM) at Killing

The task termination timeout is a cleanup timeout for each task (executed on this Node)
after it was killed (through the Scheduler REST API or the Scheduler web portal).
By default the task termination timeout is set to 10 seconds
but it can be setup on each ProActive Node individually.
It can be set at node startup by setting the "proactive.node.task.cleanup.time" property.
Example: add  "-Dproactive.node.task.cleanup.time=30" (set to 30 seconds as example) to
the node startup command.

Find an overview of termination behavior by task and language in section <<Termination behavior by language>>.


==== Step by Step Killing Example

With a 30 seconds termination timeout following steps happen.

===== First

The kill request is received by the Scheduler Server.

===== Second

The ProActive Node which executes the task receives the kill request.
The behavior depends if the Scheduler Server is configured to launch tasks in Forked or Non-Forked Mode.

*Forked Mode*:

On Linux, a SIGTERM signal is send to the JVM which executes the task.
On Windows, the TerminateProcess method of the JVM process which executes the task is executed.
That will finally lead to the Task Process being interrupted by:

* A SIGTERM signal on Linux
* TerminateProcess method called, on Windows

The task can deal with the killing event and start a cleanup procedure. If
the task has started sub-processes it is responsible to initiate graceful termination
of the sub-processes. Handling a graceful termination event looks different in each task type
and is dependent on the capabilities of the language used.
As an example Bash and Jython (Python task) can catch a SIGTERM signal.
Java and Groovy task need to register shutdownHooks to achieve similar behavior.

*Non-Forked Mode*:

In Non-Forked Mode, the task runs in a separate thread in the same JVM as the ProActive Node.
Therefore, the thread running the task will be interrupted.

Bash tasks, in Non-Forked Mode, do receive SIGTERM.
Java and Groovy tasks will receive an InterruptedException, which can be used to handle
the graceful termination of that task and sub-processes.
But, Jython (Python tasks) will not receive any notification.
If the task has started sub-processes it is responsible to initiate graceful termination of the sub-processes,
if possible.

===== Third

The ProActive Node waits as many seconds as setup in the proactive.node.task.cleanup.time property or until
the task is stopped.
Example: cleanup timeout is 30 seconds and the task takes 3 seconds to terminate, then the ProActive Node will
wait 3 seconds only.

===== Fourth

The ProActive Node initiates the task process and sub-process removal.
This will forcefully kill the whole tree of processes. Another SIGTERM
might be send shortly before the forcefully killing (SIGKILL).

===== Additional information

*On Windows* the termination procedure is similar. But the Windows SIGTERM equivalent is
executing the TerminateProcess method of the process. Whereas the SIGKILL is equivalent to
the forceful removal of the running process.


===== Termination behavior by language

.Termination Behavior for a Few Languages
|===
|Language/Execution type|Bash |Python |Java |Groovy

|Forked Mode
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Add a shutdown hook. Task waits cleanup timeout, before being killed.
| add a shutdown hook. Task waits cleanup timeout, before being killed.

|Forked Mode Run As Me
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Add a shutdown hook. Task waits cleanup timeout, before being killed.
| Add a shutdown hook. Task waits cleanup timeout, before being killed.


|Non-Forked Mode
| Handle SIGTERM. Task waits cleanup timeout, before being killed.
| Terminates immediately.
| Catch an InterruptedException. Task waits cleanup timeout, before being killed.
| Catch an InterruptedException. Task waits cleanup timeout, before being killed.


|===


== User Permissions

All users authenticated in the Resource Manager have their own role according to granted permissions.
In ProActive Scheduler, we use the standard
http://www.oracle.com/technetwork/java/javase/jaas/index.html[Java Authentication and Authorization Service (JAAS)^]
to address these needs.

The file `PROACTIVE_HOME/config/security.java.policy-server` allows to configure fine-grained access for all users, e.g. who has the right to:

* Deploy ProActive Nodes
* Execute jobs
* Pause the Scheduler
* etc

== Monitor the cluster state

Cluster monitoring typically means checking that all ProActive Nodes that were added to *ProActive Scheduler*
are up and running. We don't track for example the free disk space or software upgrade which can be
better achieved with tools like http://www.nagios.org[Nagios^].

In the *Resource Manager Web Interface* you can see how many ProActive Nodes were added to the
Resource Manager and their usage.

image::admin_web.png[]

The same information is accessible using the command line:

[source]
----
$ PROACTIVE_HOME/bin/proactive-client --listnodes
----

[[_node_states]]
=== ProActive Node States

When you look at your cluster, ProActive Nodes can be in one of the following states:

* `Deploying` - The deployment of the node has been triggered by the ProActive Resource Manager but it has not yet been added.
* `Lost` - The deployment of the node has failed for some reason.
The node has never been added to the ProActive Resource Manager and won't be usable.
* `Configuring` - Node has been added to the ProActive Resource Manager and is being configured.
* `Free` - Node is available for computations.
* `Busy` - Node has been given to user to execute computations.
* `To be removed` - Node is busy but requested to be removed. So it will be removed once the client will release it.
* `Down` - Node is unreachable or down and cannot be used anymore.
* `Needed` - Metric shows how many additional nodes the Scheduler needs to schedule all pending tasks.

The state of a ProActive Node is managed by the Resource Manager. However, each Node has a
<<_nodes_locking,user managed lock status>>.

=== JMX

The JMX interface for remote management and monitoring provides information about the running ProActive Resource Manager and allows the user to modify its configuration.
For more details about JMX concepts, please refer to official documentation about
the http://www.oracle.com/technetwork/java/javase/tech/javamanagement-140525.html[JMX architecture^].

image::jmx_archi.png[align="center"]

The following aspects (or services) of the *ProActive Scheduler* are instrumented using MBeans that are managed through a JMX agent.

* *Server status* is exposed using the RuntimeDataMBean
** The Resource Manager status
** Available/Free/Busy/Down nodes count
** Average activity/inactivity percentage
* The *Accounts Manager* exposes accounting information using the MyAccountMBean and AllAccountsMBean
** The used node time
** The provided node time
** The provided node count
* Various *management operations* are exposed using the ManagementMBean
** Setting the accounts refresh rate
** Refresh all accounts
** Reload the permission policy file

MBean server can be accessed by remote applications using one of the two available connectors

* The standard solution based on Remote Method Invocation (RMI) protocol is the RMI Connector accessible at the following url:
`service:jmx:rmi:///jndi/rmi://HOSTNAME:PORT/JMXRMAgent` where
** *HOSTNAME* is the hostname on which the Resource Manager is started
** *PORT* (5822 by default) is the port number on which the JMX RMI connector server has been started. It is defined by the property pa.rm.jmx.port .
* The ProActive Remote Objects Connector provides ProActive protocol aware connector accessible at the following url:
`service:jmx:ro:///jndi/PA_PROTOCOL://HOSTNAME:PORT/JMXRMAgent` where
** *PA_PROTOCOL* is the protocol defined by the proactive.communication.protocol property
** *HOSTNAME* is the hostname on which the Resource Manager is started
** *PORT* is the protocol dependent port number usually defined by the property proactive.PA_PROTOCOL.port

The name of the connector (JMXRMAgent by default) is defined by the property `rm.jmx.connectorname`.

The JMX url to connect to can be obtained from the Authentication API of the Resource Manager or by reading the log file located in `PROACTIVE_HOME/logs/RM.log`.
In that log file, the address you have to retrieve is the one where the JMX RMI connector server has been started

[source]
----
[INFO 2010-06-17 10:23:27,813] [RM.AbstractJMXHelper.boot] Started JMX RMI connector server at service:jmx:rmi:///jndi/rmi://kisscool.inria.fr:5822/JMXRMAgent
----

Once connected, you'll get an access to Resource Manager statistics and accounting.

For example, to connect to the ProActive Scheduler JMX Agent with JConsole tool, just enter the url of the standard RMI
Connector, as well as the username and the password.

image::jmx_jconsole_connect.png[align="center"]

Then depending on the allowed permissions browse the attributes of the MBeans.

image::jmx_jconsole.png[align="center"]

=== Accounting

The users of ProActive Scheduler request and offer nodes for computation. To keep track of how much node time was
consumed or contributed by a particular user, ProActive Scheduler associates a user to an account.

More precisely, the nodes can be manipulated by the following basic operations available to the users

* The `ADD` operation is a registration of a node in the Resource Manager initiated by a user considered as a node provider.
A node can be added, through the API, as a result of a deployment process, through an agent or manually from the command line interface.
* The `REMOVE` operation is the unregistration of a node from the Resource Manager.
A node can be removed, through the API, by a user or automatically if it is unreachable by the Resource Manager.
* The `GET` operation is a node reservation, for an unknown amount of time, by a user considered as a node owner.
For example, the ProActive Scheduler can be considered as a user that reserves a node for a task computation.
* The `RELEASE` operation on a reserved node by any user.

The following accounting data is gathered by the Resource Manager

* *The used node time*: The amount of time other users have spent using the resources of a particular user.
More precisely, for a specific node owner, it is the sum of all time intervals from `GET` to `RELEASE`.
* *The provided node time*: The amount of time a user has offered resources to the Resource Manager.
More precisely, for a specific node provider, it is the sum of all time intervals from `ADD` to `REMOVE`.
* *The provided node count*: The number of provided nodes.

The accounting information can be accessed only through a *JMX client* or the ProActive Resource Manager *command line*.

=== Metric to monitor Scheduler load

*Needed Nodes* metric shows how many additional nodes the Scheduler needs to schedule all pending tasks,
e.g. there are 4 nodes and all of them are busy, but *Needed Nodes* shows 10, which means if you would have
14 nodes instead of 4, all of these nodes would be busy.
Please notice, that *Needed Nodes* shows how many more nodes could be used right now.
However, it does not give any indication about the kind of nodes needed,
e.g. Selection Script of the task could require some specific nodes,
thus *Needed Nodes* could be positive despite still having free nodes
(they can be inappropriate for the current task).


You can find this metric in multiple places. In RM portal you can see it in Nodes History:

image::neededNodes-rm-nodeshistory.png[align="center"]

In Nodes State:

image::neededNodes-rm-nodesstate.png[align="center"]

And you can find it in RM portal detailed statistics:

image::neededNodes-rm-stats.png[align="center"]

Also, you can find *Needed Nodes* in the Scheduler portal where the Scheduler status is displayed:

image::neededNodes-scheduler.png[align="center"]

[[_run_as_me]]

== Run Computation with a user's system account

Configure a ProActive Node to execute tasks under a user's system account by ticking the
link:../user/ProActiveUserGuide.html#_run_computation_with_your_system_account[Run as me] box in the task configuration.
By default authentication is done through a password, but can also be configured using a SSH key.

Find a step by step tutorial link:AdminTutorials.html[here].

For proper execution, *the user's system account must have:*

* Execution rights to the PROACTIVE_HOME directory and all it's parent directories.
* Write access to the PROACTIVE_HOME directory.

Logs are written during task execution, as a matter of fact, the executing user needs write access to the
PROACTIVE_HOME directory. ProActive does some tests before executing as a different user, those need full execution
rights, including the PROACTIVE_HOME parent directories.

TIP: Create a ProActive group which owns the PROACTIVE_HOME directory and has write access. Add every system user, which
executes tasks under its own system account, to the ProActive group.

NOTE: On Mac OS X, the default temporary folder is not shared between all users. It is required by the RunAsMe feature.
To use a shared temporary folder, you need to set the _$TMPDIR_ environment variable and the Java property _java.io.tmpdir_
to /tmp before starting the ProActive Node.

NOTE: the same applies on Windows. To use a shared temporary folder, create a folder shared by all proactive users e.g. C:\TEMP and define
the Java property _java.io.tmpdir_ to C:\TEMP before starting the ProActive Node.

Example:
----
proactive-node -Djava.io.tmpdir=C:\TEMP
----

=== Using password

The ProActive Node will try to impersonate the user that submitted the task when running it. It means the
username and password must be the same between the Scheduler and the operating system.

=== Using SSH keys

A SSH key can be tied to the user's account and used to impersonate the user when running a task on a given machine (using SSH).
The .ssh/authorized_keys files of all machines must be configured to accept this SSH key.
The SSH key must require *no passphrase*.

When login into the scheduler portal, the private key of the user must be provided, this can be done by selecting on the login dialog : `More options > Use SSH private key`.

To enable this method, set the system property `pas.launcher.forkas.method` to _key_ when starting a ProActive Node.

Example:
----
proactive-node -Dpas.launcher.forkas.method=key
----

=== Using passwordless sudo

This configuration, only availabe on linux nodes, allows the impersonation to be performed using passwordless sudo.

To enable it at the system level, edit the /etc/sudoers file to allow passwordless sudo from the account running the proactive node
to any users which require impersonation. Passwordless sudo should be enabled for any command.

For example, the following line will allow the proactive account to impersonate to any user:
----
proactive ALL=(ALL) NOPASSWD: ALL
----

To enable this configuration on the proactive node, start it with the system property `pas.launcher.forkas.method` to _none_

Example:
----
proactive-node -Dpas.launcher.forkas.method=none
----


[[_web_applications]]
== Configure Web applications

The ProActive Scheduler deploys automatically several web applications in an
embedded Jetty web server. The binaries and sources associated to the web
applications can be found in `PROACTIVE_HOME/dist/war`.

TIP: To configure Web applications, for instance the HTTP port to use, edit the `PROACTIVE_HOME/config/web/settings.ini` file.

Web applications are deployed on the host using the HTTP port 8080 by default. The local part of the URL is based on the WAR files found in `PROACTIVE_HOME/dist/war`.
For instance, the Scheduler web portal is available at http://localhost:8080/scheduler.

=== Enable HTTPS

ProActive Workflows and Scheduling provides support for HTTPS. Similarly to
other web settings, HTTPS related properties are defined in
`PROACTIVE_HOME/config/web/settings.ini` and any update necessitates a restart
of the Scheduler instance.

Enabling HTTPS requires to set `web.https` to `true` but also to define a path
to a valid link:https://en.wikipedia.org/wiki/Keystore[keystore] through
`web.https.keystore` along with its associated password
by using `web.https.keystore.password`. A default keystore is provided for
testing purposes, however it must not be used in production.

When you are using the default keystore or a custom keystore with a self-signed
certificate you need to enable `web.https.allow_any_certificate` and optionally
`web.https.allow_any_hostname` if your certificate Common Name (CN) does not
match the fully qualified host name used for the machine hosting the Web
applications.

Port 8443 is used for listening to HTTPS connections.
This port number prevents to have root access for deploying web applications.
However, you can change it by editing `web.https.port` value.

When HTTPS is set up, you can automatically redirect any HTTP request from
users to the secured version of the protocol to make sure exchanged information
is protected. It requires to enable `web.redirect_http_to_https`.

Unlike other web applications, the Scheduler, the Resource Manager and the studio web portals together with the
Proactive cloud watch, Cloud automation service, Job planner and Notification service
require manual configuration based on your HTTPS settings. The configuration
files to edit are respectively in

* `PROACTIVE_HOME/dist/war/scheduler/scheduler.conf`
* `PROACTIVE_HOME/dist/war/rm/rm.conf`
* `PROACTIVE_HOME/config/web/settings.ini`
* `PROACTIVE_HOME/dist/war/proactive-cloud-watch/WEB-INF/classes/application.properties`
* `PROACTIVE_HOME/dist/war/cloud-automation-service/WEB-INF/classes/application.properties`
* `PROACTIVE_HOME/dist/war/job-planner/WEB-INF/classes/application.properties`
* `PROACTIVE_HOME/dist/war/notification-service/WEB-INF/classes/application.properties`



NOTE: If you are familiar with Nginx or a similar web server, you can also use
it to enable HTTPS connection to Web applications. It has the advantage to make
it possible to have ProActive web applications and additional ones to coexist.

==== Creating a valid keystore

Java is bundled with some utility binaries such as
link:http://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html[keytool].
This last allows to create and manage a keystore but also to generate keys and
certificates.

===== With a self-signed certificate

Generating a new self-signed certificate imported in a new keystore with a
custom password is as simple as executing the following command:

[source]
----
$JAVA_HOME/bin/keytool -keystore keystore -alias jetty -genkey -keyalg RSA -validity 365
----

This command prompts for information about the certificate and for a password to
protect both the keystore and the keys within it. The only mandatory responses
are to provide a password and the Fully Qualified Domain Name of the server. Once
done, the value for properties `web.https.keystore` and
`web.https.keystore.password` must be adapted based on the information you have
entered.

For more information, please look at the
link:http://www.eclipse.org/jetty/documentation/current/configuring-ssl.html[Jetty documentation].

===== With a trusted certificate

The first step is to obtain a trusted certificate that is valid for your Fully
Qualified Domain Name (FQDN). This process differs from a Certification
Authority (CA) to another. In the following, it is assumed that
link:https://letsencrypt.org[Let's Encrypt] is used. By way of illustration,
Digital Ocean provides examples for link:https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-centos-7[CentOS] and
link:https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-14-04[Ubuntu].

Assumming your FQDN is _example.org_, then after obtaining a certificate you will
get the following PEM-encoded files in `/etc/letsencrypt/live/example.org`:

  - cert.pem: Your domain's certificate
  - chain.pem: The Let's Encrypt chain certificate
  - fullchain.pem: cert.pem and chain.pem combined
  - privkey.pem: Your certificate's private key

Before loading the key and the certificate into a new keystore that is
recognized by ProActive Workflows and Scheduling, you need to combine them into
a PKCS12 format file. You can achieve this action by means of the following
OpenSSL command:

[source]
----
openssl pkcs12 \
    -inkey /etc/letsencrypt/live/example.org/privkey.pem \
    -in /etc/letsencrypt/live/example.org/cert.pem \
    -export -out jetty.pkcs12
----

Then, you can load the resulting PKCS12 file into a JSSE keystore with keytool:

[source]
----
keytool -importkeystore -srckeystore jetty.pkcs12 \
    -srcstoretype PKCS12 -destkeystore keystore
----

Both commands prompt for a password. You need to use the same and set it as a
value to property `web.https.keystore.password`. The resulting _keystore_ file
corresponds to the file whose path must be used for property
`web.https.keystore`.

IMPORTANT: link:https://bugs.openjdk.java.net/browse/JDK-8154757[Starting from update 101],
Java 8 trusts Lets Encrypt certificates. However, if you are using a less recent
version of Java for running JVMs related to ProActive Workflows and Scheduling,
you will need to update the truststore of your Java installation.

==== Updating Java truststore for accepting Lets Encrypt certificates

[source]
----
sudo keytool -trustcacerts \
    -keystore $JAVA_HOME/jre/lib/security/cacerts \
    -storepass changeit \
    -noprompt \
    -importcert \
    -file /etc/letsencrypt/live/example.org/chain.pem
----

=== Enable VNC remote visualization in the browser

The Rest API web application embeds a proxy to allow the remote display visualization of a ProActive Node running a given task
via a VNC server from the browser. To enable this feature, the scheduler has to be configured as follows:

* configure the proxy: edit the `PROACTIVE_HOME/config/web/settings.ini` file and set `novnc.enabled` to true in order
to start the proxy.
* configure the scheduler portal to use the proxy when opening a new tab browser that shows the remote visualization:
edit the `PROACTIVE_HOME/dist/war/scheduler/scheduler.conf`.
** Set `sched.novnc.url` to the public address of the proxy. This public address is the public address of the host where the sheduler is started and the port specified by the option `novnc.port` in the file `PROACTIVE_HOME/config/web/settings.ini`
** Set `sched.novnc.page.url` to the public address of the VNC client to be executed in the browser. The client is located
in the REST API with the file `novnc.html`.

For more information, see https://github.com/ow2-proactive/scheduling/blob/master/rest/README-novnc.txt

[[_catalog]]
=== Catalog
include::./Catalog.adoc[]

[[_script_engines]]
== Configure script engines

Most script engines do not need any configuration. This section talks about the script engines which can be configured
individually.

=== Docker Compose (Docker task)

The Docker Compose script engine is a wrapper around the Docker Compose command. It needs to have Docker Compose
 and Docker installed, in order to work. The Docker Compose script engine has a configuration file which configures each ProActive Node individually.
The configuration file is in `PROACTIVE_HOME/config/scriptengines/docker-compose.properties`. +
`docker-compose.properties` has the following configuration options:

---
docker.compose.command=[docker-compose executable example:/usr/local/bin/docker-compose]
---
Defines which Docker Compose executable is used by the script engine.

---
docker.compose.sudo.command=[sudo executable example:/usr/bin/sudo]
---
Defines the sudo executable which is used. The sudo property is used to give the Docker Compose command root rights.

---
docker.compose.use.sudo=false
---
Defines whether to execute Docker Compose with sudo [true] or without sudo [false]

---
docker.host=
---
Defines the DOCKER_HOST environment variable. The DOCKER_HOST variable defines the Docker socket which the Docker client connects to.
That property is useful when accessing a Docker daemon which runs inside a container or on a remote machine.
Further information about the DOCKER_HOST property can be found https://docs.docker.com/engine/reference/commandline/cli/[in the official Docker documentation^].

=== Perl scripts execution (Perl task)
Perl should be installed on your machine. By default the perl engine is installed in Linux and Mac Os. Please install the perl engine on Windows. +
The execution mechanism of perl tasks is based on running perl files on your machine.

If you encountered the issues, please check if you have installed Perl properly on you machine.
In order to verify this you can follow the next steps:

* For example you should to be able to launch the command 'perl -V'.
* Then from command line you should be able to launch a simple perl file (perl yourPerlFile.pl).

=== Python Script Engine (Python task)
Python should be installed on your machine. By default Python2 is installed on Linux and Mac OS. If another version of Python is required,
it should be installed on your machine in advance.

More over, the native Python Script Engine depends on the py4j library in order to communicate with our Java classes. Please follow the following step to install it:

* With Python2: `pip install py4j`

* With Python3: `pip3 install py4j`

Further information can be found https://www.py4j.org/install.html[in the official py4j documentation^].

[[_scheduling_policies]]
== Extending ProActive Scheduling Policy

In order to decide which pending Tasks are executed, the ProActive Scheduler uses a <<_glossary_scheduling_policy,*Scheduling Policy*>>.

This policy allows, for example, to execute task in a First-In-First-Out order or according to priorities.

The default Scheduling Policy can be extended to perfectly fit with an organization scheduling constraints.

Moreover, policy can be changed dynamically throught Java API `org.ow2.proactive.scheduler.common.Scheduler::changePolicy(String policyClassName)`.

In the following example, we show how a different policy can be used to control Task Scheduling.


=== Earliest deadline first (EDF) policy

To use the earliest deadline first (EDF) policy,  edit the file `PROACTIVE_HOME/config/scheduler/settings.ini` and change the following line:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.ExtendedSchedulerPolicy
----

to:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.edf.EDFPolicyExtended
----


=== License Policy Example

To use the LicenseSchedulingPolicy, edit the file `PROACTIVE_HOME/config/scheduler/settings.ini` and change the following line:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.ExtendedSchedulerPolicy
----

to:

----
pa.scheduler.policy=org.ow2.proactive.scheduler.policy.license.LicenseSchedulingPolicy
----

The LicenseSchedulingPolicy requires an extra configuration: the maximum number of licenses per software must be specified into
`PROACTIVE_HOME/config/scheduler/license.properties` like this

----
software_A = 3
software_B = 2
----

From now, at a job or a task level, user can specify which software licenses are required, by adding the REQUIRED_LICENSES generic information,
which value can be

----
softwareA,softwareB
----

As expected, if 10 tasks requiring software_A,software_B licenses are submitted, they will be executed 2 by 2, according to the software with the fewest licenses.
The same logic is applied at the job level, whatever the number of tasks composing each job, since all tasks of the same job share the same license.
Moreover, user can combine both levels.

== Addons

[[_get_notifications]]
=== Get Notification on Job Events Configuration

In order to enable this functionality the following properties should be set in `config/scheduler/setting.ini`:

[source, groovy]
----
pa.scheduler.notifications.email.enabled = true
pa.scheduler.notifications.email.from = username@example.com
----

To configure the email sender on Scheduler, the configuration file `config/scheduler/emailnotification.properties` should be filled in, please refer to
link:../admin/ProActiveAdminGuide.html#_smtp_configuration_examples[SMTP configuration examples] for examples.

=== Email notification

The email notification addons includes an utility Java class but also a Groovy
task (accessible from the Studio Web app) that allow to send an email
from a ProActive Workflow.

The task assumes that configuration for connecting to an SMTP server is done using
third-party credentials. It requires to define some values as key/value pairs in the third-party
credentials associated to the user that runs the task.

This configuration can be achieved from the Scheduler Web portal, the
ProActive client or even through the REST API. Please see
link:../user/ProActiveUserGuide.html#_third_party_credentials[Third-party credentials] for more information.

==== SMTP configuration examples

===== Free

[width="60%",frame="topbot",options="header,footer"]
|======================
| Key | Value
| mail.smtp.host | smtp.free.fr
| mail.smtp.username | user@free.fr
| mail.smtp.password | user_password
|======================

===== Gmail

Password authentication to Google servers requires extra configuration.

See https://www.google.com/settings/security/lesssecureapps.

[width="60%",frame="topbot",options="header,footer"]
|======================
| Key | Value
| mail.smtp.host            | smtp.gmail.com
| mail.smtp.starttls.enable | true
| mail.smtp.ssl.trust       | smtp.gmail.com
| mail.smtp.username        | user@gmail.com
| mail.smtp.password        | user_password
|======================

===== Outlook

Password authentication to Microsoft servers requires extra configuration.

See http://pchelp.ricmedia.com/how-to-fix-550-5-3-4-requested-action-not-taken-error/

[width="60%",frame="topbot",options="header,footer"]
|======================
| Key | Value
| mail.smtp.host            | smtp-mail.outlook.com
| mail.smtp.starttls.enable | true
| mail.smtp.ssl.trust       | smtp-mail.outlook.com
| mail.smtp.username        | user@gmail.com
| mail.smtp.password        | user_password
|======================

=== Statistics on ProActive Jobs and Resource Usage

include::./references/StatisticsOnProActiveJobsAndResourceUsageReference.adoc[]

== Scheduler start tuning

Scheduler start can be customized by automatically triggering user's scripts.
A list of scripts can be specified in `PROACTIVE_HOME/config/scheduler/settings.ini`
following this

`pa.scheduler.startscripts.paths=/your/script1/path;/your/script2/path;/your/script3/path`

For example, load-examples.groovy which is executed by default at scheduler start, has in charge
the full deployment of proactive-examples by: populating the running Catalog, exposing as templates
specific workflows and copying workflows dependencies into dataspaces.

== Performance tuning

[[_perf_tuning_db]]
=== Database

By default, ProActive Scheduler is configured to use http://hsqldb.org[HSQLDB]
embedded database. This last is lightweight and does not require complex
configuration. However, it cannot compete with more conventional database
management systems, especially when the load increases. Consequently, it is
recommended to setup an RDBMS such as https://mariadb.com[MariaDB],
http://www.postgresql.org[Postgres] or
http://www.microsoft.com/en-us/server-cloud/products/sql-server/[SQL Server] if
you care about performance or notice a slowdown.

ProActive Workflows and Scheduling distribution is provided with several
samples for configuring the Scheduler and Resource Manager to use most
standard RDBMS.
You can find these examples in `PROACTIVE_HOME/config/rm/templates/` and
`PROACTIVE_HOME/config/scheduler/templates/` folders.

==== Indexes

The Scheduler and RM databases rely on Hibernate to create and update the
database schema automatically on startup. Hibernate is configured for adding
indexes on frequently used columns, including columns associated to foreign
keys. The goal is to provide a default configuration that behaves in a
satisfactory manner with the different features available in the Scheduler.
For instance, the Jobs housekeeping feature requires indexes on most foreign
keys since a Job deletion may imply several cascade delete.

Unfortunately, each database vendor implements its own strategy regarding
indexes on foreign keys.  Thus, some databases such as _MariaDB_ and _MySQL_
automatically add indexes for foreign keys whereas some others like _Postgres_
or _Oracle_ do not add such indexes by default. Besides, Hibernate does not
offer control over this strategy.
As a consequence, this difference of behaviour, once linked to our default
configuration for Hibernate, may lead to duplicate indexes (two indexes with
different names for a same column) that are created for columns in RM and
Scheduler tables.

In case you are using in production _MySQL_, _MariaDB_ or a similar database
that automatically adds indexes on foreign keys, it is strongly recommended to
ask your DBA to identify and delete duplicate indexes created by the database
system since they may hurt performances.

==== Housekeeping

The Scheduler provides a housekeeping mechanism that periodically removes finished jobs
from the Scheduler Portal. It can also remove jobs and all its data from the database to save space.
This mechanism has two phases:

. Once a job is finished, it sets its scheduled time for removal (the job expiration date)
. Actual cleaning of expired jobs from the scheduler and/or the database will be periodically triggered and performed in a bulk operation

The following parameters are accessible in <<Scheduler Properties,the scheduler config file>>.
One can edit the `pa.scheduler.core.automaticremovejobdelay` parameter to set the delay before
marking a job to be removed.
The `pa.scheduler.core.removejobdelay` parameter can be used in the case where the delay should only
start once the job result has been accessed.

The housekeeping mechanism is periodically triggered through the cron
expression `pa.scheduler.core.automaticremovejobcronexpression`.
As the `pa.scheduler.job.removeFromDataBase` property is set to `true` by default,
a query is executed in-base to remove all jobs that qualify for the bulk removal.

==== Database impact on job submission performance

The choice of the database provider can influence the Scheduler and Resource Manager performance overall.
In the following figure, we see the job submission time between different database providers:

image::job-submission-time-comparison.png[align="center"]

On a 1000-jobs submission basis across ProActive's embedded database, the in-memory database and a standalone PostgreSQL server,
we can see that the in-memory database is in fact faster and more consistent in speed than the default embedded database.
However the in-memory database has no persistence of the data on disk, as opposed to the embedded database.
A standalone database is a good tradeoff between speed and data resilience.

The embedded database is used by default in ProActive. To switch to an in-memory database, you need to modify the following files:

`PROACTIVE_HOME/config/scheduler/database.properties`:

Replace the default value of `hibernate.connection.url` to:
```
hibernate.connection.url=jdbc:hsqldb:mem:scheduler;hsqldb.tx=mvcc;hsqldb.lob_file_scale=1
```

`PROACTIVE_HOME/config/rm/database.properties`:

Replace the default value of `hibernate.connection.url` to:
```
hibernate.connection.url=jdbc:hsqldb:mem:rm;hsqldb.tx=mvcc;hsqldb.lob_file_scale=1
```

To switch to a standalone database, see <<_perf_tuning_db>>.

==== Database impact on task creation performance

The choice of the database provider influences task creation time.
In the following figure, we see the task creation time for different database providers:

image::grouped-bar-direct-labels.png[align="center"]

Each experiment, consists of submitting one replicated job with some number of replicated tasks, and measuring
time to create all these replicated tasks.

//MariaDB
//10K - 95s
//20K - 296s
//Default
//10K - 750s
//20K - 1750s
//In-memory
//10K - 85s
//20K - 238s

=== Tuning Linux for 15K nodes

It is necessary to tune underlying Linux system, in case there will be plenty of nodes in the Resource Manager.

It is mandatory to allow having a large number of running processes and open files.

For example, on 15.000 nodes, the recommended ulimit settings for running processes and open files are:

```
nproc soft/hard 100000
nofile soft/hard 65536
```

In addition, we recommend to have at least 16GB RAM for 15.000 nodes.


== Nodes and Task Recovery

The Scheduler can be started in *recovery mode*. If so, the Scheduler will try to restore Resource Manager nodes and
ProActive tasks that were running in the previous execution of the Scheduler. The nodes
and task recovery feature makes sure you do not lose on-going computations if the Scheduler experiences a failure.

=== Use Case and Scope of the Feature

Suppose that at an instant T the Scheduler is running tasks. Without the nodes and task
recovery feature, if the Scheduler crashed at instant T, then the tasks that were on-going would be
*re-scheduled* on any node and *re-executed* from the beginning when the Scheduler restarts. With the nodes and task
recovery feature enabled, when the Scheduler restarts it will retrieve the *previous* RM nodes and the *on-going* tasks,
and the execution of the tasks will continue and finish as if there was no failure of the Scheduler.

WARNING: The nodes and task recovery feature is *not* applicable when the Scheduler is cleanly stopped or exits normally.
This is a mechanism that is *only* applicable upon *failure* of the Scheduler (machine crash, abrupt exit).

=== How Does Nodes and Task Recovery Work?

At startup, the Scheduler checks the nodes and the tasks that were previously persisted in database to be able to restore
their state. Here are the different cases that can occur at recovery time:

* If a node that is found in database is still *alive*, the node's information is restored in the Resource Manager. The
same restoration mechanism is applied for running tasks in the Scheduler.

* If however, no node is found in database for a given *node source*, then the node recovery will
not take place for this node source. Instead, the nodes will be re-deployed like for the first time.

* If a node that is found in database is *not alive* at recovery time, then the node will also be recreated automatically.

*A recovery is not a restart*, and as such a clean restart of the Scheduler will not preserve on-going tasks and
running nodes. Indeed, in the case of a *regular shutdown* of the Scheduler, the nodes are *cleaned and removed* from the
Resource Manager before the Scheduler exits. Thus, there will be no nodes and no running tasks to recover when the
Scheduler is started again.

To handle the particular situation in which the Scheduler is down when a running task terminates, a proper
*configuration* of the Scheduler allows the task to hold the result until the Scheduler is up and running again.

=== Nodes and Task Recovery Configuration

==== Restrictions

The nodes recovery is not available for Scheduler settings that use the PAMR protocol. This is because this
communication protocol relies on router endpoint identifiers that are regenerated when connections are reestablished.

The nodes recovery is also not available on the Nodes that are launched at the Scheduler startup by default. However
you get the choice to activate or deactivate the feature when you create your own Node Source. By default, the nodes
recovery is activated for any new Node Source.

==== How to make sure that nodes and task recovery is enabled

WARNING: The nodes and task recovery feature requires a proper configuration to ensure that nodes and tasks are *kept
alive* during the down time of the Scheduler.

* *Resource Manager* configuration (`PROACTIVE_HOME/config/rm/settings.ini`)
+
In order to enable the nodes and task recovery feature, make sure that the `pa.rm.nodes.recovery` property of the
Resource Manager is set to `true`. This property is set to `true` by default.
+

* *Node* configuration
+
The ProActive nodes should be started with a configuration that keeps them alive if the Scheduler fails. Node properties
can be set at node startup:

** Use `-Dproactive.node.ping.delay` to specify how often the node will try to communicate with the Resource Manager.
The default for this property is 30 seconds.

** Use `-Dproactive.node.reconnection.attempts` to specify how many times the node will try to reconnect with the
Resource Manager. The default for this property is to attempt 10 times.

+
These two properties define the total delay for the node to reconnect to the Resource Manager. By default, the total
delay of reconnection is 5 minutes. When this total delay is consumed, _the node shuts itself down_, and the node recovery
will not be applicable any more for this node.
+

* *Scheduler* configuration (`PROACTIVE_HOME/config/scheduler/settings.ini`)
+
In order to make sure that a task will attempt several times to send back its result to the Scheduler, you must set the
properties:

** `pa.scheduler.core.node.ping.attempts` to specify the number of times a finished task will try to contact the
Scheduler. The default for this property is to attempt only once.

** `pa.scheduler.core.nodepingfrequency` property to define a delay (in seconds) in between two attempts. The default
for this property is 20 seconds.

+
If the Scheduler is still not up and running after the total delay defined by these properties (which is 20 seconds by
default), then _the task’s result will be lost_ forever.
Note that these two properties are also used by the scheduler to decide when to *re-schedule* tasks.

==== How to disable nodes and task recovery

You can completely disable the nodes and task recovery feature by setting the `pa.rm.nodes.recovery` property of the
*Resource Manager* to `false`.

In addition, the nodes and task recovery can be configured *per Node Source* at creation time. This configuration will
be applied only if the global `pa.rm.nodes.recovery` property is set to `true`. The specification of nodes recovery per
Node Source can be done through the <<_glossary_rm_web_interface,Resource Manager Web Interface>>, through the Resource
Manager <<_glossary_rest_api,REST API>>, and through the <<_resource_manager_command_line,Command Line>> client (CLI).

** Through the *Web Interface*, the form to create a new Node Source contains a checkbox that controls nodes
recoverability.

** Through the *REST API*, the `nodesRecoverable` parameter of the `rm/nodesource/create` and `rm/nodesource` *REST
endpoints* specify whether the ProActive Nodes of a new Node Source will be recoverable. _Nodes recovery will be
disabled_ if the given value does not match any case of the "true" string.

** Through the *CLI*, the `createns` and `definens` *commands* take an optional parameter to specify whether the
ProActive Nodes of a new Node Source will be recoverable. _Nodes recovery will be disabled_ if
the optional parameter is provided and does not match any case of the "true" string.

=== Nodes and Task Recovery on Cloud Platforms

The nodes and task recovery feature of ProActive is also available for the deployments on _Microsoft Azure_ and _Amazon
EC2_ clouds.

For these platforms, when the Scheduler is restarted after a failure, it will try to contact the nodes that are
deployed on the cloud *instances*:

* If the nodes are alive then the recovery proceeds like with non-cloud infrastructures.

* If the nodes cannot be contacted, the recovery mechanism will try to re-deploy them by asking the instances to run a
node deployment script.

* If the script fails, it means that the cloud instances do not exist anymore, so the recovery mechanism will trigger a
re-deployment including the re-creation of the instances.

=== Nodes and Task Recovery Performance Tuning

By default, the persistence of the node states is slightly delayed in order to batch several database operations in the
same transaction. This is done in order to minimize the execution time overhead of the nodes and task recovery feature.
However, this default *batching mechanism* can be overriden to adapt the delay, or to disable batching.

The properties that allow performance tuning are the following:

** `pa.rm.node.db.operations.delay` defines the delay in milliseconds that is applied when the RM requests the
persistence of a node. By default the delay is set to 100 milliseconds. If the value of this property is set to `0`,
then all database operations related to the persistence of nodes are executed *immediately and synchronously*.

** `pa.rm.nodes.db.operations.update.synchronous` defines whether the node updates are persisted synchronously
(with no delay) whenever it is possible. By default this property is set to `true`.


== Troubleshooting
=== Logs

If something goes wrong the first place to look for the problem are the Scheduler logs. By default all logs are in
`PROACTIVE_HOME/logs` directory.

Users submitting jobs have access to server logs of their jobs through the *Scheduler Web interface*

image::server_logs.png[align="center"]

=== Common Problems

==== 'Path too Long' Errors When Unzipping Windows Downloads

When you unzip a Windows package using the default Windows compression utility,
you might get errors stating that the path is too long. Path length is
determined by the Windows OS. The maximum path, which includes drive letter,
colon, backslash, name components separated by backslashes, and a terminating
null character, is defined as 260 characters.

Workarounds:

  - Move the Zip file on the root level of the system drive and unzip from there.
  - Use a third-party compression utility. Unlike the default Windows
  compression utility, some third-party utilities allow for longer maximum
  path lengths.

== Reference

=== Scheduler Properties

include::./references/SchedulerPropertiesReference.adoc[]

=== Resources Manager Properties

include::./references/RMPropertiesReference.adoc[]

[[_network_properties]]
=== Network Properties

include::./references/NetworkPropertiesReference.adoc[]

[[_rest_api_properties]]
=== REST API & Web Properties

include::./references/WebPropertiesReference.adoc[]

[[_catalog_properties]]
=== Catalog Properties
include::./references/CatalogReference.adoc[]

[[_scheduler_portal_properties]]
=== Scheduler Portal Properties
include::./references/SchedulerPortalReference.adoc[]

=== Node Sources

The ProActive Resource Manager supports ProActive Nodes aggregation from heterogeneous environments.
As a node is just a process running somewhere, the process of communication to such nodes is unified.
The only part which has to be defined is the procedure of nodes deployment which could be quite
different depending on infrastructures and their limitations. After installation of the server and node parts it is
possible to configure an automatic nodes deployment. Basically, you can say to the Resource Manager how to
launch nodes and when.

In the ProActive Resource Manager, there is always a default node source consisted of DefaultInfrastructureManager
and Static policy. It is not able to deploy nodes anywhere but makes it possible to add existing nodes to the RM.


==== Node Source Infrastructure

include::./references/NodeSourceInfrastructureReference.adoc[]

==== Node Source Policy

include::./references/NodeSourcePolicyReference.adoc[]

[[_resource_manager_command_line]]
=== Command Line
include::../CLI.adoc[]

include::../Dedication.adoc[]
